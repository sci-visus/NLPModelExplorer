\section{Related Works}
Due to the increasing demand for model interpretability, many previous works have been proposed for making sense NLP models by examining individual predictions as well as the model mechanism as a whole. 
%
In the recent work~\cite{LiChenHovy2015}, the composability of the vector-based text representation is investigated by utilizing instance-level attribution techniques originated from the vision community, i.e., \cite{ZeilerFergus2014}. 
%
In the representation erasure work~\cite{li2016understanding}, the authors explain the neural model decisions by exploring the impact of altering or removing the components of the model (i.e., changing the dimension count of hidden units, or input words) on the prediction performance. 

Besides interpreting the model via carefully designed experiments, interactive demo/visualization systems, such as AllenNLP demo (\url{http://demo.allennlp.org/}), also attract attention. These systems often rely on visual encodings to summary the critical information of the model and provide a flexible environment, in which the user can experiment and explore the various input combinations and carry out error analysis.
%The training dynamic of RNN network is visualized in the RNNbow system~\cite{CashmanPattersonMosca2017}, and 
The hidden state properties of the LSTM are visualized and investigated in the LSTMvis visualization system \cite{StrobeltGehrmannPfister2018}.
Lee et al. visualize the beam search and attention component in the NMT model~\cite{lee2017interactive},  in which the user can dynamically change the probability for the next step of the search tree or change the weight of the attention.
In the visualization work on the question and answering model~\cite{ruckle2017end}, the system shows the text context and highlights the critical phrase that used to answers the questions. 
%The user is able to use the visual feature to compare the benefit and drawback of multiple attention based neural network model.

