\section{Related Works}
Due to the increasing demand for model interpretability, many previous works has been proposed for understanding NLP models in term of making sense of individual predictions as well as model mechanism as a whole. 

In the recent work~\cite{LiChenHovy2015}, the composability of the vector-based representation of text is investigate by utilizing instance based attribution techniques originated from the vision community (i.e., \cite{SimonyanVedaldiZisserman2013, ZeilerFergus2014, YosinskiCluneNguyen2015, OlahMordvintsevSchubert2017}). 
%
In the representation erasure work~\cite{li2016understanding}, the authors explain the neural model decisions by exploring the impact of various parts of the model by altering or removing the corresponding representations (i.e., the dimension count of hidden units, or input words). 

Beside interpreting the model via carefully designed experiments, interactive demo / visualization systems, such as AllenNLP demo (\url{http://demo.allennlp.org/}), also attract attention as it provide a flexible environment where the user can experiment and explore the various input combination and carry out error analysis.
%
Some of these interactive systems incorporate various visual encodings to summary the key information of the model.
%
The training dynamic of RNN network is visualized in the RNNbow system~\cite{CashmanPattersonMosca2017}, and the hidden state properties of the LSTM are investigated in the LSTMvis visualization tool~\cite{StrobeltGehrmannPfister2018}.
%Interactive Visualization and Manipulation of Attention-based Neural Machine Translation
Lee et al. visualizes the beam search and attention component in the NMT model. The work~\cite{lee2017interactive} directly involve the user into the decision-making process by enabling dynamic change the probability for the next step of the search tree or change the weight value of the attention.
%}
In the recent attention visualization work on question and answering model~\cite{ruckle2017end}, 
instead of visualizing the attention matrix directly, the system using color map to show the text context and highlights the critical phrase that used to answers the questions. %The user is able to use the visual feature to compare the benefit and drawback of multiple attention based neural network model.
%


