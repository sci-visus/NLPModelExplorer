\section{Related Works}
Due to the increasing demand for model interpretability, many previous works have been proposed for making sense of NLP models by examining individual predictions as well as the model mechanism as a whole. 
%
In recent work,~\citet{LiChenHovy2015} investigated the composability of the vector-based text representations using instance-level attribution techniques originated from the vision community~\cite[e.g.,][]{ZeilerFergus2014}. 
%
In a study of the representation of erasure, \citet{li2016understanding} explained neural model decisions by exploring the impact of altering or removing the components of the model (i.e., changing the dimension count of hidden units, or input words) on the prediction performance. 

Besides interpreting the model via carefully designed experiments, several interactive demo/visualization systems, such as AllenNLP's demos (\url{http://demo.allennlp.org/})  often rely on visual encodings to summarize the model predictions. They  provide a flexible environment in which the user can experiment with the various inputs and perform error analysis.
%The training dynamic of RNN network is visualized in the RNNbow system~\cite{CashmanPattersonMosca2017}, and 
The hidden state properties of the LSTM are visualized and investigated in the LSTMvis visualization system \cite{StrobeltGehrmannPfister2018}.
\citet{lee2017interactive} visualized the beam search and attention component in neural machine translation models, in which the user can dynamically change the probability for the next step of the search tree or change the weight of the attention.
In the visualization work on question answering~\cite{ruckle2017end}, the system shows the text context and highlights the critical phrase that is used to answers the question. 
%The user is able to use the visual feature to compare the benefit and drawback of multiple attention based neural network model.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "NLPVis-demo-paper"
%%% End:
