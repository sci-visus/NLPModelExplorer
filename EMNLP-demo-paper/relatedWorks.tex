\section{Related Works}
Due to increasing demand for interpretability to make sense of individual predictions and the model as a whole, many works has been proposed for understanding NLP models. 

In the work by Li et al., the composability of the vector-based representation of text is investigate by utilizing techniques from vision community \cite{LiChenHovy2015}. 
%
In the representation erasure work~\cite{li2016understanding}, the authors explain the neural model decisions by exploring the impact of various parts of model by altering or removing the corresponding representations (i.e., dimension of hidden units, or input words). 
%Interactive Visualization and Manipulation of Attention-based Neural Machine Translation
Lee et al. visualizes the beam search and attention component in the NMT model~\cite{lee2017interactive}. The work allows user involve into the decision-making process by enabling dynamic change the probability for the next step of the search tree or change the weight value of the attention.
%}
%End-to-End Non-Factoid Question Answering with an Interactive Visualization of Neural Attention Weights
%\textcolor{blue}{
In the recent attention visualization work on question and answering model~\cite{ruckle2017end}, 
instead of visualizing the attention matrix directly, the system using color map to show the text context and highlights the critical phrase that used to answers the questions. The user is able to use the visual feature to compare the benefit and drawback of multiple attention based neural network model.
%}
Interactive demo system, such as AllenNLP web demo (\url{http://demo.allennlp.org/}), also attract attention as it provide an flexible environment that the user can experiment and explore the various input combination and carry out error analysis.

