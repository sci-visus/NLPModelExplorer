\section{Related Works}
Due to the increasing demand for model interpretability, many works has been proposed for understanding NLP models in term of making sense of individual predictions as well as the understanding model mechanism as a whole. 

In the work~\cite{LiChenHovy2015} by Li et al., the composability of the vector-based representation of text is investigate by utilizing instance based attribution techniques originated from the vision community (i.e., \cite{SimonyanVedaldiZisserman2013, ZeilerFergus2014, YosinskiCluneNguyen2015, OlahMordvintsevSchubert2017}). 
%
In the representation erasure work~\cite{li2016understanding}, the authors explain the neural model decisions by exploring the impact of various parts of model by altering or removing the corresponding representations (i.e., dimension of hidden units, or input words). 

Beside interpret the model via carefully designed experiments, interactive systems, such as AllenNLP web demo (\url{http://demo.allennlp.org/}), also attract attention as it provide a flexible environment where the user can experiment and explore the various input combination and carry out error analysis.
%
As a result, interactive visual analytics system that incorporate novel visual encoding has been adopted in these interactive environment.
%
The training process of RNN network is visualized in the RNNbow system~\cite{CashmanPattersonMosca2017}, and the properties of the LSTM are investigated in the LSTMvis visualization tool~\cite{StrobeltGehrmannPfister2018}.
%Interactive Visualization and Manipulation of Attention-based Neural Machine Translation
Lee et al. visualizes the beam search and attention component in the NMT model~\cite{lee2017interactive}. The work allows user involve into the decision-making process by enabling dynamic change the probability for the next step of the search tree or change the weight value of the attention.
%}
%End-to-End Non-Factoid Question Answering with an Interactive Visualization of Neural Attention Weights
%\textcolor{blue}{
In the recent attention visualization work on question and answering model~\cite{ruckle2017end}, 
instead of visualizing the attention matrix directly, the system using color map to show the text context and highlights the critical phrase that used to answers the questions. The user is able to use the visual feature to compare the benefit and drawback of multiple attention based neural network model.
%}

