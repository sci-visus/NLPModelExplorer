\section{Introduction}

Neural networks have been successful in various natural language processing tasks,
where end-to-end training become a primary option (\cite{}).
As the design of neural networks evolve, there is a clear trend that more and more
parameters participate in training and prediction. While the strong expressivity
enhance model performance, the lack of interpretability presents challenging for analysis.
Without understanding the background meaning that features carry, visualization relies
on proper quantification analysis (cite). Such analysis is not easy to come up with,
and is tied to specific task (cite).


Recently attention networks become a widely-adopted setup (cite), partly because it yields
intermediate representations (e.g. alignment) that are not just beneficially expressive for performance,
but also intuitive to model. Indeed, the interpretability of attention networks introduce
a natural interface for analyzing the inside of neural networks. The wide popularity of attention networks
in recent NLP works necessitate a convenience framework for visualization and analyzing.

To facilitate exploration in attention models, we introduce an interactive visualization toolset.

exploration task

Comparsion with AllenNLP
