
\section{Related Works}
With the increasing demands for model accountability (e.g., what is the evidence for making the prediction) and model fairness (e.g., is the prediction affected by the bias in the training data),
the interpretability of the machine learning model held significant implications. As a result, a wealth of research have be focused on the explanation and interpretation of deep neural network models from both machine learning as well as visualization community.

\textbf{Model Interpretability.}
Before one can solve the model interpretability challenge, it worth to define what constitute an explanation or interpretation of a machine learning model.
As discussed in ~\cite{Lipton2016, Doshi-Velez2017}, the concept of interpretable is inherently subjective, since the recipients of the explanation are always human.
% And the discussion of interpretability quickly become philosophical.
However, despite the underspecification nature of the problem, we can still refine the concept and provide certain guideline~\cite{Doshi-Velez2017}.
%
%And as suggested in~\cite{Doshi-Velez2017}
One of the most studied areas in model interpretability is the explanation of a given prediction.
Many works have been dedicated to providing intuitive explanations for a given prediction, several~\cite{RibeiroSinghGuestrin2016, KrausePererNg2016} of which approach the problem from a model agnostic perspective that made them applicable to different applications (i.e., by fitting a simpler linear model near the prediction of interests).
%
Despite being invaluable for the providing human understandable explanation, the lack of the ability to access and explore the internal states of the model, which is vitally important to make sense of why a model fail, limits their ability for more in depth exploration and evaluation of the model.

\textbf{Visual Exploration of Neural Network Models.}
Due to the effectiveness for exploring complex relationships, visualization have long been adopted for interpreting neural network models~\cite{TzengMa2005}.
Recently, the increasingly apparent interpretability challenge for deep neural network model motivate the introduction of many visualization works that dedicated to the neural network models.
% The classification~\cite{KrausePererNg2016}
In the work~\cite{BilalJourablooYe2018} by Bilal et al., the authors analyzing classification errors in convolutional neural networks and answer the question on whether class hierarchy is learn in the network.
Liu et al.~\cite{LiuShiCao2018} explore the possibility of understanding the training process of deep generative models.
The ActiVis~\cite{KahngAndrewsKalro2018} focus on large scale network in real world setting.
The DeepEyes~\cite{Pezzotti2018} focuses on the analyzing the training process, so they can provide design guideline to the deep neutral network architect.

% \begin{itemize}
%     \item ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models
%     \item Analyzing the Training Processes of Deep Generative Models
%     \item Visual Diagnosis of Tree Boosting Methods
%     \item TreePOD: Sensitivity-Aware Selection of Pareto-Optimal Decision Trees
%     \item LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks
%     \item Do Convolutional Neural Networks learn Class Hierarchy?
%     \item DeepEyes: Progressive Visual Analytics for Designing Deep Neural Networks
% \end{itemize}

\textbf{NLP Models Visualization.}
Compare to the multitude of research for making sense of convolutional neural networks, relatively less works has been dedicated to neural linguistic models.
%
However, the previous works~\cite{KarpathyJohnson2015, LiChenHovy2015, StrobeltGehrmannPfister2018, LiuBremerJayaraman2018} not only demonstrated the benefit of visualization in understanding NLP models, but also revealed enoumous possibility for future applications.
% There is many unresolve challenge and the
The early work on charactor level recurrent network~\cite{KarpathyJohnson2015} demonstrates effectness of the hidden state in capturing the unique pattern in training text. The compositionality exibited in neural linguistic model (i.e., build sentence from meaing of words and phrases) are examined in the work by Li et al. ~\cite{LiChenHovy2015}, by utilizing methods inspired by similar work in computer vision.
The LSTMvis~\cite{StrobeltGehrmannPfister2018} work utilized a line plot style visual encoding to visualize the time varying hidden states of recurrent neutral network. The word embedding visualization work~\cite{LiuBremerJayaraman2018} illustrate how semantic relationship can be recovered by linear projections in the high-dimensional word embedding space produced by word2vec~\cite{MikolovSutskeverChen2013} or Glove~\cite{PenningtonSocherManning2014}.
