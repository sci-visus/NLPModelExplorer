
\section{Related Works}
With the increasing demands for model accountability (e.g., what is the evidence for making the prediction) and model fairness (e.g., is the prediction affected by the bias in the training data),
the interpretability of the machine learning model held significant implications. As a result, a wealth of research has been developed that focused on the explanation and interpretation of neural network models from both machine learning and visualization community.

\textbf{Model Interpretability.}
Before one can solve the model interpretability challenge, it worth to define what constitute an explanation or interpretation of a machine learning model.
As discussed in ~\cite{Lipton2016, Doshi-Velez2017}, the concept of interpretable is inherently subjective, since the recipients of the explanation are always human.
% And the discussion of interpretability quickly become philosophical.
However, despite the underspecification nature of the problem, we can still refine the concept and provide certain guidelines~\cite{Doshi-Velez2017}.
%And as suggested in~\cite{Doshi-Velez2017}
In machine learning domain, one of the most studied areas in model interpretability is the explanation of predictions made by a convolution neural network (CNN) ~\cite{SimonyanVedaldiZisserman2013, ZeilerFergus2014, YosinskiCluneNguyen2015,OlahMordvintsevSchubert2017, OlahSatyanarayanJohnson2018}.
%
These methods either identify the pattern that maximally activate a part of the network (referred to as feature visualization, i.e., what feature does the given neuron, channel, or layer captures), or reveal what area in the input contribute most to the prediction label (often referred to as attribution, i.e., attribute the prediction label in the input domain).
%
Beside the techqniues design for specific type of model, a few works~\cite{RibeiroSinghGuestrin2016, KrausePererNg2016} approach the interpretability challenge from a model agnostic perspective, which make them applicable to different applications. Intuitively, many of them aim to fit a simpler linear model near the prediction of interests to provide a simpler but easier to understanding snapshot of the model near the a specific input.
%
Despite being invaluable for providing human understandable explanation, however, the lack of the ability to access and explore the internal states of the model, which is vitally important for making sense of the inner works of a model, limits their ability for more in depth exploration and evaluation of the model.

\textbf{Visual Exploration of Neural Network Models.}
Due to the effectiveness for exploring complex relationships, visualization have long been adopted for interpreting neural network model~\cite{TzengMa2005}.
Recently, the increasingly demand for interpretability for deep neural network model motivate the introduction of many visualization works that dedicated to the neural network models.
% The classification~\cite{KrausePererNg2016}
In the work~\cite{BilalJourablooYe2018} by Bilal et al., the authors analyzing classification errors in convolutional neural networks and answer the question on whether class hierarchy is learn in the network.
Liu et al.~\cite{LiuShiCao2018} explore the possibility of understanding the training process of deep generative models.
The ActiVis~\cite{KahngAndrewsKalro2018} focus on large scale network in real world setting.
The DeepEyes~\cite{Pezzotti2018} focuses on the analyzing the training process, so they can provide design guideline to the deep neutral network architect.

% \begin{itemize}
%     \item ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models
%     \item Analyzing the Training Processes of Deep Generative Models
%     \item Visual Diagnosis of Tree Boosting Methods
%     \item TreePOD: Sensitivity-Aware Selection of Pareto-Optimal Decision Trees
%     \item LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks
%     \item Do Convolutional Neural Networks learn Class Hierarchy?
%     \item DeepEyes: Progressive Visual Analytics for Designing Deep Neural Networks
% \end{itemize}

\textbf{NLP Models Visualization.}
Compare to the multitude of research for making sense of convolutional neural networks, relatively less works has been dedicated to neural linguistic models.
%
However, the previous works~\cite{KarpathyJohnson2015, LiChenHovy2015, StrobeltGehrmannPfister2018, LiuBremerJayaraman2018} not only demonstrated the benefit of visualization in understanding NLP models, but also revealed enormous possibility for future applications.
% There is many unresolve challenge and the
The early work on character level recurrent network~\cite{KarpathyJohnson2015} demonstrates effectiveness of the hidden state in capturing the unique pattern in training text. The compositionality exibited in neural linguistic model (i.e., build sentence from meaning of words and phrases) are examined in the work by Li et al. ~\cite{LiChenHovy2015}, by utilizing methods inspired by similar work in computer vision.
In the RNNVis~\cite{MingCaoZhang2017} work, the Ming et al. explore the  hidden state units in recurrent neural network based on their expected response to input texts.
The RNNbow~\cite{CashmanPattersonMosca2017} work visualizes the backpropagation gradient information during the training of a recurrent neutral network to help the domain experts understand the how the model is trained.
The LSTMvis~\cite{StrobeltGehrmannPfister2018} work utilizes a line plot style visual encoding to visualize the time varying hidden states of recurrent neutral network. The word embedding visualization work~\cite{LiuBremerJayaraman2018} illustrate how semantic relationship can be recovered by linear projections in the high-dimensional word embedding space produced by word2vec~\cite{MikolovSutskeverChen2013} or Glove~\cite{PenningtonSocherManning2014}.
