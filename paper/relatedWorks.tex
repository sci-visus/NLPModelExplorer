\section{Related Works}

How to interpret classification task
\begin{itemize}
    \item What feature / dimension / part-of-the-input should change to
    alter the outcome
    \item What Examples can you provide that is similar to the current
    instance
\end{itemize}
\begin{itemize}
    \item Interacting with Predictions: Visual Inspection of Black-box Machine Learning Models,
    Use partial dependency (varying along one feature and observer how prediction changes)
    \item “Why Should I Trust You?” Explaining the Predictions of Any Classifier (LIME)
\end{itemize}

(Limitation of the state-of-the arts)
Model independent explanations is useful to gain intuition, but they lack the ability to link the external observation with inner states/mechanism of the model.

Depth learning vis
\begin{itemize}
    \item ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models
    \item Analyzing the Training Processes of Deep Generative Models
    \item Visual Diagnosis of Tree Boosting Methods
    \item TreePOD: Sensitivity-Aware Selection of Pareto-Optimal Decision Trees
    \item LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks
    \item Do Convolutional Neural Networks learn Class Hierarchy?
    \item DeepEyes: Progressive Visual Analytics for Designing Deep Neural Networks
\end{itemize}

Less emphasis on where does the fail
