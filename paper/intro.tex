\firstsection{Introduction}
\maketitle

% (What is the high level motivation)
% What is the obsatcle for effectively apply machine learning model
With the recent advances in neural network based model, machine learning has gained unprecedented popularity, and its adaption permeated many fields of studies.
%
However, from researchers to practitioners, one often need to overcome many obstacles during the training, debugging, and tuning processes to unleash the full potential of these models.
%
Interpreting the internal mechanism and analyzing how predictions are made are critical to both the design and deployment of a model.
More importantly, the ability to pinpoint where and how an error is made and come up with hypotheses for the cause of failure is the key to identify the limitation and improve upon existing models.
However, providing meaningful answers to these questions are extremely challenging and often regarded as impossible by many.

Recently, substantial research has been developed to combat the model interpretability challenge. Most notably, the works~\cite{SimonyanVedaldiZisserman2013, ZeilerFergus2014, YosinskiCluneNguyen2015, OlahMordvintsevSchubert2017, LiuShiLi2017, OlahSatyanarayanJohnson2018, BilalJourablooYe2018} on interpreting convolution neural network (CNN) model from both machine learning and visualization community have revealed the enormous potential and instant benefits of model interpretation, yet they also remind us how little we actually know about the inner mechanism of these deep neural networks.
%
Compare to image classification task, natural language processing systems have innate restrictions (e.g., the discrete nature of words) that makes the road to interpretation more treacherous. For example, \emph{feature visualization}~\cite{OlahMordvintsevSchubert2017}, an often deployed technique for illustrating what image features a given part of the network (e.g., neuron, layer, or channel) capture, can not be easily generalized to natural language. Image (pixel values) represents a continouous space, in which human can easily recognize the existance (or the absence) of patterns or shapes. For natural language, even though we can encode a word as a vector~\cite{MikolovSutskeverChen2013, PenningtonSocherManning2014}, the word embedding space is still discretely defined, where the interpolation between two vectors (i.e., two words) do not hold clear meaning.
%
These restrictions call for new avenues for solving the interpretation challenges of natural language models, which motivate the proposed work.

% Beside explore the internal states of the model, understand how different parts of the model work together and how one may affect one another is as or if not more important.
Moreover, despite many recent advances, the majority of the existing techniques study the model as an invariant object, where the model's behaviors are recorded and analyzed in an offline fashion.
%
However, the exploratory nature of the model interpretation analysis often leads to many \emph{what if ...} types of questions. Such as, what if I perturbed the current input? Will the prediction be stable? What if we change one of the critical internal states of the model, how would the modification affect the prediction? What if the current prediction is wrong, how and where should we change the model parameters to produce the correct result? These type of queries form a natural way to gain understanding and come up with hypotheses of the model mechanisms by \emph{interrogating} how the components of a model interact with each other in a dynamic setting. Theoretically, we can code up specialized experiments for each of these scenarios. However, such a process is not only tedious but also ignored the iterative nature of the exploratory analysis. Often, these questions are not pre-determined. Instead, new questions arise as we explore the data and analyze previous observations.

In this work, we aim to provide immediate and informative answers to these \emph{what if} questions by exploiting the expressive power of visualization and flexible integration with the underlying computation model. Instead of viewing the model as an invariant object, we approach the interpretability challenge by studying them in a dynamic environment. By employing a perturbation-driven paradigm, we probe the model and examine how changes in one part of the pipeline (the input, internal states, and output prediction, see Fig.~\ref{fig:modelPipeline}) affect others, which in turn provide a new perspective to attack the model interpretation problem.
%
We implement our vision of the perturbation-driven system around the natural language inference problem~\cite{Parikh2016}.
%
However, the componenets of the visualization and the overall paradigm can be easily extended to other NLP tasks, such as neural machine translation, and paragram summerization, etc.
%
In the simplest form, the inference task ask the question about whether \emph{sentence A} can infer the content of \emph{sentence B} (\emph{entail}), or they contradict with each other (\emph{contradiction}), or they talk about different/unrelated things (\emph{neutral}). Language inference addresses the fundimental challenge of sentence semantic similarity, and is one the core natural language process (NLP) tasks (detials in Section~\ref{sec:languageInference}).
%
One recent advance in NLP is the introduction of attention mechanisms~\cite{VaswaniShazeerParmar2017} (see details in Section~\ref{sec:attention}). Intuitively, the \emph{attention} captures where in the input the model should focus on, which assigns importance to individual word or a pair of word (i.e., the alignment between words in different sentences).
%
There has been many speculation on how attention works in various models. In this work, by utilizing the the proposed system, we explore how changes in sentence input, attention, or prediction affect each other. Such type of exploration is not only essentail to interpret the language inference problem, but also other NLP model utilized the same/similar mechansims.
%
Finally, we also enhanced the standard visual encoding (e.g., as a bipariate graph or as an matrix) of the attention matrix by overlaying sentence linguistic structure to allow grammar guided simplification and the ability to examine how attention corresponds to grammarical structure.
%

The key contributions of the proposed works is summarized as the following:
\begin{itemize}
    \item Help understand how the model fail by examining stability of the prediction and visual summarization of predictions;

    \item Enhance the visual encoding of the attention by overlaying sentence linguistic structure to allow grammar guided simplification;

    \item Enable the interrogation of the relationship between input sentences, attention, and the prediction by interactively exploring how the change in one place affect other parts of the model.

\end{itemize}
%
% In addition,

%\begin{itemize}
%    \item Study the classifier  vs. single instance evaluation
%    \item Model agnostic vs. reveal internal states
%    \item View model as invariant object vs. interrogate the model via partial/full updates
%\end{itemize}

%Questions such as, how stable predictions are concerning small perturbation in the input, how are these predictions arrived (i.e., the model may come up with the correct prediction by pickup unintended feature/alignment), and what are the often made mistakes, are all essential in systematic examination and evaluation of the model.
%%%%%%%%% accessing internal states are important %%%%%%%%%%%%


%%%%%%%%%% interrogate relationship between different components, modified different components %%%%%%%%%%%


% ######## Why use perturbation ###########
% Iterate the model design and debug the system hinged on the ability to quickly
% identify the errors made by a model.
%
% Perturbe the input is what NLP researchers have subconsciously been doing to study and test a model.
%
% Interpret/probe the relationship between attention and the prediction result

% On the one hand, recognize where the model fail is non-trivial. The standard evaluation approach (i.e., the prediction accuracy on the test set) provides limited information and cannot answer many important questions (e.g., how stable the prediction is, which part of the model may contribute to the failure) that are essential to the model diagnostic process. On the other hand, infer the cause of model failure is even more challenging. Neural network models are often referred to as black boxes due to the hard to recognize  difficult to interpret parameter, hypothesizing what happened in prediction process require the ability to peek inside the model and understand the intricate relationships among model inputs, critical internal mechanisms, as well the output predictions.
