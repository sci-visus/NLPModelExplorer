\firstsection{Introduction}
\maketitle

% (What is the high level motivation)
% What is the obsatcle for effectively apply machine learning model
With the recent advances in neural network based model, machine learning has gained unprecedented popularity, and its adaption permeated many fields of studies.
%
However, from researchers to practitioners, one often need to overcome many obstacles during the training, debugging, and tuning processes to unleash the full potential of these models.
%
Interpreting the internal mechanism and analyzing how predictions are made are critical to both the design and deployment of a given model.
More importantly, the ability to pinpoint where and how an error is made and come up with hypothesis for the cause of failure is the key to identify the limitation and improve upon existing models.
However, providing meaningful answers to these questions are extremely challenging and often regarded as impossible by many.

Recently, substantial research has been developed to combat the model interpretability challenge. Most notably, the works~\cite{} on interpretation of convolution neural network (CNN) model from both machine learning and visualization community has reveal the enormous potential and instant benefits of model interpretation, yet they also remind us how little we actually know regarding the inner mechanism of these deep neural network models.
%
Compare to image classification task, natural language processing systems have a number of innate restriction (e.g., the discrete nature of words) that makes interpreting them even more challenging. For example, \emph{feature visualization}~\cite{}, an often deployed technique for illustrating what kind of image feature a given part of the network (e.g., neuron, layer, or channel) captures, can not be easily generalized and applied for natural language due to the requirement of a continuous solution space (i.e., the image pixel values). Even though we can encode a word as an vector~\cite{}, the word embedding space is still a discretely defined space, where the interpolation between two vectors (i.e., two words) do not corresponds to explicit meaning.
%
In this work, we introduce a visualization system specific tailor to natural language model, specifical natural language inference, in which we address many domain specific challenges.

Many of the existing techniques study the model as an invariant object, where the model's behavior are recorded and analyzed in an offline fashion.
%
However, the exploration natural of these type of analysis often lead to \emph{what if ...} types of questions. What if I perturbed the current input? Will the prediction be stable? What if we change one of the key internal states of the model, how would the change affect the prediction. What if the current prediction is wrong, how and where should we change the model in order to produce the correct result? These type of dynamic queries is a natural way for us to gain understanding and come up with hypothesis of the model mechanisms by \emph{interrogating} the relationship between the input, internal states, and output prediction. Yet, we often need to tediously setup specialized experiment to obtain the answers.
%
In this work, we aim to provide quick and intuitive answers to these \emph{what if ...} questions. Instead of viewing the model as an invariant entitle, we employ a perturbation centric mentally to probe the model and examine how changes in one part of the pipeline affect others, which in term provide a new perspective to look at the model interpretation problem.





%\begin{itemize}
%    \item Study the classifier  vs. single instance evaluation
%    \item Model agnostic vs. reveal internal states
%    \item View model as invariant object vs. interrogate the model via partial/full updates
%\end{itemize}

%Questions such as, how stable predictions are concerning small perturbation in the input, how are these predictions arrived (i.e., the model may come up with the correct prediction by pickup unintended feature/alignment), and what are the often made mistakes, are all essential in systematic examination and evaluation of the model.
%%%%%%%%% accessing internal states are important %%%%%%%%%%%%


%%%%%%%%%% interrogate relationship between different components, modified different components %%%%%%%%%%%


% ######## Why use perturbation ###########
% Iterate the model design and debug the system hinged on the ability to quickly
% identify the errors made by a model.
%
% Perturbe the input is what NLP researchers have subconsciously been doing to study and test a model.
%
% Interpret/probe the relationship between attention and the prediction result

% On the one hand, recognize where the model fail is non-trivial. The standard evaluation approach (i.e., the prediction accuracy on the test set) provides limited information and cannot answer many important questions (e.g., how stable the prediction is, which part of the model may contribute to the failure) that are essential to the model diagnostic process. On the other hand, infer the cause of model failure is even more challenging. Neural network models are often referred to as black boxes due to the hard to recognize  difficult to interpret parameter, hypothesizing what happened in prediction process require the ability to peek inside the model and understand the intricate relationships among model inputs, critical internal mechanisms, as well the output predictions.

The key contributions of the proposed works is summarized as the following:
\begin{itemize}
    \item Identify the mistakes made by the model through perturbation of input sentences and visual summarization of the predictions;

    \item Enhance the visual representation of attention by overlaying sentence linguistic structure to allow grammar guided simplification;

    \item Enable the interrogation of the relationship between input sentences, attention, and the prediction by interactively exploring how the change in one of these elements affect other parts of the model.

\end{itemize}
