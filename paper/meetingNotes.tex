%%%%% Meeting notes %%%%%

Jan 11, 2018

Discussion topics:

- What we are doing: local perturbation

- are we building a "saliant map" in our mind about the sentence?
  (which part of the sentence is most sensitive to the prediction)
  Should we just build it directly

- What information is important from Tao's point of view

- Comparison to other local explaination techniques
    - our method reveal internal states (attention)

- Where does the interpretability come from
    - It is interpretable because the application (model is still a black box)
    - It is interpretable due to the inherent complexity of
      the model, sparsity linear model, rule list, dicision tree, etc.
