\section{Background}

Recently, with the wide adoption of long-short term memory
(LSTM) network and the introduction of attention mechanism,
neural network based model have
dominated nearly all linguistic tasks and thoroughtly refreshed the
baseline performances.
%
However, the disruptive advance also brings enormous challenges.
Due to the opaque nature of the neural network model, interpret and making sense
of many internal model mechanisms can be extremely challenging.
%
Many methods has been proprosed

The discrete nature of words and
sentences presents additional challenge for make sense of the model.
%

\subsection{Textual Entailment Problem}

\begin{itemize}
    \item what is the textaul entailment problem
    \item the importance of textual entailment problem
    \item how easily can the visualization method extends other NLP problem
\end{itemize}

Textual entailment is one of major natural language tasks, namely translation,
summarization, question

\subsection{Interprebilty Challenges}

The introduction of attention mechanism~\cite{} allow simpler models to match or even outperform more complex / deeper models by exploiting the alignment of bits of local text between sentences. The new challenges

Making sense and explaining predictions made by deep neural networks is not only essential for validating and improving the model, but also becoming a necessity with increasing demands for model accountability (e.g., what is the evidence for making the decision) and model fairness (e.g., is the prediction affected by the bias in the training data).
