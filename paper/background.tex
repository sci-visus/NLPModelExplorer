

\section{Background}

\begin{itemize}
    \item Motivation for TE: reading comprehension, similarity of senten ce
    \item what is it?
    \item its connection with other NLP tasks (one of the key)
    \item
\end{itemize}

The grand challenge in natural language processing (NLP) such as
machine reading require a deeper understanding the

% Recently, with the wide adoption of long-short term memory
% (LSTM) network and the introduction of attention mechanism,
% neural network based model have dominated nearly all linguistic tasks
% and thoroughtly refreshed many baseline performances.
% %
% However, the disruptive advance also brings enormous challenges.
% Netural network work based on has long been critizied for their opaque nature,
% and often been regarded as back box approach.
% Due to the opaque nature of the neural network model, interpret and making sense
% of many internal model mechanisms can be extremely challenging.

The discrete nature of words and sentences presents additional challenge for
interpreting the model, since many visualization technique often employed
for images rely on continues nature of the input space (e.g., one can interpolate
real values much easier than interplolate between words/sentences).
%

% \subsection{Natural Language Inference}


% \begin{itemize}
%     \item what is the textaul entailment problem
%     \item the importance of textual entailment problem
%     \item how easily can the visualization method extends other NLP problem
% \end{itemize}

Textual entailment is one of major natural language tasks, namely translation, summarization, question

% \subsection{Interprebilty Challenges}

The introduction of attention mechanism~\cite{} allow simpler models to match or even outperform more complex / deeper models by exploiting the alignment of bits of local text between sentences. The new challenges

Making sense and explaining predictions made by deep neural networks is not only essential for validating and improving the model, but also becoming a necessity with increasing demands for model accountability (e.g., what is the evidence for making the decision) and model fairness (e.g., is the prediction affected by the bias in the training data).
