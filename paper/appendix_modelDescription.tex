% \documentclass[journal]{vgtc}                % final (journal style)
\documentclass[review,journal]{vgtc}         % review (journal style)
% \documentclass[review,journal]{vgtc}         % review (journal style)
%\documentclass[widereview]{vgtc}             % wide-spaced review
%\documentclass[preprint,journal]{vgtc}       % preprint (journal style)

%% Uncomment one of the lines above depending on where your paper is
%% in the conference process. ``review'' and ``widereview'' are for review
%% submission, ``preprint'' is for pre-publication, and the final version
%% doesn't use a specific qualifier.

%% Please use one of the ``review'' options in combination with the
%% assigned online id (see below) ONLY if your paper uses a double blind
%% review process. Some conferences, like IEEE Vis and InfoVis, have NOT
%% in the past.

%% Please note that the use of figures other than the optional teaser is not permitted on the first page
%% of the journal version.  Figures should begin on the second page and be
%% in CMYK or Grey scale format, otherwise, colour shifting may occur
%% during the printing process.  Papers submitted with figures other than the optional teaser on the
%% first page will be refused. Also, the teaser figure should only have the
%% width of the abstract as the template enforces it.

%% These few lines make a distinction between latex and pdflatex calls and they
%% bring in essential packages for graphics and font handling.
%% Note that due to the \DeclareGraphicsExtensions{} call it is no longer necessary
%% to provide the the path and extension of a graphics file:
%% \includegraphics{diamondrule} is completely sufficient.
%%
\ifpdf%                                % if we use pdflatex
  \pdfoutput=1\relax                   % create PDFs from pdfLaTeX
  \pdfcompresslevel=9                  % PDF Compression
  \pdfoptionpdfminorversion=7          % create PDF 1.7
  \ExecuteOptions{pdftex}
  \usepackage{graphicx}                % allow us to embed graphics files
  \DeclareGraphicsExtensions{.pdf,.png,.jpg,.jpeg} % for pdflatex we expect .pdf, .png, or .jpg files
\else%                                 % else we use pure latex
  \ExecuteOptions{dvips}
  \usepackage{graphicx}                % allow us to embed graphics files
  \DeclareGraphicsExtensions{.eps}     % for pure latex we expect eps files
\fi%

%% it is recomended to use ``\autoref{sec:bla}'' instead of ``Fig.~\ref{sec:bla}''
\graphicspath{{figures/}{pictures/}{figs/}{./}} % where to search for the images

\usepackage{microtype}                 % use micro-typography (slightly more compact, better to read)
\PassOptionsToPackage{warn}{textcomp}  % to address font issues with \textrightarrow
\usepackage{textcomp}                  % use better special symbols
\usepackage{mathptmx}                  % use matching math font
\usepackage{times}                     % we use Times as the main font
\renewcommand*\ttdefault{txtt}         % a nicer typewriter font
\usepackage{cite}                      % needed to automatically sort the references
\usepackage{tabu}                      % only used for the table example
\usepackage{booktabs}                  % only used for the table example
\usepackage{amsmath,bm}
%% We encourage the use of mathptmx for consistent usage of times font
%% throughout the proceedings. However, if you encounter conflicts
%% with other math-related packages, you may want to disable it.

%% In preprint mode you may define your own headline.
%\preprinttext{To appear in IEEE Transactions on Visualization and Computer Graphics.}

\usepackage[pdftex,dvipsnames]{xcolor}  % Coloured text etc.
\usepackage{listings}

\newcommand{\taoli}[1]{\textcolor{orange}{[#1]}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    %breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\newcommand{\zhimin}[1]{\textcolor{magenta}{[ZHIMIN: #1]}}
\newcommand{\shusen}[1]{\textcolor{red}{[#1]}}

\usepackage{tikz}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 

%% If you are submitting a paper to a conference for review with a double
%% blind reviewing process, please replace the value ``0'' below with your
%% OnlineID. Otherwise, you may safely leave it at ``0''.
\onlineid{1095}

%% declare the category of your paper, only shown in review mode
\vgtccategory{Research}
%% please declare the paper type of your paper to help reviewers, only shown in review mode
%% choices:
%% * algorithm/technique
%% * application/design study
%% * evaluation
%% * system
%% * theory/model
\vgtcpapertype{Application}

%% Paper title.
\title{
Supplementary Materials
%Where and How Do Models Fail? A Visual Exploration Tool for Analyzing and Interpreting Natural Language Inference Models
}
\vgtcinsertpkg

\begin{document}
\firstsection{Appendix A: Detailed description of the Model}
\maketitle

The Decomposable Attention~\cite{parikh2016emnlp} model is an end-to-end model that takes vectorized premise sentence and hypothesis sentence, and output a probability distribution over three labels. The pipeline can be separated into three phases: encoder, attention and classifier. Here we briefly formalize these three layers.

\subsection{Encoder}
Denote the premise sentence as $\overline{\boldsymbol{P}} = (\overline{\boldsymbol{p}}_1, \overline{\boldsymbol{p}}_2, ..., \overline{\boldsymbol{p}}_m)$ and hypothesis sentence as $\overline{\boldsymbol{H}} = (\overline{\boldsymbol{h}}_1, \overline{\boldsymbol{h}}_2, ..., \overline{\boldsymbol{h}}_n)$ where $\overline{\boldsymbol{p}}_i$ and $\overline{\boldsymbol{h}}_j$ are the input embedding vectors respectively. The purpose of encoder is to generate specialized encodings for attention. Define the encoder function as
\begin{align}
  \boldsymbol{P} &= F(\overline{\boldsymbol{P}})\\
  \boldsymbol{H} &= F(\overline{\boldsymbol{H}})
\end{align}

where $F$ function is the encoder layer (e.g. a context-free two-layer ReLU). Let $\boldsymbol{P}$ denote the output premise encoding of size $m \times d$, and $\boldsymbol{H}$ the hypothesis encoding of size $n \times d$ where $d$ is the encoding size.

\subsection{Attention} \label{sec:att}
The job of attention layer is to capture pairwise interaction between encodings of premise and hypothesis. First we compute a similarity score matrix $\boldsymbol{S}$ ($m \times n$) by the following
\begin{align}
  S_{ij} &= \boldsymbol{p}_i ^T \boldsymbol{h}_j
\end{align}

Then attention weights are obtained by normalizing $\boldsymbol{S}$ bidirectionally. We will use $\leftarrow$ and $\rightarrow$ to differentiate the notation. That is, for premise token $\boldsymbol{p}_i$, a probability distribution over all $\boldsymbol{h}_j$`s are done as
\begin{align}
  \overleftarrow{\boldsymbol{a}}_i &= \frac{exp(S_{ij})}{\sum_{j \in h} exp(S_{ij})}
\end{align}

where $\overleftarrow{\boldsymbol{a}}_i$ is of dimension $1 \times n$ naturally. Similarly, for every $\boldsymbol{h}_j$, there is a probability distribution over all premise tokens.
\begin{align}
  \overrightarrow{\boldsymbol{a}}_j &= \frac{exp(S_{ij})}{\sum_{i \in p} exp(S_{ij})} \label{eq:aright}
\end{align}

where $\overrightarrow{\boldsymbol{a}}_j$ is a $1 \times m$ dimension. In case a token is not correlated with any other tokens, the model added an extra artificial token to both premise and hypothesis to allow a word aligning with null.

\subsection{Classifier} \label{sec:cls}
Classifier takes features from previous steps and predict a probability distribution over three labels. First, it aggregates features following the bidirectional attentions as
\begin{align}
  \boldsymbol{u}_i &= G([\overleftarrow{\boldsymbol{a}}_i \cdot \boldsymbol{H} \hspace{1ex} | \hspace{1ex}  \overline{\boldsymbol{p}}_i]) \\
  \boldsymbol{v}_j &= G([\overrightarrow{\boldsymbol{a}}_j \cdot \boldsymbol{P} \hspace{1ex}  | \hspace{1ex}  \overline{\boldsymbol{h}}_j])
\end{align}

where G is the aggregation layer (e.g. two-layer ReLU) and $[\cdot|\cdot]$ denotes concatenation. With the aggregated premise feature $\boldsymbol{u}_i$ and hypothesis feature $\boldsymbol{v}_j$, the classifier does further feature conjunction to predict weight on each entailment score:
\begin{align}
  \boldsymbol{y} &= Z([\sum_i \boldsymbol{u}_i | \sum_j \boldsymbol{v}_j]) \label{eq:yscore}
\end{align}

where $Z$ is another neural layer (e.g. two-layer ReLU followed by a linear layer). For training, the scores $\boldsymbol{y}$ are connected to multi-class cross entropy loss function. For inference, the predicted label is $\arg\max_i y_i$.

%\taoli{Do we need citation in Appendix?}

\textbf{Comment} We adopted the same network setting in the Decomposable Attention paper~\cite{parikh2016emnlp}. That are, network hidden size is $200$; input word embeddings are $200$d embeddings (downsampled from $300$d GloVe embeddings~\cite{PenningtonSocherManning2014} by a learnable linear layer). For learning, dropout~\cite{SrivastavaHinton2014} is $0.2$ and learning rate is $0.05$. Optimization algorithm is AdaGrad~\cite{duchi2011adaptive} with mini-batch size 64 and number of epochs 300.

\section{Appendix B: Code Example: Link Visualization with Model}
\begin{lstlisting}[language=Python, caption=Code for generating the visualization.]
from visPackage import *
from modelInterface import *
from sentenceGenerator import *

#initialize NLP model
model = modelInterface(
    wordDict="../data/snli_1.0/snli_1.0.word.dict",
    wordVec="../data/glove.hdf5", 
    model="../data/local_300_parikh")

#sentence perturbation
gen = sentenceGenerator()
#dependency parsing
dep = dependencyTree()

#define visualization components
visLayout = {
    "column":[{"row":["Selector", 
                      "Sentence", 
                      "Pipeline"]},
            {"row":["AttentionGraph", 
                    "AttentionMatrix", 
                    "Prediction"]}]
    }

#create vis interface
vis = textEntailVisModule(visLayout)

#setup model callbacks
vis.setPredictionHook(model.predict)
vis.setAttentionHook(model.attention)
vis.setPredictionUpdateHook(model.updatePrediction)
vis.setAttentionUpdateHook(model.updateAttention)
vis.setReloadModelCallback(model.reloadModel)
vis.setPipelineStatisticHook(model.pipelineStatistic)

#setup perturbation and grammar parsing callbacks
vis.setSentencePerturbationHook(gen.perturbSentence)
vis.setSentenceParserHook(dep.getDependencyTree)

#open browser to show visualization
modelVis.show()

\end{lstlisting}
\bibliographystyle{abbrv-doi}
%\bibliographystyle{abbrv-doi-narrow}
%\bibliographystyle{abbrv-doi-hyperref}
%\bibliographystyle{abbrv-doi-hyperref-narrow}

\bibliography{entailVis.bib}
\end{document}