\section{Application Scenarios}
\label{sec:caseStudy}
% \shusen{A full case study that driven by the visualization task and the question associated with them}
%In this section, we discuss application scenarios, in which the domain experts utilize the 
To better illustrate how the proposed perturbation-driven exploration tool helps researchers interpret the neural network model, we present five application scenarios gathered by the domain experts who integrated the proposed tool into their analysis workflow.

\begin{figure}[htbp]
\centering
%\vspace{-2mm}
 \includegraphics[width=1.0\linewidth]{predictStability}
 \caption{
Prediction stability assessment. In (a)(b)(c), we estimate the overall prediction stability (regarding synonymous perturbation) for each type prediction over the entire development set (10k examples). The user can drill down to individual examples by filtering via the histogram and scatterplot (see Fig.~\ref{fig:summaryView}) for a case by case exploration, as illustrated in (d)(e). For highly unstable outliers, we often observe the prediction of the original sentences near the decision boundary (e.g., the yellow circle in (f) corresponds to a entailment prediction that is very close to neural), however, some predictions, such as the one illustrated in (d) can alter the prediction quite drastically with minor perturbation (illustrated in $d_1$, where the word ``pile'' is replace by ``heap'' in the hypothesis sentence).
%
}
\label{fig:predictStability}
\end{figure}

\subsection{Scenario 1: Assess the Model Prediction Stability}
The robustness of the prediction is often hard to evaluate, however, they provide valuable information for the researchers to better understand the model.
%
In the proposed work, we approach the prediction robustness from sensitivity analysis point of view. The stability of the prediction is measured by how often the predicted labels are altered after small perturbations are applied to the input. 
%
Compare to other types of input (e.g., image), perturbation of the natural language can be particular tricky, as small alteration of words can drastically change the meaning of the sentence. As discussed in Section~\ref{sec:sentence}, we try to maintain semantic of the sentence by only replace words with their synonymous and only replace one word for each pair.
As illustrated in Fig.~\ref{fig:predictStability}, by utilizing the proposed tool, the domain expert can not only examine visual summary of the stability, but also quickly dive into individual examples for case by case analysis.


In Fig.~\ref{fig:predictStability}(a)(b)(c),  %by viewing the distribution of the stability in the histogram
we compare the overall prediction stability (regarding synonymous perturbation) for each type prediction over the entire development set (10k examples).
%
We observe a drastic difference between the stability for entailment predictions compare to the contradiction and neutral ones.
%
Such a distinction can be partially explained by how entailment relationship is defined. As discussed in the Section~\ref{sec:languageInference}, the relationship is only valid if the concept in premise is more specific than the concept in hypothesis (therefore, you can infer the hypothesis from the premise, but not other way around). This means, we may change the entailment relationships simply by replacing nouns and verb by synonymous, whereas, the same does not apply for neutral and contradiction.
%
The inherent sensitivity difference may motivate domain experts designing dedicate model mechanism to address the such a disparity.

Beside presenting the summary view, the tool also allow user to quickly narrow down the select to single example  by filtering through the histogram and scatterplot (see Fig.~\ref{fig:summaryView}). 
%
Through exploring multiple samples (in the entailment category, E/E in Fig.~\ref{fig:summaryView}(a)), the domain experts noticed many highly unstable outliers are from sentence pairs where the prediction is near the decision boundary (see Fig.~\ref{fig:predictStability}(e), the yellow circle corresponds to a \emph{entailment} prediction that is very close to \emph{neutral}). 
However, some predictions, such as the one illustrated in Fig.~ref{fig:predictStability}(d) can alter the prediction quite drastically with minor perturbation (illustrated in $d_1$, where the word ``heap'' is replace by ``pile'' in the hypothesis sentence. We will try to understand the what is happened inside the model in the following section (see Fig.~\ref{fig:att2pred}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% perturb input, attention
\subsection{Scenario 2: Examine the Decision Making Process}
The predicted label alone provide limited information, often the domain experts want to know how does the model arrive at the conclusion. And if the prediction is incorrect where does the error occur.
%
Examining the decision making process is not only instrumental for evaluating the model performance but also essential for hypothesizing improvement strategies for developing future models.
%
In the NLI model, the three stages (encoder, attention, classifier) work in synergy to produce the prediction.
Therefore, making sense of the prediction involving understand how different part of the model affect the final prediction.

In the previous section, we have noticed some very unusual behavior, where a minor perturbation of the text from \emph{pile} to \emph{heap} lead to drastically change in the prediction (Fig.~\ref{fig:predictStability}(d)). Here, we want to make sense of what lead to the failed prediction. In this example, the premise \textbf{P} is ``A very young child in a red plaid coat and pink winter hat makes a snowball in a large \textbf{pile} of snow.'', and the original hypothesis \textbf{H1} is ``A child in a red plaid coat and pink winter hat makes a snowball in a large \textbf{heap} of snow.''. The perturbed hypothesis \textbf{H2} is ``A child in a red plaid coat and pink winter hat makes a snowball in a large \textbf{pile} of snow.''

As illustrated in Fig.~\ref{fig:att2pred}(a), based on the graph attention visualization, we can see the for the (\textbf{P}, \textbf{H2}) pair, the word \textbf{pile} and \textbf{heap} is not well-aligned (see Fig.~\ref{fig:att2pred}(a)). 
%
To verify whether the alignment is what contribute to the mis-classification, we can utilize the attention editing functionality in the matrix view (Fig.~\ref{fig:attentionView}) to make the word \textbf{pile} align correctly with \textbf{heap} (shown in Fig.~\ref{fig:att2pred}(b)). 
%
In the update prediction view (see Fig.~\ref{fig:att2pred} (d)), we can see the new attention moved the original prediction from neutral toward the entailment (which is the true label) and straddle on the class boundary. This is very interesting, as showed in Fig~\ref{fig:predictStability}(d), the original pair (\textbf{P}, \textbf{H1}) has almost identify alignment (not shown in the figure). This mean is the model do not strongly believe ``\textbf{pile} of snow'' and ``\textbf{heap} snow'' has the same meaning.

\begin{figure}[htbp]
\centering
\vspace{-2mm}
 \includegraphics[width=1.0\linewidth]{att2pred}
 \caption{
Editing the original attention (a) to correctly align the word "heap" with "pile" as shown in (b) (these two words are highlight in orange).  
The change of attention lead to change of prediction from neutral to the class boundary between neutral and entialment.
%
}
\label{fig:att2pred}
\end{figure}

%%%% are we obtain correct prediction from wrong alignment %%%%%
%By looking into the model decision making process, we can assess whether the model is behaving as intended. For the attention based NLI model, one underlying assumption is that the attention should capture the correct alignment between words in the premise and hypothesis sentence for the classifier to make an informed/correct decision. However, since there is only three output labels, it is also likely in some cases the model produce correct predictions due to pure luck with attention that does not make sense (i.e., the important words are mis-align). It is also possible that even the alignment is perfect, the model still generate a wrong prediction.


%%%% hypothesis on which part of model is wrong? %%%%%




\begin{figure*}[t]
\centering
\vspace{-2mm}
 \includegraphics[width=0.9\linewidth]{failledEncoding}
 \caption{
The prediction is failed due to incorrect alignment. For all the failed case, 
 }
\label{fig:failedEncoding}
\end{figure*}

%One of the essential task for can be made in either of these stages.
% \shusen{difference in sensitivity among entail natural and contradict relationships}
% the generate the correct prediction for the wrong reason


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Scenario 3: What Does It Take to Correct a Wrong Prediction?}
Once, we get a sense of how prediction are being made, where it went wrong. The next obvious questions are what does it take to correct the wrong prediction. 
In addition, when we interpreting how decision are made, we always fix the model and make forward pass in the neural network.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Scenario 4: Explore Relationship Between Grammar and Attention}
There is no simple answer to what exactly is the information attention captures.
Can attention capture grammar structure? Is 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Scenario 5: Handcraft Example Exploration}
To test the limitation of the model, domain experts often handcraft ``extreme'' examples (as such the Facebook IPO example discussed in Section~\ref{sec:languageInference}) that they know most model will have a difficult time making correct inference. 
%
%The exploration is just started once the prediction result is examined, 
Often the researchers have a set of experiments they plan test, from which they likely develop new hypothesis for further experiments.
%
In some way, we can think of such a process as a nature blend of all previously discussed scenarios. However,  instead with specific analysis goal in mind, the domain experts focus on probing around and see if they can find anything interesting or out-of-ordinary behaviors in the model.
%
Having an environment, in which the experts can freely modify the words or attention then observe the corresponding changes in other parts of the model pipeline, ensures an streamlined exploration experience that free the users from interruption and tedious grunt work, allow them to focus on more productive activity. 

%\subsection{Is the Prediction Stable?}
%\subsection{Where Are the Mistakes?}
%\subsection{How Attention Affect the Prediction?}
%\subsection{What Does It Take to Change the Prediction?}
%\subsection{Is Attention All You Need?}
