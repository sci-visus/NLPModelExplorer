How to interpret classification task
\begin{itemize}
    \item What feature / dimension / part-of-the-input should change to
    alter the outcome
    \item What Examples can you provide that is similar to the current
    instance

\end{itemize}

Related works
\begin{itemize}
    \item Interacting with Predictions: Visual Inspection of Black-box Machine Learning Models,
    Use partial dependency (varying along one feature and observer how prediction changes)
    \item “Why Should I Trust You?” Explaining the Predictions of Any Classifier (LIME)

\end{itemize}
