\section{Related Works}

\begin{itemize}
    \item Depth learning vis
    \item Vis
\end{itemize}

How to interpret classification task
\begin{itemize}
    \item What feature / dimension / part-of-the-input should change to
    alter the outcome
    \item What Examples can you provide that is similar to the current
    instance

\end{itemize}

(Limitation of the state-of-the arts)
Model independent explanations is useful to gain intuition, but they lack the ability to link the external observation with inner states/mechanism of the model.


Papers:
\begin{itemize}
    \item Interacting with Predictions: Visual Inspection of Black-box Machine Learning Models,
    Use partial dependency (varying along one feature and observer how prediction changes)
    \item “Why Should I Trust You?” Explaining the Predictions of Any Classifier (LIME)

\end{itemize}
