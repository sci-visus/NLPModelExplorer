loading word vector from ../../learnlab/data/glove.hdf5
loading data from ../../learnlab/data/snli_1.0-train.hdf5
loading data from ../../learnlab/data/snli_1.0-val.hdf5
Epoch: 1, Batch: 1000/9683, Batch size: 2, LR: 0.0500, Loss: 1.0989, Acc: 0.3371, Speed: 302.9k, Time: 4.6195
Epoch: 1, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 1.0967, Acc: 0.3482, Speed: 306.8k, Time: 9.0765
Epoch: 1, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 1.0940, Acc: 0.3564, Speed: 303.2k, Time: 13.5966
Epoch: 1, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 1.0919, Acc: 0.3650, Speed: 304.4k, Time: 18.0472
Epoch: 1, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 1.0900, Acc: 0.3717, Speed: 304.3k, Time: 22.6037
Epoch: 1, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 1.0887, Acc: 0.3749, Speed: 304.0k, Time: 27.2201
Epoch: 1, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 1.0880, Acc: 0.3775, Speed: 303.7k, Time: 31.6937
Epoch: 1, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 1.0870, Acc: 0.3798, Speed: 304.8k, Time: 36.1816
Epoch: 1, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 1.0864, Acc: 0.3814, Speed: 305.3k, Time: 40.6090
Train 0.3829
Val 0.4108
1	0.382903	0.410832

saving model to local_200_parikh.pt
Epoch: 2, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 1.0792, Acc: 0.3999, Speed: 312.4k, Time: 4.3889
Epoch: 2, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 1.0741, Acc: 0.4076, Speed: 310.7k, Time: 8.7859
Epoch: 2, Batch: 3000/9683, Batch size: 1, LR: 0.0500, Loss: 1.0623, Acc: 0.4216, Speed: 309.9k, Time: 13.2070
Epoch: 2, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 1.0535, Acc: 0.4317, Speed: 314.9k, Time: 17.4166
Epoch: 2, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 1.0483, Acc: 0.4377, Speed: 313.7k, Time: 21.7903
Epoch: 2, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 1.0432, Acc: 0.4424, Speed: 314.5k, Time: 26.2479
Epoch: 2, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 1.0389, Acc: 0.4472, Speed: 314.5k, Time: 30.6098
Epoch: 2, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 1.0353, Acc: 0.4507, Speed: 315.1k, Time: 34.9667
Epoch: 2, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 1.0325, Acc: 0.4538, Speed: 311.8k, Time: 39.7682
Train 0.4561
Val 0.4894
1	0.382903	0.410832
2	0.456146	0.489381

saving model to local_200_parikh.pt
Epoch: 3, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 1.0039, Acc: 0.4824, Speed: 310.4k, Time: 4.4340
Epoch: 3, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.9996, Acc: 0.4874, Speed: 311.0k, Time: 8.8499
Epoch: 3, Batch: 3000/9683, Batch size: 1, LR: 0.0500, Loss: 0.9992, Acc: 0.4885, Speed: 309.7k, Time: 13.3027
Epoch: 3, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.9979, Acc: 0.4904, Speed: 306.0k, Time: 17.9499
Epoch: 3, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.9971, Acc: 0.4920, Speed: 307.7k, Time: 22.3389
Epoch: 3, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.9958, Acc: 0.4937, Speed: 307.8k, Time: 26.7530
Epoch: 3, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.9956, Acc: 0.4947, Speed: 306.5k, Time: 31.2890
Epoch: 3, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.9948, Acc: 0.4961, Speed: 308.9k, Time: 35.6113
Epoch: 3, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.9939, Acc: 0.4975, Speed: 310.5k, Time: 39.9393
Train 0.4986
Val 0.5151
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090

saving model to local_200_parikh.pt
Epoch: 4, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.9888, Acc: 0.5057, Speed: 318.9k, Time: 4.4224
Epoch: 4, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.9835, Acc: 0.5130, Speed: 320.1k, Time: 8.6733
Epoch: 4, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.9802, Acc: 0.5175, Speed: 317.2k, Time: 13.0483
Epoch: 4, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.9730, Acc: 0.5249, Speed: 319.5k, Time: 17.3126
Epoch: 4, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.9659, Acc: 0.5307, Speed: 319.5k, Time: 21.5911
Epoch: 4, Batch: 6000/9683, Batch size: 36, LR: 0.0500, Loss: 0.9586, Acc: 0.5367, Speed: 319.1k, Time: 25.8976
Epoch: 4, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.9529, Acc: 0.5409, Speed: 319.2k, Time: 30.2205
Epoch: 4, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.9485, Acc: 0.5443, Speed: 319.2k, Time: 34.5555
Epoch: 4, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.9436, Acc: 0.5477, Speed: 318.5k, Time: 38.9348
Train 0.5496
Val 0.5849
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900

saving model to local_200_parikh.pt
Epoch: 5, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.9015, Acc: 0.5745, Speed: 311.0k, Time: 4.3776
Epoch: 5, Batch: 2000/9683, Batch size: 4, LR: 0.0500, Loss: 0.8983, Acc: 0.5773, Speed: 320.9k, Time: 8.5543
Epoch: 5, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8957, Acc: 0.5794, Speed: 317.3k, Time: 12.9532
Epoch: 5, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8938, Acc: 0.5812, Speed: 314.5k, Time: 17.4168
Epoch: 5, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8931, Acc: 0.5814, Speed: 308.9k, Time: 22.1758
Epoch: 5, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8918, Acc: 0.5823, Speed: 307.6k, Time: 26.8171
Epoch: 5, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8907, Acc: 0.5831, Speed: 305.5k, Time: 31.5590
Epoch: 5, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8894, Acc: 0.5839, Speed: 303.6k, Time: 36.2810
Epoch: 5, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8879, Acc: 0.5850, Speed: 301.6k, Time: 41.1213
Train 0.5856
Val 0.6084
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373

saving model to local_200_parikh.pt
Epoch: 6, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8719, Acc: 0.5963, Speed: 273.7k, Time: 4.9183
Epoch: 6, Batch: 2000/9683, Batch size: 1, LR: 0.0500, Loss: 0.8719, Acc: 0.5966, Speed: 282.5k, Time: 9.6882
Epoch: 6, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8722, Acc: 0.5960, Speed: 287.1k, Time: 14.2457
Epoch: 6, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8725, Acc: 0.5962, Speed: 293.7k, Time: 18.6259
Epoch: 6, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8718, Acc: 0.5965, Speed: 293.5k, Time: 23.2899
Epoch: 6, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8709, Acc: 0.5973, Speed: 296.9k, Time: 27.7024
Epoch: 6, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8713, Acc: 0.5969, Speed: 296.7k, Time: 32.3410
Epoch: 6, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8701, Acc: 0.5974, Speed: 298.6k, Time: 36.8596
Epoch: 6, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8693, Acc: 0.5981, Speed: 298.1k, Time: 41.5744
Train 0.5988
Val 0.6225
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498

saving model to local_200_parikh.pt
Epoch: 7, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8548, Acc: 0.6060, Speed: 294.4k, Time: 4.8058
Epoch: 7, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8547, Acc: 0.6081, Speed: 304.1k, Time: 9.1268
Epoch: 7, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8538, Acc: 0.6093, Speed: 299.2k, Time: 13.9211
Epoch: 7, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8524, Acc: 0.6109, Speed: 301.9k, Time: 18.4290
Epoch: 7, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8524, Acc: 0.6109, Speed: 299.5k, Time: 23.2317
Epoch: 7, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8513, Acc: 0.6118, Speed: 302.0k, Time: 27.6235
Epoch: 7, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8508, Acc: 0.6122, Speed: 301.9k, Time: 32.1525
Epoch: 7, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8508, Acc: 0.6123, Speed: 301.5k, Time: 36.7381
Epoch: 7, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8504, Acc: 0.6125, Speed: 300.6k, Time: 41.3406
Train 0.6130
Val 0.6327
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659

saving model to local_200_parikh.pt
Epoch: 8, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8451, Acc: 0.6186, Speed: 291.3k, Time: 4.7544
Epoch: 8, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8420, Acc: 0.6197, Speed: 286.1k, Time: 9.6513
Epoch: 8, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8386, Acc: 0.6217, Speed: 293.2k, Time: 14.0246
Epoch: 8, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8377, Acc: 0.6222, Speed: 298.9k, Time: 18.3352
Epoch: 8, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8353, Acc: 0.6231, Speed: 297.5k, Time: 23.0870
Epoch: 8, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8335, Acc: 0.6246, Speed: 296.9k, Time: 27.7683
Epoch: 8, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8319, Acc: 0.6254, Speed: 299.0k, Time: 32.1418
Epoch: 8, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8301, Acc: 0.6263, Speed: 298.4k, Time: 36.8164
Epoch: 8, Batch: 9000/9683, Batch size: 4, LR: 0.0500, Loss: 0.8283, Acc: 0.6274, Speed: 297.3k, Time: 41.7452
Train 0.6278
Val 0.6567
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742

saving model to local_200_parikh.pt
Epoch: 9, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8139, Acc: 0.6368, Speed: 281.2k, Time: 4.9421
Epoch: 9, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8133, Acc: 0.6374, Speed: 284.3k, Time: 9.7166
Epoch: 9, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8119, Acc: 0.6380, Speed: 289.1k, Time: 14.3509
Epoch: 9, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8119, Acc: 0.6379, Speed: 293.5k, Time: 18.8441
Epoch: 9, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8117, Acc: 0.6379, Speed: 294.0k, Time: 23.4901
Epoch: 9, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8103, Acc: 0.6388, Speed: 291.4k, Time: 28.4010
Epoch: 9, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8091, Acc: 0.6394, Speed: 295.0k, Time: 32.7241
Epoch: 9, Batch: 8000/9683, Batch size: 3, LR: 0.0500, Loss: 0.8085, Acc: 0.6398, Speed: 293.2k, Time: 37.6527
Epoch: 9, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.8077, Acc: 0.6405, Speed: 291.0k, Time: 42.6011
Train 0.6405
Val 0.6689
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936

saving model to local_200_parikh.pt
Epoch: 10, Batch: 1000/9683, Batch size: 1, LR: 0.0500, Loss: 0.7936, Acc: 0.6499, Speed: 317.7k, Time: 4.3904
Epoch: 10, Batch: 2000/9683, Batch size: 23, LR: 0.0500, Loss: 0.7921, Acc: 0.6508, Speed: 316.4k, Time: 8.6799
Epoch: 10, Batch: 3000/9683, Batch size: 6, LR: 0.0500, Loss: 0.7907, Acc: 0.6515, Speed: 317.6k, Time: 12.9561
Epoch: 10, Batch: 4000/9683, Batch size: 17, LR: 0.0500, Loss: 0.7901, Acc: 0.6517, Speed: 318.1k, Time: 17.1856
Epoch: 10, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7882, Acc: 0.6526, Speed: 316.9k, Time: 21.6272
Epoch: 10, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7854, Acc: 0.6544, Speed: 316.6k, Time: 26.0270
Epoch: 10, Batch: 7000/9683, Batch size: 2, LR: 0.0500, Loss: 0.7838, Acc: 0.6554, Speed: 316.0k, Time: 30.3526
Epoch: 10, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7822, Acc: 0.6564, Speed: 316.8k, Time: 34.6556
Epoch: 10, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7799, Acc: 0.6577, Speed: 318.7k, Time: 38.8407
Train 0.6586
Val 0.7002
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234

saving model to local_200_parikh.pt
Epoch: 11, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7538, Acc: 0.6743, Speed: 319.8k, Time: 4.3617
Epoch: 11, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7532, Acc: 0.6743, Speed: 323.9k, Time: 8.5921
Epoch: 11, Batch: 3000/9683, Batch size: 1, LR: 0.0500, Loss: 0.7536, Acc: 0.6742, Speed: 324.5k, Time: 12.7734
Epoch: 11, Batch: 4000/9683, Batch size: 31, LR: 0.0500, Loss: 0.7519, Acc: 0.6746, Speed: 322.5k, Time: 17.1370
Epoch: 11, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7494, Acc: 0.6761, Speed: 322.4k, Time: 21.3987
Epoch: 11, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7490, Acc: 0.6764, Speed: 322.4k, Time: 25.6924
Epoch: 11, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7476, Acc: 0.6771, Speed: 318.5k, Time: 30.3268
Epoch: 11, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7478, Acc: 0.6769, Speed: 318.6k, Time: 34.6485
Epoch: 11, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7471, Acc: 0.6772, Speed: 318.2k, Time: 39.0016
Train 0.6777
Val 0.7091
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074

saving model to local_200_parikh.pt
Epoch: 12, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7266, Acc: 0.6891, Speed: 320.7k, Time: 4.3781
Epoch: 12, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7285, Acc: 0.6874, Speed: 321.0k, Time: 8.6877
Epoch: 12, Batch: 3000/9683, Batch size: 1, LR: 0.0500, Loss: 0.7268, Acc: 0.6885, Speed: 318.0k, Time: 13.0560
Epoch: 12, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7267, Acc: 0.6887, Speed: 316.3k, Time: 17.4204
Epoch: 12, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7261, Acc: 0.6890, Speed: 314.9k, Time: 21.7607
Epoch: 12, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7262, Acc: 0.6894, Speed: 316.2k, Time: 26.1169
Epoch: 12, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7242, Acc: 0.6901, Speed: 317.3k, Time: 30.4434
Epoch: 12, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7230, Acc: 0.6906, Speed: 317.0k, Time: 34.8100
Epoch: 12, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7225, Acc: 0.6911, Speed: 317.4k, Time: 39.0906
Train 0.6914
Val 0.7277
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670

saving model to local_200_parikh.pt
Epoch: 13, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7045, Acc: 0.7007, Speed: 315.8k, Time: 4.4187
Epoch: 13, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7028, Acc: 0.7029, Speed: 316.0k, Time: 8.7507
Epoch: 13, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.7006, Acc: 0.7047, Speed: 318.5k, Time: 13.0633
Epoch: 13, Batch: 4000/9683, Batch size: 12, LR: 0.0500, Loss: 0.6990, Acc: 0.7060, Speed: 318.0k, Time: 17.3309
Epoch: 13, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6984, Acc: 0.7061, Speed: 317.5k, Time: 21.6879
Epoch: 13, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6980, Acc: 0.7062, Speed: 317.4k, Time: 25.9580
Epoch: 13, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6969, Acc: 0.7068, Speed: 312.6k, Time: 30.7627
Epoch: 13, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6967, Acc: 0.7068, Speed: 309.3k, Time: 35.6824
Epoch: 13, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6957, Acc: 0.7075, Speed: 305.7k, Time: 40.6089
Train 0.7080
Val 0.7367
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714

saving model to local_200_parikh.pt
Epoch: 14, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6783, Acc: 0.7182, Speed: 305.1k, Time: 4.5072
Epoch: 14, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6825, Acc: 0.7153, Speed: 311.8k, Time: 8.7540
Epoch: 14, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6811, Acc: 0.7162, Speed: 311.8k, Time: 13.1331
Epoch: 14, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6823, Acc: 0.7152, Speed: 313.2k, Time: 17.4410
Epoch: 14, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6809, Acc: 0.7159, Speed: 315.0k, Time: 21.7111
Epoch: 14, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6798, Acc: 0.7160, Speed: 316.2k, Time: 26.0517
Epoch: 14, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6786, Acc: 0.7167, Speed: 317.2k, Time: 30.3806
Epoch: 14, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6772, Acc: 0.7175, Speed: 317.2k, Time: 34.6846
Epoch: 14, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6761, Acc: 0.7180, Speed: 317.3k, Time: 39.0126
Train 0.7184
Val 0.7444
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437

saving model to local_200_parikh.pt
Epoch: 15, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6696, Acc: 0.7198, Speed: 321.0k, Time: 4.3279
Epoch: 15, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6641, Acc: 0.7230, Speed: 327.2k, Time: 8.4859
Epoch: 15, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6636, Acc: 0.7237, Speed: 331.7k, Time: 12.5288
Epoch: 15, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6610, Acc: 0.7253, Speed: 329.9k, Time: 16.8287
Epoch: 15, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6608, Acc: 0.7256, Speed: 327.5k, Time: 21.2051
Epoch: 15, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6603, Acc: 0.7258, Speed: 325.0k, Time: 25.4951
Epoch: 15, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6592, Acc: 0.7263, Speed: 324.0k, Time: 29.8080
Epoch: 15, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6586, Acc: 0.7269, Speed: 323.4k, Time: 34.0936
Epoch: 15, Batch: 9000/9683, Batch size: 2, LR: 0.0500, Loss: 0.6587, Acc: 0.7272, Speed: 322.6k, Time: 38.4449
Train 0.7270
Val 0.7559
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919

saving model to local_200_parikh.pt
Epoch: 16, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6443, Acc: 0.7355, Speed: 322.4k, Time: 4.3597
Epoch: 16, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6477, Acc: 0.7340, Speed: 316.1k, Time: 8.7358
Epoch: 16, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6472, Acc: 0.7343, Speed: 316.7k, Time: 13.1181
Epoch: 16, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6468, Acc: 0.7341, Speed: 316.4k, Time: 17.4625
Epoch: 16, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6474, Acc: 0.7337, Speed: 317.5k, Time: 21.7887
Epoch: 16, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6467, Acc: 0.7341, Speed: 318.1k, Time: 26.1515
Epoch: 16, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6456, Acc: 0.7346, Speed: 317.2k, Time: 30.5003
Epoch: 16, Batch: 8000/9683, Batch size: 24, LR: 0.0500, Loss: 0.6454, Acc: 0.7347, Speed: 316.5k, Time: 34.8295
Epoch: 16, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6451, Acc: 0.7346, Speed: 318.2k, Time: 38.9744
Train 0.7343
Val 0.7592
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171

saving model to local_200_parikh.pt
Epoch: 17, Batch: 1000/9683, Batch size: 23, LR: 0.0500, Loss: 0.6352, Acc: 0.7394, Speed: 312.2k, Time: 4.4330
Epoch: 17, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6379, Acc: 0.7369, Speed: 311.7k, Time: 8.8125
Epoch: 17, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6340, Acc: 0.7392, Speed: 314.1k, Time: 13.1694
Epoch: 17, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6328, Acc: 0.7399, Speed: 315.0k, Time: 17.4713
Epoch: 17, Batch: 5000/9683, Batch size: 2, LR: 0.0500, Loss: 0.6330, Acc: 0.7400, Speed: 314.5k, Time: 21.7936
Epoch: 17, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6334, Acc: 0.7398, Speed: 315.9k, Time: 26.1452
Epoch: 17, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6328, Acc: 0.7399, Speed: 315.8k, Time: 30.5004
Epoch: 17, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6328, Acc: 0.7398, Speed: 316.4k, Time: 34.8183
Epoch: 17, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6327, Acc: 0.7399, Speed: 316.7k, Time: 39.1305
Train 0.7398
Val 0.7643
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252

saving model to local_200_parikh.pt
Epoch: 18, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6284, Acc: 0.7410, Speed: 311.4k, Time: 4.4315
Epoch: 18, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6263, Acc: 0.7433, Speed: 308.9k, Time: 8.9914
Epoch: 18, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6261, Acc: 0.7437, Speed: 312.0k, Time: 13.3495
Epoch: 18, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6256, Acc: 0.7444, Speed: 313.1k, Time: 17.6430
Epoch: 18, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6249, Acc: 0.7447, Speed: 314.6k, Time: 21.9760
Epoch: 18, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6256, Acc: 0.7441, Speed: 314.0k, Time: 26.3655
Epoch: 18, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6256, Acc: 0.7437, Speed: 313.6k, Time: 30.7258
Epoch: 18, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6255, Acc: 0.7440, Speed: 314.6k, Time: 35.0352
Epoch: 18, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6244, Acc: 0.7445, Speed: 314.1k, Time: 39.4610
Train 0.7446
Val 0.7708
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755

saving model to local_200_parikh.pt
Epoch: 19, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6130, Acc: 0.7492, Speed: 311.6k, Time: 4.4008
Epoch: 19, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6147, Acc: 0.7488, Speed: 315.5k, Time: 8.7023
Epoch: 19, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6163, Acc: 0.7481, Speed: 316.4k, Time: 13.0354
Epoch: 19, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6171, Acc: 0.7482, Speed: 317.7k, Time: 17.3838
Epoch: 19, Batch: 5000/9683, Batch size: 2, LR: 0.0500, Loss: 0.6180, Acc: 0.7476, Speed: 318.3k, Time: 21.6916
Epoch: 19, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6163, Acc: 0.7484, Speed: 318.1k, Time: 25.9958
Epoch: 19, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6152, Acc: 0.7492, Speed: 318.5k, Time: 30.2935
Epoch: 19, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6150, Acc: 0.7492, Speed: 318.5k, Time: 34.5853
Epoch: 19, Batch: 9000/9683, Batch size: 2, LR: 0.0500, Loss: 0.6154, Acc: 0.7490, Speed: 318.3k, Time: 38.9667
Train 0.7488
Val 0.7741
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108

saving model to local_200_parikh.pt
Epoch: 20, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6024, Acc: 0.7557, Speed: 315.2k, Time: 4.4200
Epoch: 20, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6062, Acc: 0.7547, Speed: 315.9k, Time: 8.7636
Epoch: 20, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6071, Acc: 0.7540, Speed: 314.7k, Time: 13.1297
Epoch: 20, Batch: 4000/9683, Batch size: 27, LR: 0.0500, Loss: 0.6058, Acc: 0.7545, Speed: 316.6k, Time: 17.4489
Epoch: 20, Batch: 5000/9683, Batch size: 3, LR: 0.0500, Loss: 0.6057, Acc: 0.7542, Speed: 316.2k, Time: 21.7682
Epoch: 20, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6058, Acc: 0.7538, Speed: 316.4k, Time: 26.0933
Epoch: 20, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6063, Acc: 0.7538, Speed: 315.1k, Time: 30.4827
Epoch: 20, Batch: 8000/9683, Batch size: 3, LR: 0.0500, Loss: 0.6074, Acc: 0.7534, Speed: 314.7k, Time: 34.9007
Epoch: 20, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6081, Acc: 0.7531, Speed: 315.5k, Time: 39.2562
Train 0.7527
Val 0.7797
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697

saving model to local_200_parikh.pt
Epoch: 21, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5987, Acc: 0.7590, Speed: 314.7k, Time: 4.3802
Epoch: 21, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5995, Acc: 0.7578, Speed: 322.2k, Time: 8.5744
Epoch: 21, Batch: 3000/9683, Batch size: 9, LR: 0.0500, Loss: 0.6019, Acc: 0.7553, Speed: 318.0k, Time: 12.9001
Epoch: 21, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6033, Acc: 0.7541, Speed: 316.7k, Time: 17.2754
Epoch: 21, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6030, Acc: 0.7541, Speed: 316.5k, Time: 21.5926
Epoch: 21, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6033, Acc: 0.7539, Speed: 317.1k, Time: 25.9220
Epoch: 21, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6048, Acc: 0.7532, Speed: 316.9k, Time: 30.3025
Epoch: 21, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6043, Acc: 0.7535, Speed: 318.2k, Time: 34.5301
Epoch: 21, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.6037, Acc: 0.7540, Speed: 318.8k, Time: 38.8763
Train 0.7538
Val 0.7780
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970

skip saving model for perf <= 0.7797
Epoch: 22, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5959, Acc: 0.7580, Speed: 309.7k, Time: 4.3671
Epoch: 22, Batch: 2000/9683, Batch size: 44, LR: 0.0500, Loss: 0.5978, Acc: 0.7570, Speed: 315.3k, Time: 8.6556
Epoch: 22, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5988, Acc: 0.7563, Speed: 320.2k, Time: 12.7896
Epoch: 22, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5985, Acc: 0.7562, Speed: 318.6k, Time: 17.1778
Epoch: 22, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5982, Acc: 0.7566, Speed: 319.5k, Time: 21.4535
Epoch: 22, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5984, Acc: 0.7567, Speed: 319.3k, Time: 25.8155
Epoch: 22, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5979, Acc: 0.7569, Speed: 318.9k, Time: 30.1909
Epoch: 22, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5974, Acc: 0.7572, Speed: 317.5k, Time: 34.6871
Epoch: 22, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5976, Acc: 0.7573, Speed: 316.6k, Time: 39.1848
Train 0.7576
Val 0.7847
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676

saving model to local_200_parikh.pt
Epoch: 23, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5901, Acc: 0.7617, Speed: 309.8k, Time: 4.4468
Epoch: 23, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5898, Acc: 0.7612, Speed: 313.1k, Time: 8.8635
Epoch: 23, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5905, Acc: 0.7615, Speed: 312.8k, Time: 13.2220
Epoch: 23, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5909, Acc: 0.7611, Speed: 315.7k, Time: 17.4848
Epoch: 23, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5913, Acc: 0.7608, Speed: 316.5k, Time: 21.8012
Epoch: 23, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5918, Acc: 0.7605, Speed: 315.3k, Time: 26.1517
Epoch: 23, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5918, Acc: 0.7606, Speed: 315.8k, Time: 30.5115
Epoch: 23, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5918, Acc: 0.7606, Speed: 315.2k, Time: 34.8806
Epoch: 23, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5922, Acc: 0.7604, Speed: 316.5k, Time: 39.1635
Train 0.7603
Val 0.7851
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083

saving model to local_200_parikh.pt
Epoch: 24, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5823, Acc: 0.7654, Speed: 314.5k, Time: 4.3922
Epoch: 24, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5842, Acc: 0.7634, Speed: 321.1k, Time: 8.6582
Epoch: 24, Batch: 3000/9683, Batch size: 33, LR: 0.0500, Loss: 0.5839, Acc: 0.7639, Speed: 319.7k, Time: 12.9557
Epoch: 24, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5840, Acc: 0.7637, Speed: 320.8k, Time: 17.2476
Epoch: 24, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5853, Acc: 0.7631, Speed: 320.2k, Time: 21.6321
Epoch: 24, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5851, Acc: 0.7630, Speed: 320.8k, Time: 25.8394
Epoch: 24, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5846, Acc: 0.7635, Speed: 321.6k, Time: 30.1295
Epoch: 24, Batch: 8000/9683, Batch size: 31, LR: 0.0500, Loss: 0.5849, Acc: 0.7632, Speed: 320.1k, Time: 34.5034
Epoch: 24, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5853, Acc: 0.7629, Speed: 319.7k, Time: 38.8196
Train 0.7626
Val 0.7897
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656

saving model to local_200_parikh.pt
Epoch: 25, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5773, Acc: 0.7671, Speed: 304.9k, Time: 4.4127
Epoch: 25, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5807, Acc: 0.7657, Speed: 312.6k, Time: 8.7159
Epoch: 25, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5816, Acc: 0.7655, Speed: 314.8k, Time: 13.0521
Epoch: 25, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5811, Acc: 0.7654, Speed: 313.1k, Time: 17.4248
Epoch: 25, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5810, Acc: 0.7655, Speed: 314.3k, Time: 21.7340
Epoch: 25, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5814, Acc: 0.7651, Speed: 316.9k, Time: 26.0371
Epoch: 25, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5813, Acc: 0.7652, Speed: 316.7k, Time: 30.3944
Epoch: 25, Batch: 8000/9683, Batch size: 1, LR: 0.0500, Loss: 0.5807, Acc: 0.7651, Speed: 317.4k, Time: 34.7136
Epoch: 25, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5804, Acc: 0.7654, Speed: 318.4k, Time: 38.9392
Train 0.7656
Val 0.7908
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773

saving model to local_200_parikh.pt
Epoch: 26, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5827, Acc: 0.7636, Speed: 318.4k, Time: 4.2777
Epoch: 26, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5810, Acc: 0.7642, Speed: 316.5k, Time: 8.5874
Epoch: 26, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5813, Acc: 0.7644, Speed: 318.8k, Time: 12.9146
Epoch: 26, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5788, Acc: 0.7656, Speed: 317.0k, Time: 17.3388
Epoch: 26, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5780, Acc: 0.7667, Speed: 318.5k, Time: 21.6606
Epoch: 26, Batch: 6000/9683, Batch size: 55, LR: 0.0500, Loss: 0.5773, Acc: 0.7669, Speed: 318.9k, Time: 25.9629
Epoch: 26, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5774, Acc: 0.7671, Speed: 317.5k, Time: 30.3012
Epoch: 26, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5772, Acc: 0.7670, Speed: 318.3k, Time: 34.6084
Epoch: 26, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5778, Acc: 0.7669, Speed: 319.3k, Time: 38.8623
Train 0.7667
Val 0.7863
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302

skip saving model for perf <= 0.7908
Epoch: 27, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5760, Acc: 0.7680, Speed: 312.0k, Time: 4.3722
Epoch: 27, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5773, Acc: 0.7682, Speed: 312.9k, Time: 8.7018
Epoch: 27, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5772, Acc: 0.7681, Speed: 313.8k, Time: 13.0463
Epoch: 27, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5748, Acc: 0.7687, Speed: 316.5k, Time: 17.3351
Epoch: 27, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5749, Acc: 0.7686, Speed: 315.9k, Time: 21.6863
Epoch: 27, Batch: 6000/9683, Batch size: 1, LR: 0.0500, Loss: 0.5753, Acc: 0.7685, Speed: 316.0k, Time: 26.0693
Epoch: 27, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5745, Acc: 0.7688, Speed: 316.0k, Time: 30.4327
Epoch: 27, Batch: 8000/9683, Batch size: 5, LR: 0.0500, Loss: 0.5746, Acc: 0.7687, Speed: 315.6k, Time: 34.7930
Epoch: 27, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5743, Acc: 0.7689, Speed: 315.7k, Time: 39.1530
Train 0.7689
Val 0.7938
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822

saving model to local_200_parikh.pt
Epoch: 28, Batch: 1000/9683, Batch size: 63, LR: 0.0500, Loss: 0.5668, Acc: 0.7726, Speed: 309.3k, Time: 4.4467
Epoch: 28, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5685, Acc: 0.7707, Speed: 316.1k, Time: 8.7949
Epoch: 28, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5680, Acc: 0.7714, Speed: 320.1k, Time: 13.0490
Epoch: 28, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5681, Acc: 0.7715, Speed: 320.1k, Time: 17.3483
Epoch: 28, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5678, Acc: 0.7717, Speed: 320.1k, Time: 21.6051
Epoch: 28, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5675, Acc: 0.7718, Speed: 319.6k, Time: 25.9971
Epoch: 28, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5681, Acc: 0.7716, Speed: 318.9k, Time: 30.3007
Epoch: 28, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5681, Acc: 0.7716, Speed: 319.0k, Time: 34.6465
Epoch: 28, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5687, Acc: 0.7713, Speed: 318.4k, Time: 38.9332
Train 0.7710
Val 0.7980
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988

saving model to local_200_parikh.pt
Epoch: 29, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5682, Acc: 0.7724, Speed: 308.5k, Time: 4.4647
Epoch: 29, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5682, Acc: 0.7725, Speed: 312.5k, Time: 8.8522
Epoch: 29, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5676, Acc: 0.7722, Speed: 316.6k, Time: 13.1053
Epoch: 29, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5681, Acc: 0.7720, Speed: 316.8k, Time: 17.4516
Epoch: 29, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5685, Acc: 0.7715, Speed: 316.6k, Time: 21.7837
Epoch: 29, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5679, Acc: 0.7717, Speed: 316.6k, Time: 26.1227
Epoch: 29, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5686, Acc: 0.7713, Speed: 318.1k, Time: 30.3672
Epoch: 29, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5692, Acc: 0.7710, Speed: 318.4k, Time: 34.5857
Epoch: 29, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5687, Acc: 0.7713, Speed: 319.0k, Time: 38.8035
Train 0.7711
Val 0.7958
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752

skip saving model for perf <= 0.7980
Epoch: 30, Batch: 1000/9683, Batch size: 1, LR: 0.0500, Loss: 0.5706, Acc: 0.7699, Speed: 317.6k, Time: 4.3760
Epoch: 30, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5679, Acc: 0.7714, Speed: 322.8k, Time: 8.5951
Epoch: 30, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5658, Acc: 0.7726, Speed: 310.3k, Time: 13.3264
Epoch: 30, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5660, Acc: 0.7722, Speed: 311.7k, Time: 17.6036
Epoch: 30, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5654, Acc: 0.7725, Speed: 312.7k, Time: 21.9762
Epoch: 30, Batch: 6000/9683, Batch size: 9, LR: 0.0500, Loss: 0.5652, Acc: 0.7727, Speed: 313.2k, Time: 26.2855
Epoch: 30, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5650, Acc: 0.7730, Speed: 314.4k, Time: 30.6492
Epoch: 30, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5640, Acc: 0.7735, Speed: 315.1k, Time: 34.9901
Epoch: 30, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5639, Acc: 0.7734, Speed: 315.0k, Time: 39.3340
Train 0.7732
Val 0.7991
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106

saving model to local_200_parikh.pt
Epoch: 31, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5574, Acc: 0.7762, Speed: 319.0k, Time: 4.3268
Epoch: 31, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5588, Acc: 0.7756, Speed: 317.2k, Time: 8.6339
Epoch: 31, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5598, Acc: 0.7753, Speed: 318.4k, Time: 12.9241
Epoch: 31, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5607, Acc: 0.7751, Speed: 318.7k, Time: 17.1847
Epoch: 31, Batch: 5000/9683, Batch size: 9, LR: 0.0500, Loss: 0.5609, Acc: 0.7749, Speed: 316.7k, Time: 21.5888
Epoch: 31, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5608, Acc: 0.7748, Speed: 316.0k, Time: 25.9878
Epoch: 31, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5612, Acc: 0.7744, Speed: 315.9k, Time: 30.3751
Epoch: 31, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5609, Acc: 0.7748, Speed: 315.5k, Time: 34.8221
Epoch: 31, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5614, Acc: 0.7743, Speed: 316.3k, Time: 39.1506
Train 0.7745
Val 0.8011
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138

saving model to local_200_parikh.pt
Epoch: 32, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5597, Acc: 0.7757, Speed: 311.4k, Time: 4.4155
Epoch: 32, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5571, Acc: 0.7759, Speed: 297.3k, Time: 9.2051
Epoch: 32, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5605, Acc: 0.7750, Speed: 290.3k, Time: 14.1479
Epoch: 32, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5613, Acc: 0.7752, Speed: 289.6k, Time: 19.0641
Epoch: 32, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5604, Acc: 0.7758, Speed: 287.4k, Time: 24.0231
Epoch: 32, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5599, Acc: 0.7759, Speed: 288.0k, Time: 28.7593
Epoch: 32, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5596, Acc: 0.7761, Speed: 291.9k, Time: 33.0985
Epoch: 32, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5592, Acc: 0.7763, Speed: 294.6k, Time: 37.4754
Epoch: 32, Batch: 9000/9683, Batch size: 26, LR: 0.0500, Loss: 0.5586, Acc: 0.7765, Speed: 296.5k, Time: 41.8589
Train 0.7765
Val 0.8035
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475

saving model to local_200_parikh.pt
Epoch: 33, Batch: 1000/9683, Batch size: 8, LR: 0.0500, Loss: 0.5588, Acc: 0.7764, Speed: 307.7k, Time: 4.3959
Epoch: 33, Batch: 2000/9683, Batch size: 3, LR: 0.0500, Loss: 0.5575, Acc: 0.7767, Speed: 312.3k, Time: 8.7549
Epoch: 33, Batch: 3000/9683, Batch size: 1, LR: 0.0500, Loss: 0.5577, Acc: 0.7763, Speed: 313.1k, Time: 13.0780
Epoch: 33, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5580, Acc: 0.7767, Speed: 313.4k, Time: 17.4144
Epoch: 33, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5566, Acc: 0.7776, Speed: 315.5k, Time: 21.7732
Epoch: 33, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5556, Acc: 0.7779, Speed: 315.6k, Time: 26.0605
Epoch: 33, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5561, Acc: 0.7778, Speed: 316.8k, Time: 30.4155
Epoch: 33, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5559, Acc: 0.7777, Speed: 317.3k, Time: 34.7873
Epoch: 33, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5560, Acc: 0.7774, Speed: 316.5k, Time: 39.1578
Train 0.7776
Val 0.8001
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122

skip saving model for perf <= 0.8035
Epoch: 34, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5518, Acc: 0.7800, Speed: 296.1k, Time: 4.7246
Epoch: 34, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5518, Acc: 0.7799, Speed: 284.0k, Time: 9.6734
Epoch: 34, Batch: 3000/9683, Batch size: 28, LR: 0.0500, Loss: 0.5518, Acc: 0.7800, Speed: 282.5k, Time: 14.6007
Epoch: 34, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5540, Acc: 0.7789, Speed: 281.7k, Time: 19.5031
Epoch: 34, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5536, Acc: 0.7791, Speed: 280.7k, Time: 24.5036
Epoch: 34, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5541, Acc: 0.7788, Speed: 282.4k, Time: 29.3877
Epoch: 34, Batch: 7000/9683, Batch size: 28, LR: 0.0500, Loss: 0.5533, Acc: 0.7791, Speed: 281.7k, Time: 34.3069
Epoch: 34, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5530, Acc: 0.7793, Speed: 281.3k, Time: 39.2845
Epoch: 34, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5529, Acc: 0.7794, Speed: 280.8k, Time: 44.2217
Train 0.7793
Val 0.8007
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732

skip saving model for perf <= 0.8035
Epoch: 35, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5523, Acc: 0.7795, Speed: 311.2k, Time: 4.4547
Epoch: 35, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5536, Acc: 0.7782, Speed: 315.8k, Time: 8.7139
Epoch: 35, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5528, Acc: 0.7788, Speed: 316.5k, Time: 13.1141
Epoch: 35, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5526, Acc: 0.7784, Speed: 316.8k, Time: 17.4320
Epoch: 35, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5525, Acc: 0.7784, Speed: 317.6k, Time: 21.7112
Epoch: 35, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5520, Acc: 0.7786, Speed: 317.1k, Time: 26.0032
Epoch: 35, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5520, Acc: 0.7787, Speed: 317.4k, Time: 30.3252
Epoch: 35, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5521, Acc: 0.7783, Speed: 318.6k, Time: 34.5762
Epoch: 35, Batch: 9000/9683, Batch size: 2, LR: 0.0500, Loss: 0.5515, Acc: 0.7789, Speed: 318.8k, Time: 38.8345
Train 0.7790
Val 0.8032
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170

skip saving model for perf <= 0.8035
Epoch: 36, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5470, Acc: 0.7842, Speed: 314.2k, Time: 4.4007
Epoch: 36, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5490, Acc: 0.7821, Speed: 319.8k, Time: 8.6409
Epoch: 36, Batch: 3000/9683, Batch size: 1, LR: 0.0500, Loss: 0.5496, Acc: 0.7807, Speed: 320.9k, Time: 12.8793
Epoch: 36, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5499, Acc: 0.7808, Speed: 323.3k, Time: 17.0609
Epoch: 36, Batch: 5000/9683, Batch size: 17, LR: 0.0500, Loss: 0.5511, Acc: 0.7800, Speed: 324.5k, Time: 21.2531
Epoch: 36, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5502, Acc: 0.7804, Speed: 323.3k, Time: 25.5527
Epoch: 36, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5496, Acc: 0.7809, Speed: 322.7k, Time: 29.8384
Epoch: 36, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5490, Acc: 0.7811, Speed: 323.1k, Time: 34.1049
Epoch: 36, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5483, Acc: 0.7814, Speed: 323.9k, Time: 38.2572
Train 0.7812
Val 0.8050
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999

saving model to local_200_parikh.pt
Epoch: 37, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5403, Acc: 0.7856, Speed: 330.0k, Time: 4.2472
Epoch: 37, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5450, Acc: 0.7824, Speed: 328.0k, Time: 8.5099
Epoch: 37, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5456, Acc: 0.7823, Speed: 330.7k, Time: 12.6292
Epoch: 37, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5456, Acc: 0.7825, Speed: 329.2k, Time: 16.8217
Epoch: 37, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5456, Acc: 0.7824, Speed: 329.2k, Time: 20.9484
Epoch: 37, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5453, Acc: 0.7824, Speed: 330.2k, Time: 25.1019
Epoch: 37, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5453, Acc: 0.7826, Speed: 329.1k, Time: 29.3473
Epoch: 37, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5451, Acc: 0.7826, Speed: 328.5k, Time: 33.5203
Epoch: 37, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5454, Acc: 0.7823, Speed: 329.3k, Time: 37.6744
Train 0.7824
Val 0.8076
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641

saving model to local_200_parikh.pt
Epoch: 38, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5491, Acc: 0.7807, Speed: 335.7k, Time: 4.1771
Epoch: 38, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5440, Acc: 0.7826, Speed: 334.3k, Time: 8.3863
Epoch: 38, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5434, Acc: 0.7830, Speed: 331.7k, Time: 12.6554
Epoch: 38, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5443, Acc: 0.7822, Speed: 328.4k, Time: 16.8850
Epoch: 38, Batch: 5000/9683, Batch size: 5, LR: 0.0500, Loss: 0.5447, Acc: 0.7819, Speed: 328.9k, Time: 21.0494
Epoch: 38, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5444, Acc: 0.7823, Speed: 328.7k, Time: 25.1969
Epoch: 38, Batch: 7000/9683, Batch size: 9, LR: 0.0500, Loss: 0.5446, Acc: 0.7824, Speed: 329.5k, Time: 29.2990
Epoch: 38, Batch: 8000/9683, Batch size: 1, LR: 0.0500, Loss: 0.5459, Acc: 0.7819, Speed: 329.5k, Time: 33.4693
Epoch: 38, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5461, Acc: 0.7818, Speed: 328.9k, Time: 37.6642
Train 0.7817
Val 0.8056
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609

skip saving model for perf <= 0.8076
Epoch: 39, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5453, Acc: 0.7809, Speed: 327.7k, Time: 4.2206
Epoch: 39, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5419, Acc: 0.7834, Speed: 322.5k, Time: 8.4908
Epoch: 39, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5419, Acc: 0.7832, Speed: 324.1k, Time: 12.6737
Epoch: 39, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5416, Acc: 0.7832, Speed: 326.5k, Time: 16.7993
Epoch: 39, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5429, Acc: 0.7831, Speed: 328.0k, Time: 20.9775
Epoch: 39, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5439, Acc: 0.7826, Speed: 329.3k, Time: 25.1046
Epoch: 39, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5441, Acc: 0.7824, Speed: 329.7k, Time: 29.3086
Epoch: 39, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5445, Acc: 0.7824, Speed: 328.9k, Time: 33.5530
Epoch: 39, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5445, Acc: 0.7824, Speed: 327.7k, Time: 37.8411
Train 0.7827
Val 0.8110
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995

saving model to local_200_parikh.pt
Epoch: 40, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5371, Acc: 0.7859, Speed: 324.6k, Time: 4.2658
Epoch: 40, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5392, Acc: 0.7850, Speed: 328.8k, Time: 8.4528
Epoch: 40, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5405, Acc: 0.7838, Speed: 329.1k, Time: 12.6190
Epoch: 40, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5411, Acc: 0.7834, Speed: 331.1k, Time: 16.8068
Epoch: 40, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5416, Acc: 0.7833, Speed: 329.8k, Time: 21.0065
Epoch: 40, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5408, Acc: 0.7842, Speed: 329.5k, Time: 25.1982
Epoch: 40, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5410, Acc: 0.7841, Speed: 329.3k, Time: 29.4078
Epoch: 40, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5407, Acc: 0.7841, Speed: 328.3k, Time: 33.6762
Epoch: 40, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5410, Acc: 0.7840, Speed: 329.0k, Time: 37.7605
Train 0.7841
Val 0.8101
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080

skip saving model for perf <= 0.8110
Epoch: 41, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5381, Acc: 0.7856, Speed: 320.4k, Time: 4.2937
Epoch: 41, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5408, Acc: 0.7854, Speed: 326.9k, Time: 8.4412
Epoch: 41, Batch: 3000/9683, Batch size: 3, LR: 0.0500, Loss: 0.5414, Acc: 0.7848, Speed: 326.4k, Time: 12.6163
Epoch: 41, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5409, Acc: 0.7845, Speed: 325.4k, Time: 16.9234
Epoch: 41, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5398, Acc: 0.7846, Speed: 324.5k, Time: 21.1912
Epoch: 41, Batch: 6000/9683, Batch size: 2, LR: 0.0500, Loss: 0.5409, Acc: 0.7844, Speed: 325.5k, Time: 25.3903
Epoch: 41, Batch: 7000/9683, Batch size: 1, LR: 0.0500, Loss: 0.5404, Acc: 0.7845, Speed: 325.9k, Time: 29.5698
Epoch: 41, Batch: 8000/9683, Batch size: 9, LR: 0.0500, Loss: 0.5402, Acc: 0.7847, Speed: 326.3k, Time: 33.7809
Epoch: 41, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5402, Acc: 0.7847, Speed: 326.4k, Time: 37.9819
Train 0.7847
Val 0.8054
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406

skip saving model for perf <= 0.8110
Epoch: 42, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5356, Acc: 0.7873, Speed: 326.4k, Time: 4.1758
Epoch: 42, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5395, Acc: 0.7856, Speed: 325.5k, Time: 8.3948
Epoch: 42, Batch: 3000/9683, Batch size: 4, LR: 0.0500, Loss: 0.5400, Acc: 0.7852, Speed: 322.7k, Time: 12.6915
Epoch: 42, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5385, Acc: 0.7856, Speed: 322.4k, Time: 16.9241
Epoch: 42, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5391, Acc: 0.7854, Speed: 322.9k, Time: 21.1279
Epoch: 42, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5402, Acc: 0.7848, Speed: 323.9k, Time: 25.3492
Epoch: 42, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5401, Acc: 0.7850, Speed: 325.8k, Time: 29.5302
Epoch: 42, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5399, Acc: 0.7851, Speed: 326.2k, Time: 33.7216
Epoch: 42, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5394, Acc: 0.7852, Speed: 327.5k, Time: 37.8241
Train 0.7853
Val 0.8110
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995

skip saving model for perf <= 0.8110
Epoch: 43, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5272, Acc: 0.7907, Speed: 324.8k, Time: 4.2181
Epoch: 43, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5298, Acc: 0.7900, Speed: 324.8k, Time: 8.4664
Epoch: 43, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5327, Acc: 0.7882, Speed: 325.8k, Time: 12.6505
Epoch: 43, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5345, Acc: 0.7875, Speed: 326.5k, Time: 16.9195
Epoch: 43, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5349, Acc: 0.7874, Speed: 326.5k, Time: 21.1159
Epoch: 43, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5348, Acc: 0.7874, Speed: 325.9k, Time: 25.3556
Epoch: 43, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5356, Acc: 0.7872, Speed: 326.9k, Time: 29.5061
Epoch: 43, Batch: 8000/9683, Batch size: 4, LR: 0.0500, Loss: 0.5360, Acc: 0.7869, Speed: 328.5k, Time: 33.6397
Epoch: 43, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5358, Acc: 0.7871, Speed: 330.5k, Time: 37.5555
Train 0.7868
Val 0.8045
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491

skip saving model for perf <= 0.8110
Epoch: 44, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5361, Acc: 0.7865, Speed: 324.4k, Time: 4.3280
Epoch: 44, Batch: 2000/9683, Batch size: 16, LR: 0.0500, Loss: 0.5351, Acc: 0.7866, Speed: 325.1k, Time: 8.5400
Epoch: 44, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5374, Acc: 0.7850, Speed: 323.6k, Time: 12.7252
Epoch: 44, Batch: 4000/9683, Batch size: 3, LR: 0.0500, Loss: 0.5373, Acc: 0.7854, Speed: 324.3k, Time: 16.9304
Epoch: 44, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5378, Acc: 0.7852, Speed: 324.5k, Time: 21.2180
Epoch: 44, Batch: 6000/9683, Batch size: 58, LR: 0.0500, Loss: 0.5380, Acc: 0.7853, Speed: 323.6k, Time: 25.4966
Epoch: 44, Batch: 7000/9683, Batch size: 1, LR: 0.0500, Loss: 0.5372, Acc: 0.7858, Speed: 325.2k, Time: 29.6548
Epoch: 44, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5371, Acc: 0.7858, Speed: 326.1k, Time: 33.7295
Epoch: 44, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5370, Acc: 0.7860, Speed: 326.8k, Time: 37.9025
Train 0.7861
Val 0.8077
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743

skip saving model for perf <= 0.8110
Epoch: 45, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5362, Acc: 0.7870, Speed: 318.9k, Time: 4.2501
Epoch: 45, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5362, Acc: 0.7865, Speed: 321.9k, Time: 8.5058
Epoch: 45, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5357, Acc: 0.7870, Speed: 321.8k, Time: 12.7921
Epoch: 45, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5352, Acc: 0.7869, Speed: 323.7k, Time: 17.0107
Epoch: 45, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5344, Acc: 0.7875, Speed: 324.8k, Time: 21.1797
Epoch: 45, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5334, Acc: 0.7880, Speed: 325.7k, Time: 25.3323
Epoch: 45, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5338, Acc: 0.7878, Speed: 324.7k, Time: 29.6386
Epoch: 45, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5344, Acc: 0.7877, Speed: 324.8k, Time: 33.8644
Epoch: 45, Batch: 9000/9683, Batch size: 22, LR: 0.0500, Loss: 0.5345, Acc: 0.7878, Speed: 325.0k, Time: 38.0962
Train 0.7876
Val 0.8117
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706

saving model to local_200_parikh.pt
Epoch: 46, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5382, Acc: 0.7881, Speed: 317.5k, Time: 4.2291
Epoch: 46, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5340, Acc: 0.7894, Speed: 324.4k, Time: 8.3932
Epoch: 46, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5354, Acc: 0.7884, Speed: 324.8k, Time: 12.6748
Epoch: 46, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5349, Acc: 0.7884, Speed: 325.1k, Time: 16.8945
Epoch: 46, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5341, Acc: 0.7883, Speed: 327.1k, Time: 21.0893
Epoch: 46, Batch: 6000/9683, Batch size: 37, LR: 0.0500, Loss: 0.5342, Acc: 0.7880, Speed: 327.9k, Time: 25.2648
Epoch: 46, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5326, Acc: 0.7886, Speed: 323.2k, Time: 29.8579
Epoch: 46, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5326, Acc: 0.7882, Speed: 319.6k, Time: 34.5508
Epoch: 46, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5324, Acc: 0.7881, Speed: 314.3k, Time: 39.4930
Train 0.7880
Val 0.8111
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096

skip saving model for perf <= 0.8117
Epoch: 47, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5332, Acc: 0.7865, Speed: 329.8k, Time: 4.2783
Epoch: 47, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5303, Acc: 0.7891, Speed: 325.7k, Time: 8.4895
Epoch: 47, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5294, Acc: 0.7896, Speed: 325.3k, Time: 12.6701
Epoch: 47, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5306, Acc: 0.7890, Speed: 325.4k, Time: 16.9364
Epoch: 47, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5307, Acc: 0.7893, Speed: 325.8k, Time: 21.1741
Epoch: 47, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5311, Acc: 0.7892, Speed: 325.9k, Time: 25.3904
Epoch: 47, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5308, Acc: 0.7894, Speed: 326.7k, Time: 29.5844
Epoch: 47, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5299, Acc: 0.7897, Speed: 326.3k, Time: 33.8168
Epoch: 47, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5301, Acc: 0.7895, Speed: 325.2k, Time: 38.1273
Train 0.7895
Val 0.8124
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417

saving model to local_200_parikh.pt
Epoch: 48, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5231, Acc: 0.7922, Speed: 317.4k, Time: 4.2951
Epoch: 48, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5255, Acc: 0.7912, Speed: 324.5k, Time: 8.4659
Epoch: 48, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5265, Acc: 0.7910, Speed: 327.8k, Time: 12.7429
Epoch: 48, Batch: 4000/9683, Batch size: 2, LR: 0.0500, Loss: 0.5258, Acc: 0.7910, Speed: 328.9k, Time: 16.8902
Epoch: 48, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5270, Acc: 0.7904, Speed: 326.6k, Time: 21.1390
Epoch: 48, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5273, Acc: 0.7899, Speed: 326.5k, Time: 25.3749
Epoch: 48, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5276, Acc: 0.7900, Speed: 325.8k, Time: 29.5700
Epoch: 48, Batch: 8000/9683, Batch size: 2, LR: 0.0500, Loss: 0.5279, Acc: 0.7900, Speed: 325.6k, Time: 33.7870
Epoch: 48, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5289, Acc: 0.7897, Speed: 326.1k, Time: 37.9769
Train 0.7897
Val 0.8125
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519

saving model to local_200_parikh.pt
Epoch: 49, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5264, Acc: 0.7929, Speed: 315.9k, Time: 4.3366
Epoch: 49, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5257, Acc: 0.7925, Speed: 320.6k, Time: 8.6238
Epoch: 49, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5269, Acc: 0.7919, Speed: 318.0k, Time: 12.9102
Epoch: 49, Batch: 4000/9683, Batch size: 1, LR: 0.0500, Loss: 0.5275, Acc: 0.7912, Speed: 320.7k, Time: 17.1395
Epoch: 49, Batch: 5000/9683, Batch size: 2, LR: 0.0500, Loss: 0.5279, Acc: 0.7908, Speed: 322.5k, Time: 21.4170
Epoch: 49, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5279, Acc: 0.7905, Speed: 325.3k, Time: 25.4008
Epoch: 49, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5296, Acc: 0.7897, Speed: 327.5k, Time: 29.4318
Epoch: 49, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5300, Acc: 0.7895, Speed: 329.1k, Time: 33.4792
Epoch: 49, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5297, Acc: 0.7898, Speed: 330.3k, Time: 37.5210
Train 0.7898
Val 0.8139
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942

saving model to local_200_parikh.pt
Epoch: 50, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5290, Acc: 0.7885, Speed: 314.7k, Time: 4.3736
Epoch: 50, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5294, Acc: 0.7880, Speed: 317.7k, Time: 8.6392
Epoch: 50, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5291, Acc: 0.7881, Speed: 321.0k, Time: 12.8090
Epoch: 50, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5292, Acc: 0.7886, Speed: 333.8k, Time: 16.4327
Epoch: 50, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5293, Acc: 0.7888, Speed: 342.6k, Time: 20.0364
Epoch: 50, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5289, Acc: 0.7891, Speed: 348.6k, Time: 23.6347
Epoch: 50, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5287, Acc: 0.7894, Speed: 353.9k, Time: 27.2375
Epoch: 50, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5282, Acc: 0.7899, Speed: 357.9k, Time: 30.8409
Epoch: 50, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5279, Acc: 0.7899, Speed: 360.1k, Time: 34.4413
Train 0.7901
Val 0.8114
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401

skip saving model for perf <= 0.8139
Epoch: 51, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5226, Acc: 0.7915, Speed: 385.0k, Time: 3.6033
Epoch: 51, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5224, Acc: 0.7919, Speed: 379.5k, Time: 7.1994
Epoch: 51, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5233, Acc: 0.7925, Speed: 379.2k, Time: 10.7986
Epoch: 51, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5250, Acc: 0.7914, Speed: 380.3k, Time: 14.3980
Epoch: 51, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5248, Acc: 0.7913, Speed: 379.8k, Time: 17.9971
Epoch: 51, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5249, Acc: 0.7912, Speed: 380.9k, Time: 21.6031
Epoch: 51, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5251, Acc: 0.7912, Speed: 382.1k, Time: 25.2039
Epoch: 51, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5247, Acc: 0.7916, Speed: 381.3k, Time: 28.8000
Epoch: 51, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5251, Acc: 0.7915, Speed: 382.4k, Time: 32.4061
Train 0.7914
Val 0.8108
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792

skip saving model for perf <= 0.8139
Epoch: 52, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5272, Acc: 0.7917, Speed: 386.5k, Time: 3.6020
Epoch: 52, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5260, Acc: 0.7914, Speed: 386.6k, Time: 7.2024
Epoch: 52, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5254, Acc: 0.7916, Speed: 385.3k, Time: 10.8004
Epoch: 52, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5248, Acc: 0.7917, Speed: 384.7k, Time: 14.3915
Epoch: 52, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5248, Acc: 0.7920, Speed: 383.6k, Time: 17.9891
Epoch: 52, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5254, Acc: 0.7916, Speed: 383.8k, Time: 21.5885
Epoch: 52, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5249, Acc: 0.7916, Speed: 383.0k, Time: 25.1839
Epoch: 52, Batch: 8000/9683, Batch size: 14, LR: 0.0500, Loss: 0.5250, Acc: 0.7916, Speed: 383.1k, Time: 28.7823
Epoch: 52, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5244, Acc: 0.7919, Speed: 383.1k, Time: 32.3801
Train 0.7917
Val 0.8133
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332

skip saving model for perf <= 0.8139
Epoch: 53, Batch: 1000/9683, Batch size: 39, LR: 0.0500, Loss: 0.5174, Acc: 0.7944, Speed: 380.2k, Time: 3.5962
Epoch: 53, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5174, Acc: 0.7949, Speed: 381.9k, Time: 7.1966
Epoch: 53, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5175, Acc: 0.7948, Speed: 381.5k, Time: 10.7941
Epoch: 53, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5202, Acc: 0.7935, Speed: 381.7k, Time: 14.3917
Epoch: 53, Batch: 5000/9683, Batch size: 43, LR: 0.0500, Loss: 0.5209, Acc: 0.7931, Speed: 381.3k, Time: 17.9895
Epoch: 53, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5225, Acc: 0.7923, Speed: 381.2k, Time: 21.5881
Epoch: 53, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5225, Acc: 0.7923, Speed: 381.6k, Time: 25.1855
Epoch: 53, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5224, Acc: 0.7924, Speed: 382.7k, Time: 28.7909
Epoch: 53, Batch: 9000/9683, Batch size: 7, LR: 0.0500, Loss: 0.5226, Acc: 0.7923, Speed: 382.9k, Time: 32.3911
Train 0.7923
Val 0.8155
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466

saving model to local_200_parikh.pt
Epoch: 54, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5194, Acc: 0.7942, Speed: 383.2k, Time: 3.5991
Epoch: 54, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5185, Acc: 0.7940, Speed: 378.9k, Time: 7.1947
Epoch: 54, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5186, Acc: 0.7936, Speed: 379.3k, Time: 10.7922
Epoch: 54, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5210, Acc: 0.7931, Speed: 380.2k, Time: 14.3954
Epoch: 54, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5209, Acc: 0.7929, Speed: 381.3k, Time: 17.9971
Epoch: 54, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5207, Acc: 0.7930, Speed: 381.6k, Time: 21.5981
Epoch: 54, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5206, Acc: 0.7930, Speed: 382.5k, Time: 25.1993
Epoch: 54, Batch: 8000/9683, Batch size: 4, LR: 0.0500, Loss: 0.5204, Acc: 0.7935, Speed: 382.3k, Time: 28.8003
Epoch: 54, Batch: 9000/9683, Batch size: 15, LR: 0.0500, Loss: 0.5207, Acc: 0.7932, Speed: 381.9k, Time: 32.3994
Train 0.7932
Val 0.8152
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161

skip saving model for perf <= 0.8155
Epoch: 55, Batch: 1000/9683, Batch size: 1, LR: 0.0500, Loss: 0.5210, Acc: 0.7916, Speed: 385.8k, Time: 3.6025
Epoch: 55, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5220, Acc: 0.7911, Speed: 383.4k, Time: 7.1991
Epoch: 55, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5228, Acc: 0.7913, Speed: 383.2k, Time: 10.8007
Epoch: 55, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5211, Acc: 0.7923, Speed: 383.5k, Time: 14.3994
Epoch: 55, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5202, Acc: 0.7928, Speed: 383.8k, Time: 18.0011
Epoch: 55, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5195, Acc: 0.7933, Speed: 384.4k, Time: 21.6068
Epoch: 55, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5197, Acc: 0.7933, Speed: 383.6k, Time: 25.2077
Epoch: 55, Batch: 8000/9683, Batch size: 56, LR: 0.0500, Loss: 0.5205, Acc: 0.7930, Speed: 382.6k, Time: 28.8066
Epoch: 55, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5201, Acc: 0.7932, Speed: 382.7k, Time: 32.4073
Train 0.7931
Val 0.8147
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653

skip saving model for perf <= 0.8155
Epoch: 56, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5135, Acc: 0.7968, Speed: 352.9k, Time: 3.9093
Epoch: 56, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5155, Acc: 0.7954, Speed: 346.9k, Time: 7.9699
Epoch: 56, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5160, Acc: 0.7953, Speed: 341.8k, Time: 12.1845
Epoch: 56, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5166, Acc: 0.7955, Speed: 338.0k, Time: 16.3684
Epoch: 56, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5174, Acc: 0.7952, Speed: 336.3k, Time: 20.5514
Epoch: 56, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5179, Acc: 0.7950, Speed: 335.0k, Time: 24.7012
Epoch: 56, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5181, Acc: 0.7948, Speed: 335.9k, Time: 28.7361
Epoch: 56, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5186, Acc: 0.7945, Speed: 334.1k, Time: 32.9710
Epoch: 56, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5188, Acc: 0.7944, Speed: 334.1k, Time: 37.0959
Train 0.7943
Val 0.8139
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942

skip saving model for perf <= 0.8155
Epoch: 57, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5175, Acc: 0.7934, Speed: 337.9k, Time: 4.2517
Epoch: 57, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5151, Acc: 0.7956, Speed: 333.9k, Time: 8.4613
Epoch: 57, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5171, Acc: 0.7947, Speed: 328.7k, Time: 12.7455
Epoch: 57, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5178, Acc: 0.7943, Speed: 335.3k, Time: 16.5566
Epoch: 57, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5190, Acc: 0.7940, Speed: 333.3k, Time: 20.7675
Epoch: 57, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5195, Acc: 0.7940, Speed: 334.6k, Time: 24.8515
Epoch: 57, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5182, Acc: 0.7948, Speed: 334.2k, Time: 28.9395
Epoch: 57, Batch: 8000/9683, Batch size: 40, LR: 0.0500, Loss: 0.5180, Acc: 0.7948, Speed: 332.5k, Time: 33.1916
Epoch: 57, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5186, Acc: 0.7945, Speed: 333.3k, Time: 37.1702
Train 0.7944
Val 0.8156
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568

saving model to local_200_parikh.pt
Epoch: 58, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5157, Acc: 0.7958, Speed: 336.5k, Time: 4.0396
Epoch: 58, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5139, Acc: 0.7959, Speed: 339.5k, Time: 8.1553
Epoch: 58, Batch: 3000/9683, Batch size: 60, LR: 0.0500, Loss: 0.5149, Acc: 0.7953, Speed: 334.2k, Time: 12.3544
Epoch: 58, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5165, Acc: 0.7947, Speed: 339.6k, Time: 16.2038
Epoch: 58, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5169, Acc: 0.7944, Speed: 338.9k, Time: 20.3074
Epoch: 58, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5177, Acc: 0.7941, Speed: 339.2k, Time: 24.3407
Epoch: 58, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5180, Acc: 0.7940, Speed: 337.3k, Time: 28.5706
Epoch: 58, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5183, Acc: 0.7939, Speed: 337.3k, Time: 32.7760
Epoch: 58, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5187, Acc: 0.7940, Speed: 335.5k, Time: 36.9948
Train 0.7940
Val 0.8157
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669

saving model to local_200_parikh.pt
Epoch: 59, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5131, Acc: 0.7974, Speed: 312.0k, Time: 4.3477
Epoch: 59, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5118, Acc: 0.7967, Speed: 319.3k, Time: 8.5558
Epoch: 59, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5123, Acc: 0.7967, Speed: 326.0k, Time: 12.6017
Epoch: 59, Batch: 4000/9683, Batch size: 1, LR: 0.0500, Loss: 0.5138, Acc: 0.7964, Speed: 333.0k, Time: 16.5366
Epoch: 59, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5158, Acc: 0.7959, Speed: 331.8k, Time: 20.7752
Epoch: 59, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5171, Acc: 0.7952, Speed: 330.4k, Time: 25.0262
Epoch: 59, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5170, Acc: 0.7953, Speed: 330.0k, Time: 29.1845
Epoch: 59, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5172, Acc: 0.7951, Speed: 330.3k, Time: 33.3760
Epoch: 59, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5177, Acc: 0.7948, Speed: 330.6k, Time: 37.5255
Train 0.7948
Val 0.8153
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263

skip saving model for perf <= 0.8157
Epoch: 60, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5126, Acc: 0.7976, Speed: 332.9k, Time: 3.9983
Epoch: 60, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5158, Acc: 0.7951, Speed: 340.9k, Time: 7.9688
Epoch: 60, Batch: 3000/9683, Batch size: 33, LR: 0.0500, Loss: 0.5170, Acc: 0.7943, Speed: 341.8k, Time: 11.9315
Epoch: 60, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5166, Acc: 0.7944, Speed: 344.2k, Time: 15.9053
Epoch: 60, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5172, Acc: 0.7942, Speed: 342.3k, Time: 20.0142
Epoch: 60, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5166, Acc: 0.7950, Speed: 343.9k, Time: 23.9853
Epoch: 60, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5160, Acc: 0.7951, Speed: 343.7k, Time: 27.9600
Epoch: 60, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5155, Acc: 0.7954, Speed: 344.4k, Time: 31.9219
Epoch: 60, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5159, Acc: 0.7953, Speed: 345.2k, Time: 35.8911
Train 0.7952
Val 0.8146
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551

skip saving model for perf <= 0.8157
Epoch: 61, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5132, Acc: 0.7966, Speed: 341.2k, Time: 4.0499
Epoch: 61, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5119, Acc: 0.7982, Speed: 344.7k, Time: 8.0281
Epoch: 61, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5120, Acc: 0.7983, Speed: 348.0k, Time: 11.9841
Epoch: 61, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5117, Acc: 0.7982, Speed: 347.1k, Time: 15.9529
Epoch: 61, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5136, Acc: 0.7971, Speed: 346.6k, Time: 19.9058
Epoch: 61, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5126, Acc: 0.7975, Speed: 346.6k, Time: 23.8548
Epoch: 61, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5138, Acc: 0.7968, Speed: 347.2k, Time: 27.8159
Epoch: 61, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5135, Acc: 0.7970, Speed: 347.4k, Time: 31.8097
Epoch: 61, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5141, Acc: 0.7967, Speed: 347.1k, Time: 35.7411
Train 0.7965
Val 0.8180
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006

saving model to local_200_parikh.pt
Epoch: 62, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5092, Acc: 0.7988, Speed: 336.6k, Time: 4.0447
Epoch: 62, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5098, Acc: 0.7981, Speed: 342.0k, Time: 8.0237
Epoch: 62, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5101, Acc: 0.7978, Speed: 347.1k, Time: 11.9776
Epoch: 62, Batch: 4000/9683, Batch size: 58, LR: 0.0500, Loss: 0.5101, Acc: 0.7983, Speed: 347.8k, Time: 15.9261
Epoch: 62, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5117, Acc: 0.7976, Speed: 346.4k, Time: 19.9105
Epoch: 62, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5121, Acc: 0.7973, Speed: 346.6k, Time: 23.8758
Epoch: 62, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5131, Acc: 0.7967, Speed: 346.5k, Time: 27.8458
Epoch: 62, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5127, Acc: 0.7971, Speed: 346.7k, Time: 31.7969
Epoch: 62, Batch: 9000/9683, Batch size: 15, LR: 0.0500, Loss: 0.5136, Acc: 0.7969, Speed: 346.3k, Time: 35.7965
Train 0.7968
Val 0.8179
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905

skip saving model for perf <= 0.8180
Epoch: 63, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5202, Acc: 0.7966, Speed: 338.5k, Time: 4.0185
Epoch: 63, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5200, Acc: 0.7965, Speed: 338.6k, Time: 8.0244
Epoch: 63, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5197, Acc: 0.7955, Speed: 343.0k, Time: 11.9878
Epoch: 63, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5180, Acc: 0.7960, Speed: 345.7k, Time: 15.9594
Epoch: 63, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5172, Acc: 0.7961, Speed: 345.7k, Time: 19.9351
Epoch: 63, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5158, Acc: 0.7967, Speed: 345.5k, Time: 23.8952
Epoch: 63, Batch: 7000/9683, Batch size: 5, LR: 0.0500, Loss: 0.5161, Acc: 0.7963, Speed: 345.1k, Time: 27.8546
Epoch: 63, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5169, Acc: 0.7959, Speed: 346.4k, Time: 31.8337
Epoch: 63, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5171, Acc: 0.7955, Speed: 346.2k, Time: 35.8010
Train 0.7955
Val 0.8171
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092

skip saving model for perf <= 0.8180
Epoch: 64, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5078, Acc: 0.7979, Speed: 335.2k, Time: 4.0513
Epoch: 64, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5077, Acc: 0.7979, Speed: 340.5k, Time: 8.0084
Epoch: 64, Batch: 3000/9683, Batch size: 5, LR: 0.0500, Loss: 0.5083, Acc: 0.7983, Speed: 343.2k, Time: 11.9614
Epoch: 64, Batch: 4000/9683, Batch size: 16, LR: 0.0500, Loss: 0.5104, Acc: 0.7974, Speed: 345.2k, Time: 15.9330
Epoch: 64, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5112, Acc: 0.7973, Speed: 346.9k, Time: 19.8836
Epoch: 64, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5116, Acc: 0.7976, Speed: 346.3k, Time: 23.8370
Epoch: 64, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5118, Acc: 0.7971, Speed: 346.8k, Time: 27.8242
Epoch: 64, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5118, Acc: 0.7972, Speed: 346.9k, Time: 31.7830
Epoch: 64, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5120, Acc: 0.7973, Speed: 346.4k, Time: 35.7561
Train 0.7975
Val 0.8182
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210

saving model to local_200_parikh.pt
Epoch: 65, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5101, Acc: 0.7965, Speed: 345.0k, Time: 4.0298
Epoch: 65, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5090, Acc: 0.7979, Speed: 346.7k, Time: 7.9870
Epoch: 65, Batch: 3000/9683, Batch size: 10, LR: 0.0500, Loss: 0.5106, Acc: 0.7976, Speed: 345.1k, Time: 11.9531
Epoch: 65, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5107, Acc: 0.7973, Speed: 347.0k, Time: 15.9272
Epoch: 65, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5111, Acc: 0.7973, Speed: 346.7k, Time: 19.8886
Epoch: 65, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5112, Acc: 0.7971, Speed: 347.3k, Time: 23.8482
Epoch: 65, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5116, Acc: 0.7970, Speed: 346.7k, Time: 27.7998
Epoch: 65, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5112, Acc: 0.7974, Speed: 346.3k, Time: 31.7797
Epoch: 65, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5110, Acc: 0.7976, Speed: 346.1k, Time: 35.7330
Train 0.7975
Val 0.8173
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295

skip saving model for perf <= 0.8182
Epoch: 66, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5051, Acc: 0.8008, Speed: 335.0k, Time: 4.0171
Epoch: 66, Batch: 2000/9683, Batch size: 32, LR: 0.0500, Loss: 0.5076, Acc: 0.7993, Speed: 343.9k, Time: 7.9658
Epoch: 66, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5083, Acc: 0.7992, Speed: 349.6k, Time: 11.7915
Epoch: 66, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5093, Acc: 0.7981, Speed: 355.2k, Time: 15.4766
Epoch: 66, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5095, Acc: 0.7982, Speed: 359.8k, Time: 19.1702
Epoch: 66, Batch: 6000/9683, Batch size: 28, LR: 0.0500, Loss: 0.5100, Acc: 0.7981, Speed: 360.9k, Time: 22.8568
Epoch: 66, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5093, Acc: 0.7984, Speed: 362.4k, Time: 26.5461
Epoch: 66, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5086, Acc: 0.7987, Speed: 364.0k, Time: 30.2327
Epoch: 66, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5093, Acc: 0.7983, Speed: 358.9k, Time: 34.5261
Train 0.7981
Val 0.8170
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990

skip saving model for perf <= 0.8182
Epoch: 67, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5115, Acc: 0.7978, Speed: 333.1k, Time: 4.0734
Epoch: 67, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5126, Acc: 0.7978, Speed: 326.5k, Time: 8.4224
Epoch: 67, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5122, Acc: 0.7980, Speed: 324.1k, Time: 12.7558
Epoch: 67, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5118, Acc: 0.7980, Speed: 322.3k, Time: 17.0832
Epoch: 67, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5125, Acc: 0.7976, Speed: 321.4k, Time: 21.4204
Epoch: 67, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5128, Acc: 0.7974, Speed: 321.2k, Time: 25.7802
Epoch: 67, Batch: 7000/9683, Batch size: 38, LR: 0.0500, Loss: 0.5129, Acc: 0.7973, Speed: 320.5k, Time: 30.1344
Epoch: 67, Batch: 8000/9683, Batch size: 52, LR: 0.0500, Loss: 0.5130, Acc: 0.7973, Speed: 319.6k, Time: 34.4674
Epoch: 67, Batch: 9000/9683, Batch size: 4, LR: 0.0500, Loss: 0.5128, Acc: 0.7974, Speed: 319.5k, Time: 38.8014
Train 0.7974
Val 0.8183
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311

saving model to local_200_parikh.pt
Epoch: 68, Batch: 1000/9683, Batch size: 1, LR: 0.0500, Loss: 0.5104, Acc: 0.7973, Speed: 335.8k, Time: 4.0453
Epoch: 68, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5130, Acc: 0.7968, Speed: 341.3k, Time: 8.0492
Epoch: 68, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5110, Acc: 0.7975, Speed: 341.7k, Time: 12.0407
Epoch: 68, Batch: 4000/9683, Batch size: 8, LR: 0.0500, Loss: 0.5101, Acc: 0.7981, Speed: 342.9k, Time: 16.0351
Epoch: 68, Batch: 5000/9683, Batch size: 8, LR: 0.0500, Loss: 0.5104, Acc: 0.7980, Speed: 344.7k, Time: 20.0423
Epoch: 68, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5106, Acc: 0.7979, Speed: 346.3k, Time: 24.0268
Epoch: 68, Batch: 7000/9683, Batch size: 40, LR: 0.0500, Loss: 0.5109, Acc: 0.7980, Speed: 346.0k, Time: 28.0123
Epoch: 68, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5102, Acc: 0.7984, Speed: 345.2k, Time: 32.0135
Epoch: 68, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5107, Acc: 0.7982, Speed: 344.9k, Time: 36.0024
Train 0.7984
Val 0.8175
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498

skip saving model for perf <= 0.8183
Epoch: 69, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5079, Acc: 0.8005, Speed: 334.4k, Time: 4.0558
Epoch: 69, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5070, Acc: 0.7999, Speed: 335.3k, Time: 8.1133
Epoch: 69, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5074, Acc: 0.7995, Speed: 337.8k, Time: 12.1285
Epoch: 69, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5070, Acc: 0.8001, Speed: 339.3k, Time: 16.1114
Epoch: 69, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5072, Acc: 0.7997, Speed: 340.9k, Time: 20.1475
Epoch: 69, Batch: 6000/9683, Batch size: 5, LR: 0.0500, Loss: 0.5070, Acc: 0.8000, Speed: 341.3k, Time: 24.1462
Epoch: 69, Batch: 7000/9683, Batch size: 8, LR: 0.0500, Loss: 0.5081, Acc: 0.7996, Speed: 341.9k, Time: 28.1613
Epoch: 69, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5083, Acc: 0.7995, Speed: 342.4k, Time: 32.1540
Epoch: 69, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5084, Acc: 0.7995, Speed: 342.9k, Time: 36.1411
Train 0.7994
Val 0.8176
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600

skip saving model for perf <= 0.8183
Epoch: 70, Batch: 1000/9683, Batch size: 2, LR: 0.0500, Loss: 0.5089, Acc: 0.7987, Speed: 341.3k, Time: 4.0617
Epoch: 70, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5053, Acc: 0.8003, Speed: 342.1k, Time: 8.0567
Epoch: 70, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5066, Acc: 0.7998, Speed: 343.6k, Time: 12.0472
Epoch: 70, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5041, Acc: 0.8008, Speed: 343.4k, Time: 16.0580
Epoch: 70, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5037, Acc: 0.8008, Speed: 341.5k, Time: 20.0975
Epoch: 70, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5053, Acc: 0.8002, Speed: 343.0k, Time: 24.1225
Epoch: 70, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5067, Acc: 0.7995, Speed: 343.3k, Time: 28.1646
Epoch: 70, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5069, Acc: 0.7995, Speed: 342.6k, Time: 32.1870
Epoch: 70, Batch: 9000/9683, Batch size: 26, LR: 0.0500, Loss: 0.5069, Acc: 0.7995, Speed: 342.6k, Time: 36.1814
Train 0.7995
Val 0.8196
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632

saving model to local_200_parikh.pt
Epoch: 71, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5038, Acc: 0.8015, Speed: 338.1k, Time: 4.0609
Epoch: 71, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5056, Acc: 0.8006, Speed: 342.4k, Time: 8.0602
Epoch: 71, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5068, Acc: 0.7997, Speed: 343.8k, Time: 12.0664
Epoch: 71, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5068, Acc: 0.7994, Speed: 343.1k, Time: 16.0630
Epoch: 71, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5085, Acc: 0.7986, Speed: 344.9k, Time: 20.0530
Epoch: 71, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5079, Acc: 0.7988, Speed: 345.0k, Time: 24.0476
Epoch: 71, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5077, Acc: 0.7988, Speed: 344.1k, Time: 28.0950
Epoch: 71, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5081, Acc: 0.7987, Speed: 343.4k, Time: 32.1011
Epoch: 71, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5074, Acc: 0.7990, Speed: 343.3k, Time: 36.1385
Train 0.7988
Val 0.8180
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006

skip saving model for perf <= 0.8196
Epoch: 72, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5078, Acc: 0.7991, Speed: 337.8k, Time: 4.0986
Epoch: 72, Batch: 2000/9683, Batch size: 1, LR: 0.0500, Loss: 0.5069, Acc: 0.7995, Speed: 340.3k, Time: 8.1031
Epoch: 72, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5063, Acc: 0.7999, Speed: 338.8k, Time: 12.1337
Epoch: 72, Batch: 4000/9683, Batch size: 52, LR: 0.0500, Loss: 0.5055, Acc: 0.7999, Speed: 342.3k, Time: 16.1320
Epoch: 72, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5060, Acc: 0.8000, Speed: 343.1k, Time: 20.1219
Epoch: 72, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5064, Acc: 0.7998, Speed: 343.1k, Time: 24.1215
Epoch: 72, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5059, Acc: 0.8000, Speed: 342.9k, Time: 28.1245
Epoch: 72, Batch: 8000/9683, Batch size: 1, LR: 0.0500, Loss: 0.5058, Acc: 0.8002, Speed: 343.1k, Time: 32.1278
Epoch: 72, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5062, Acc: 0.8001, Speed: 342.6k, Time: 36.1267
Train 0.7999
Val 0.8194
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429

skip saving model for perf <= 0.8196
Epoch: 73, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5049, Acc: 0.8011, Speed: 334.0k, Time: 4.0754
Epoch: 73, Batch: 2000/9683, Batch size: 25, LR: 0.0500, Loss: 0.5044, Acc: 0.8005, Speed: 340.9k, Time: 8.0764
Epoch: 73, Batch: 3000/9683, Batch size: 2, LR: 0.0500, Loss: 0.5041, Acc: 0.8002, Speed: 344.0k, Time: 12.0793
Epoch: 73, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5041, Acc: 0.8003, Speed: 343.8k, Time: 16.0688
Epoch: 73, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5056, Acc: 0.7998, Speed: 343.6k, Time: 20.0761
Epoch: 73, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5050, Acc: 0.8001, Speed: 342.4k, Time: 24.1094
Epoch: 73, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5054, Acc: 0.7998, Speed: 342.8k, Time: 28.1048
Epoch: 73, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5054, Acc: 0.7998, Speed: 343.0k, Time: 32.1092
Epoch: 73, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5057, Acc: 0.7998, Speed: 343.1k, Time: 36.1199
Train 0.7996
Val 0.8180
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006

skip saving model for perf <= 0.8196
Epoch: 74, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5023, Acc: 0.8012, Speed: 346.2k, Time: 4.0717
Epoch: 74, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5005, Acc: 0.8019, Speed: 342.8k, Time: 8.0914
Epoch: 74, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4991, Acc: 0.8025, Speed: 342.5k, Time: 12.0987
Epoch: 74, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5008, Acc: 0.8020, Speed: 343.5k, Time: 16.0756
Epoch: 74, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5023, Acc: 0.8015, Speed: 346.8k, Time: 19.8635
Epoch: 74, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5030, Acc: 0.8016, Speed: 351.2k, Time: 23.5956
Epoch: 74, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5036, Acc: 0.8014, Speed: 353.2k, Time: 27.3353
Epoch: 74, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5040, Acc: 0.8011, Speed: 355.5k, Time: 31.1140
Epoch: 74, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5045, Acc: 0.8009, Speed: 356.3k, Time: 34.8587
Train 0.8011
Val 0.8184
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413

skip saving model for perf <= 0.8196
Epoch: 75, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5064, Acc: 0.8003, Speed: 356.1k, Time: 3.8454
Epoch: 75, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5057, Acc: 0.8012, Speed: 362.1k, Time: 7.5966
Epoch: 75, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5025, Acc: 0.8023, Speed: 365.1k, Time: 11.3537
Epoch: 75, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5035, Acc: 0.8012, Speed: 366.2k, Time: 15.1080
Epoch: 75, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5033, Acc: 0.8013, Speed: 364.7k, Time: 18.8743
Epoch: 75, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5028, Acc: 0.8015, Speed: 365.2k, Time: 22.6258
Epoch: 75, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5031, Acc: 0.8014, Speed: 365.8k, Time: 26.3492
Epoch: 75, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5027, Acc: 0.8015, Speed: 365.7k, Time: 30.0990
Epoch: 75, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5029, Acc: 0.8012, Speed: 366.1k, Time: 33.8555
Train 0.8014
Val 0.8192
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226

skip saving model for perf <= 0.8196
Epoch: 76, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5041, Acc: 0.7997, Speed: 354.1k, Time: 3.9751
Epoch: 76, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5044, Acc: 0.8009, Speed: 355.3k, Time: 7.8482
Epoch: 76, Batch: 3000/9683, Batch size: 1, LR: 0.0500, Loss: 0.5028, Acc: 0.8016, Speed: 357.1k, Time: 11.5914
Epoch: 76, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5027, Acc: 0.8010, Speed: 360.3k, Time: 15.3444
Epoch: 76, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5016, Acc: 0.8016, Speed: 362.3k, Time: 19.0868
Epoch: 76, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5020, Acc: 0.8014, Speed: 363.5k, Time: 22.8399
Epoch: 76, Batch: 7000/9683, Batch size: 1, LR: 0.0500, Loss: 0.5014, Acc: 0.8017, Speed: 362.8k, Time: 26.5877
Epoch: 76, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5021, Acc: 0.8015, Speed: 362.8k, Time: 30.3433
Epoch: 76, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5016, Acc: 0.8016, Speed: 363.6k, Time: 34.1079
Train 0.8016
Val 0.8206
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648

saving model to local_200_parikh.pt
Epoch: 77, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5036, Acc: 0.7996, Speed: 340.0k, Time: 4.0768
Epoch: 77, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5024, Acc: 0.8003, Speed: 341.0k, Time: 8.0757
Epoch: 77, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5006, Acc: 0.8015, Speed: 341.3k, Time: 12.0618
Epoch: 77, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4992, Acc: 0.8021, Speed: 343.4k, Time: 16.0657
Epoch: 77, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4989, Acc: 0.8025, Speed: 342.9k, Time: 20.0815
Epoch: 77, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4991, Acc: 0.8025, Speed: 342.7k, Time: 24.0912
Epoch: 77, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5000, Acc: 0.8021, Speed: 343.0k, Time: 28.0949
Epoch: 77, Batch: 8000/9683, Batch size: 43, LR: 0.0500, Loss: 0.5007, Acc: 0.8020, Speed: 343.6k, Time: 32.0897
Epoch: 77, Batch: 9000/9683, Batch size: 4, LR: 0.0500, Loss: 0.5013, Acc: 0.8018, Speed: 343.6k, Time: 36.1150
Train 0.8017
Val 0.8200
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039

skip saving model for perf <= 0.8206
Epoch: 78, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5025, Acc: 0.8026, Speed: 336.7k, Time: 4.0806
Epoch: 78, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5021, Acc: 0.8032, Speed: 339.2k, Time: 8.1264
Epoch: 78, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5014, Acc: 0.8028, Speed: 342.4k, Time: 12.1673
Epoch: 78, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5024, Acc: 0.8023, Speed: 341.9k, Time: 16.1643
Epoch: 78, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5029, Acc: 0.8017, Speed: 341.5k, Time: 20.1820
Epoch: 78, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5027, Acc: 0.8018, Speed: 342.8k, Time: 24.1854
Epoch: 78, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5035, Acc: 0.8014, Speed: 342.5k, Time: 28.1840
Epoch: 78, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5040, Acc: 0.8009, Speed: 342.5k, Time: 32.1914
Epoch: 78, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5035, Acc: 0.8011, Speed: 342.3k, Time: 36.2278
Train 0.8009
Val 0.8185
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514

skip saving model for perf <= 0.8206
Epoch: 79, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4953, Acc: 0.8044, Speed: 340.6k, Time: 4.0544
Epoch: 79, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5010, Acc: 0.8023, Speed: 343.3k, Time: 8.1282
Epoch: 79, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5011, Acc: 0.8020, Speed: 343.3k, Time: 12.1332
Epoch: 79, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5033, Acc: 0.8009, Speed: 342.4k, Time: 16.1300
Epoch: 79, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5038, Acc: 0.8005, Speed: 343.5k, Time: 20.1250
Epoch: 79, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5031, Acc: 0.8011, Speed: 343.1k, Time: 24.1303
Epoch: 79, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5035, Acc: 0.8009, Speed: 342.5k, Time: 28.1786
Epoch: 79, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5038, Acc: 0.8009, Speed: 342.6k, Time: 32.1879
Epoch: 79, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5039, Acc: 0.8008, Speed: 343.0k, Time: 36.2060
Train 0.8007
Val 0.8210
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953

saving model to local_200_parikh.pt
Epoch: 80, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5031, Acc: 0.8012, Speed: 334.1k, Time: 4.0678
Epoch: 80, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5005, Acc: 0.8025, Speed: 338.8k, Time: 8.0539
Epoch: 80, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4998, Acc: 0.8024, Speed: 347.2k, Time: 11.8520
Epoch: 80, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5007, Acc: 0.8022, Speed: 352.1k, Time: 15.5933
Epoch: 80, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5017, Acc: 0.8020, Speed: 354.4k, Time: 19.3355
Epoch: 80, Batch: 6000/9683, Batch size: 5, LR: 0.0500, Loss: 0.5019, Acc: 0.8019, Speed: 357.4k, Time: 23.1038
Epoch: 80, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5027, Acc: 0.8016, Speed: 358.7k, Time: 26.8548
Epoch: 80, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5045, Acc: 0.8008, Speed: 360.1k, Time: 30.7013
Epoch: 80, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5039, Acc: 0.8011, Speed: 360.0k, Time: 34.5298
Train 0.8010
Val 0.8212
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156

saving model to local_200_parikh.pt
Epoch: 81, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5061, Acc: 0.8013, Speed: 343.8k, Time: 4.0477
Epoch: 81, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5002, Acc: 0.8035, Speed: 344.3k, Time: 8.0601
Epoch: 81, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5015, Acc: 0.8028, Speed: 345.0k, Time: 12.0855
Epoch: 81, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5019, Acc: 0.8026, Speed: 344.4k, Time: 16.1104
Epoch: 81, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5015, Acc: 0.8026, Speed: 344.1k, Time: 20.1165
Epoch: 81, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5018, Acc: 0.8023, Speed: 343.4k, Time: 24.1218
Epoch: 81, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5014, Acc: 0.8025, Speed: 343.8k, Time: 28.1192
Epoch: 81, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5017, Acc: 0.8022, Speed: 343.9k, Time: 32.1107
Epoch: 81, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5010, Acc: 0.8024, Speed: 343.6k, Time: 36.1141
Train 0.8025
Val 0.8215
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461

saving model to local_200_parikh.pt
Epoch: 82, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4913, Acc: 0.8060, Speed: 335.3k, Time: 4.0913
Epoch: 82, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4957, Acc: 0.8044, Speed: 340.1k, Time: 8.0877
Epoch: 82, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4969, Acc: 0.8039, Speed: 343.8k, Time: 12.1135
Epoch: 82, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4979, Acc: 0.8031, Speed: 343.4k, Time: 16.1368
Epoch: 82, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4984, Acc: 0.8030, Speed: 344.7k, Time: 20.1382
Epoch: 82, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4993, Acc: 0.8029, Speed: 344.1k, Time: 24.1382
Epoch: 82, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4988, Acc: 0.8029, Speed: 343.1k, Time: 28.1325
Epoch: 82, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4990, Acc: 0.8028, Speed: 342.7k, Time: 32.1175
Epoch: 82, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4994, Acc: 0.8026, Speed: 342.8k, Time: 36.1377
Train 0.8025
Val 0.8216
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563

saving model to local_200_parikh.pt
Epoch: 83, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5026, Acc: 0.8002, Speed: 336.7k, Time: 4.0633
Epoch: 83, Batch: 2000/9683, Batch size: 4, LR: 0.0500, Loss: 0.5003, Acc: 0.8018, Speed: 340.9k, Time: 8.0605
Epoch: 83, Batch: 3000/9683, Batch size: 5, LR: 0.0500, Loss: 0.4974, Acc: 0.8038, Speed: 340.5k, Time: 12.0667
Epoch: 83, Batch: 4000/9683, Batch size: 8, LR: 0.0500, Loss: 0.4976, Acc: 0.8037, Speed: 342.2k, Time: 16.0607
Epoch: 83, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4986, Acc: 0.8032, Speed: 344.0k, Time: 20.0698
Epoch: 83, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4989, Acc: 0.8030, Speed: 344.1k, Time: 24.1019
Epoch: 83, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4992, Acc: 0.8029, Speed: 344.0k, Time: 28.1270
Epoch: 83, Batch: 8000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4988, Acc: 0.8029, Speed: 344.1k, Time: 32.1431
Epoch: 83, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4993, Acc: 0.8027, Speed: 343.9k, Time: 36.1286
Train 0.8027
Val 0.8207
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750

skip saving model for perf <= 0.8216
Epoch: 84, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5019, Acc: 0.8014, Speed: 337.3k, Time: 4.0706
Epoch: 84, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4978, Acc: 0.8026, Speed: 338.5k, Time: 8.0849
Epoch: 84, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4990, Acc: 0.8023, Speed: 339.6k, Time: 12.1140
Epoch: 84, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4988, Acc: 0.8025, Speed: 340.8k, Time: 16.1143
Epoch: 84, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4984, Acc: 0.8026, Speed: 341.1k, Time: 20.1517
Epoch: 84, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4979, Acc: 0.8029, Speed: 342.9k, Time: 24.1778
Epoch: 84, Batch: 7000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4975, Acc: 0.8032, Speed: 342.5k, Time: 28.1626
Epoch: 84, Batch: 8000/9683, Batch size: 17, LR: 0.0500, Loss: 0.4979, Acc: 0.8030, Speed: 342.4k, Time: 32.1740
Epoch: 84, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4987, Acc: 0.8027, Speed: 342.8k, Time: 36.1608
Train 0.8026
Val 0.8220
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969

saving model to local_200_parikh.pt
Epoch: 85, Batch: 1000/9683, Batch size: 6, LR: 0.0500, Loss: 0.4940, Acc: 0.8068, Speed: 330.7k, Time: 4.1057
Epoch: 85, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4941, Acc: 0.8051, Speed: 337.2k, Time: 8.0932
Epoch: 85, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4943, Acc: 0.8049, Speed: 342.4k, Time: 12.0940
Epoch: 85, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4956, Acc: 0.8041, Speed: 342.0k, Time: 16.1009
Epoch: 85, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4957, Acc: 0.8043, Speed: 342.7k, Time: 20.1041
Epoch: 85, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4973, Acc: 0.8037, Speed: 343.0k, Time: 24.1083
Epoch: 85, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4981, Acc: 0.8032, Speed: 342.9k, Time: 28.0908
Epoch: 85, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4989, Acc: 0.8029, Speed: 344.2k, Time: 32.1026
Epoch: 85, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4990, Acc: 0.8028, Speed: 343.1k, Time: 36.1704
Train 0.8025
Val 0.8220
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969

skip saving model for perf <= 0.8220
Epoch: 86, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5028, Acc: 0.8017, Speed: 334.9k, Time: 4.1065
Epoch: 86, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5022, Acc: 0.8015, Speed: 338.2k, Time: 8.1159
Epoch: 86, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4976, Acc: 0.8036, Speed: 339.0k, Time: 12.1178
Epoch: 86, Batch: 4000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4966, Acc: 0.8043, Speed: 338.8k, Time: 16.1277
Epoch: 86, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4983, Acc: 0.8037, Speed: 342.1k, Time: 20.1407
Epoch: 86, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4982, Acc: 0.8038, Speed: 342.2k, Time: 24.1328
Epoch: 86, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4986, Acc: 0.8035, Speed: 341.9k, Time: 28.1475
Epoch: 86, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4990, Acc: 0.8034, Speed: 341.3k, Time: 32.1764
Epoch: 86, Batch: 9000/9683, Batch size: 57, LR: 0.0500, Loss: 0.4991, Acc: 0.8033, Speed: 342.4k, Time: 36.1755
Train 0.8034
Val 0.8222
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173

saving model to local_200_parikh.pt
Epoch: 87, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4957, Acc: 0.8045, Speed: 335.9k, Time: 4.1043
Epoch: 87, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4958, Acc: 0.8040, Speed: 339.6k, Time: 8.0987
Epoch: 87, Batch: 3000/9683, Batch size: 55, LR: 0.0500, Loss: 0.4962, Acc: 0.8039, Speed: 342.1k, Time: 12.1111
Epoch: 87, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4972, Acc: 0.8037, Speed: 344.5k, Time: 16.1088
Epoch: 87, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4967, Acc: 0.8039, Speed: 344.2k, Time: 20.1152
Epoch: 87, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4963, Acc: 0.8041, Speed: 343.7k, Time: 24.1086
Epoch: 87, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4961, Acc: 0.8043, Speed: 343.7k, Time: 28.1070
Epoch: 87, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4968, Acc: 0.8039, Speed: 343.8k, Time: 32.1277
Epoch: 87, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4977, Acc: 0.8034, Speed: 343.9k, Time: 36.1275
Train 0.8033
Val 0.8211
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055

skip saving model for perf <= 0.8222
Epoch: 88, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4962, Acc: 0.8054, Speed: 336.5k, Time: 4.0598
Epoch: 88, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4979, Acc: 0.8034, Speed: 341.0k, Time: 8.0677
Epoch: 88, Batch: 3000/9683, Batch size: 29, LR: 0.0500, Loss: 0.4977, Acc: 0.8035, Speed: 342.3k, Time: 12.0767
Epoch: 88, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4975, Acc: 0.8036, Speed: 341.7k, Time: 16.0744
Epoch: 88, Batch: 5000/9683, Batch size: 4, LR: 0.0500, Loss: 0.4976, Acc: 0.8039, Speed: 341.4k, Time: 20.0661
Epoch: 88, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4974, Acc: 0.8040, Speed: 341.8k, Time: 24.1200
Epoch: 88, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4973, Acc: 0.8039, Speed: 341.9k, Time: 28.1287
Epoch: 88, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4973, Acc: 0.8039, Speed: 342.7k, Time: 32.1198
Epoch: 88, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4973, Acc: 0.8040, Speed: 343.3k, Time: 36.1138
Train 0.8041
Val 0.8230
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985

saving model to local_200_parikh.pt
Epoch: 89, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5004, Acc: 0.8036, Speed: 336.5k, Time: 4.0655
Epoch: 89, Batch: 2000/9683, Batch size: 20, LR: 0.0500, Loss: 0.5057, Acc: 0.8009, Speed: 339.2k, Time: 8.0745
Epoch: 89, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5040, Acc: 0.8012, Speed: 344.2k, Time: 12.0079
Epoch: 89, Batch: 4000/9683, Batch size: 11, LR: 0.0500, Loss: 0.5025, Acc: 0.8016, Speed: 349.7k, Time: 15.7496
Epoch: 89, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.5007, Acc: 0.8026, Speed: 353.1k, Time: 19.4955
Epoch: 89, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4998, Acc: 0.8031, Speed: 354.5k, Time: 23.2700
Epoch: 89, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4989, Acc: 0.8036, Speed: 356.3k, Time: 27.0114
Epoch: 89, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4982, Acc: 0.8038, Speed: 357.2k, Time: 30.8086
Epoch: 89, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4983, Acc: 0.8039, Speed: 358.1k, Time: 34.5933
Train 0.8036
Val 0.8225
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477

skip saving model for perf <= 0.8230
Epoch: 90, Batch: 1000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4993, Acc: 0.8037, Speed: 344.9k, Time: 4.0248
Epoch: 90, Batch: 2000/9683, Batch size: 11, LR: 0.0500, Loss: 0.4961, Acc: 0.8040, Speed: 340.5k, Time: 8.0553
Epoch: 90, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4967, Acc: 0.8036, Speed: 345.5k, Time: 12.0612
Epoch: 90, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4958, Acc: 0.8043, Speed: 344.5k, Time: 16.0432
Epoch: 90, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4963, Acc: 0.8040, Speed: 343.9k, Time: 20.0557
Epoch: 90, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4959, Acc: 0.8041, Speed: 344.0k, Time: 24.0772
Epoch: 90, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4959, Acc: 0.8041, Speed: 343.4k, Time: 28.0891
Epoch: 90, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4955, Acc: 0.8044, Speed: 344.1k, Time: 32.0829
Epoch: 90, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4960, Acc: 0.8043, Speed: 344.4k, Time: 36.0919
Train 0.8043
Val 0.8226
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579

skip saving model for perf <= 0.8230
Epoch: 91, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4932, Acc: 0.8054, Speed: 337.5k, Time: 4.0810
Epoch: 91, Batch: 2000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4921, Acc: 0.8058, Speed: 340.2k, Time: 8.1119
Epoch: 91, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4907, Acc: 0.8067, Speed: 341.1k, Time: 12.1155
Epoch: 91, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4930, Acc: 0.8053, Speed: 341.0k, Time: 16.1248
Epoch: 91, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4940, Acc: 0.8046, Speed: 341.1k, Time: 20.1218
Epoch: 91, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4944, Acc: 0.8047, Speed: 341.1k, Time: 24.1278
Epoch: 91, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4953, Acc: 0.8044, Speed: 341.3k, Time: 28.1526
Epoch: 91, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4961, Acc: 0.8040, Speed: 341.7k, Time: 32.1542
Epoch: 91, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4965, Acc: 0.8037, Speed: 343.2k, Time: 36.1679
Train 0.8037
Val 0.8230
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985

skip saving model for perf <= 0.8230
Epoch: 92, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4935, Acc: 0.8036, Speed: 340.1k, Time: 4.0601
Epoch: 92, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4959, Acc: 0.8038, Speed: 341.1k, Time: 8.0738
Epoch: 92, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4951, Acc: 0.8048, Speed: 342.1k, Time: 12.0566
Epoch: 92, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4953, Acc: 0.8045, Speed: 343.5k, Time: 16.0380
Epoch: 92, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4949, Acc: 0.8048, Speed: 344.4k, Time: 20.0493
Epoch: 92, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4950, Acc: 0.8049, Speed: 344.7k, Time: 24.0435
Epoch: 92, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4958, Acc: 0.8045, Speed: 343.4k, Time: 28.0582
Epoch: 92, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4958, Acc: 0.8043, Speed: 344.2k, Time: 32.0538
Epoch: 92, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4955, Acc: 0.8044, Speed: 344.0k, Time: 36.0528
Train 0.8043
Val 0.8249
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916

saving model to local_200_parikh.pt
Epoch: 93, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4983, Acc: 0.8036, Speed: 340.7k, Time: 4.0746
Epoch: 93, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4953, Acc: 0.8048, Speed: 345.1k, Time: 8.0575
Epoch: 93, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4955, Acc: 0.8045, Speed: 344.2k, Time: 12.0569
Epoch: 93, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4960, Acc: 0.8042, Speed: 345.8k, Time: 16.0521
Epoch: 93, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4955, Acc: 0.8042, Speed: 345.2k, Time: 20.0868
Epoch: 93, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4955, Acc: 0.8044, Speed: 344.3k, Time: 24.0855
Epoch: 93, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4954, Acc: 0.8048, Speed: 344.1k, Time: 28.1091
Epoch: 93, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4961, Acc: 0.8044, Speed: 343.8k, Time: 32.1061
Epoch: 93, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4959, Acc: 0.8045, Speed: 343.9k, Time: 36.1096
Train 0.8046
Val 0.8249
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916

skip saving model for perf <= 0.8249
Epoch: 94, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4957, Acc: 0.8023, Speed: 335.0k, Time: 4.1369
Epoch: 94, Batch: 2000/9683, Batch size: 11, LR: 0.0500, Loss: 0.4927, Acc: 0.8050, Speed: 342.1k, Time: 8.1420
Epoch: 94, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4928, Acc: 0.8049, Speed: 343.3k, Time: 12.1443
Epoch: 94, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4948, Acc: 0.8039, Speed: 341.4k, Time: 16.1463
Epoch: 94, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4950, Acc: 0.8040, Speed: 343.0k, Time: 20.1443
Epoch: 94, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4950, Acc: 0.8042, Speed: 344.0k, Time: 24.1679
Epoch: 94, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4949, Acc: 0.8042, Speed: 344.2k, Time: 28.1645
Epoch: 94, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4939, Acc: 0.8047, Speed: 343.1k, Time: 32.1732
Epoch: 94, Batch: 9000/9683, Batch size: 32, LR: 0.0500, Loss: 0.4939, Acc: 0.8048, Speed: 342.7k, Time: 36.1697
Train 0.8047
Val 0.8234
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392

skip saving model for perf <= 0.8249
Epoch: 95, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4897, Acc: 0.8070, Speed: 336.3k, Time: 4.0889
Epoch: 95, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4920, Acc: 0.8055, Speed: 340.1k, Time: 8.1445
Epoch: 95, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4926, Acc: 0.8057, Speed: 341.7k, Time: 12.1434
Epoch: 95, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4932, Acc: 0.8056, Speed: 346.7k, Time: 15.9885
Epoch: 95, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4935, Acc: 0.8055, Speed: 348.8k, Time: 19.7885
Epoch: 95, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4933, Acc: 0.8058, Speed: 352.1k, Time: 23.5281
Epoch: 95, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4931, Acc: 0.8059, Speed: 353.9k, Time: 27.2696
Epoch: 95, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4925, Acc: 0.8059, Speed: 355.6k, Time: 31.0105
Epoch: 95, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4931, Acc: 0.8058, Speed: 356.8k, Time: 34.7539
Train 0.8059
Val 0.8243
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306

skip saving model for perf <= 0.8249
Epoch: 96, Batch: 1000/9683, Batch size: 9, LR: 0.0500, Loss: 0.4956, Acc: 0.8032, Speed: 343.0k, Time: 4.0127
Epoch: 96, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4955, Acc: 0.8046, Speed: 344.3k, Time: 8.0184
Epoch: 96, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4959, Acc: 0.8044, Speed: 343.1k, Time: 12.0345
Epoch: 96, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4959, Acc: 0.8044, Speed: 343.9k, Time: 16.0498
Epoch: 96, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4962, Acc: 0.8042, Speed: 343.5k, Time: 20.0691
Epoch: 96, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4959, Acc: 0.8044, Speed: 342.9k, Time: 24.0519
Epoch: 96, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4956, Acc: 0.8047, Speed: 343.9k, Time: 28.0607
Epoch: 96, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4956, Acc: 0.8046, Speed: 343.6k, Time: 32.0828
Epoch: 96, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4957, Acc: 0.8046, Speed: 343.3k, Time: 36.0649
Train 0.8044
Val 0.8202
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242

skip saving model for perf <= 0.8249
Epoch: 97, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4920, Acc: 0.8043, Speed: 339.1k, Time: 4.0850
Epoch: 97, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4925, Acc: 0.8048, Speed: 338.9k, Time: 8.0948
Epoch: 97, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4927, Acc: 0.8047, Speed: 341.7k, Time: 12.1015
Epoch: 97, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4933, Acc: 0.8052, Speed: 341.4k, Time: 16.1087
Epoch: 97, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4925, Acc: 0.8057, Speed: 341.4k, Time: 20.1051
Epoch: 97, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4926, Acc: 0.8056, Speed: 342.4k, Time: 24.1291
Epoch: 97, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4918, Acc: 0.8060, Speed: 342.8k, Time: 28.1243
Epoch: 97, Batch: 8000/9683, Batch size: 6, LR: 0.0500, Loss: 0.4922, Acc: 0.8059, Speed: 343.3k, Time: 32.1447
Epoch: 97, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4925, Acc: 0.8059, Speed: 343.0k, Time: 36.1689
Train 0.8055
Val 0.8238
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798

skip saving model for perf <= 0.8249
Epoch: 98, Batch: 1000/9683, Batch size: 59, LR: 0.0500, Loss: 0.4933, Acc: 0.8048, Speed: 336.8k, Time: 4.0738
Epoch: 98, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4924, Acc: 0.8055, Speed: 340.6k, Time: 8.0715
Epoch: 98, Batch: 3000/9683, Batch size: 54, LR: 0.0500, Loss: 0.4933, Acc: 0.8056, Speed: 341.2k, Time: 12.0925
Epoch: 98, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4935, Acc: 0.8052, Speed: 343.4k, Time: 16.0542
Epoch: 98, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4940, Acc: 0.8049, Speed: 344.6k, Time: 20.0428
Epoch: 98, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4938, Acc: 0.8049, Speed: 344.9k, Time: 24.0403
Epoch: 98, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4941, Acc: 0.8047, Speed: 343.8k, Time: 28.0525
Epoch: 98, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4941, Acc: 0.8050, Speed: 344.7k, Time: 32.0451
Epoch: 98, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4940, Acc: 0.8049, Speed: 343.8k, Time: 36.0717
Train 0.8049
Val 0.8224
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376

skip saving model for perf <= 0.8249
Epoch: 99, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4947, Acc: 0.8049, Speed: 335.7k, Time: 4.0584
Epoch: 99, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4940, Acc: 0.8062, Speed: 337.3k, Time: 8.0782
Epoch: 99, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4919, Acc: 0.8072, Speed: 341.0k, Time: 12.0781
Epoch: 99, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4932, Acc: 0.8058, Speed: 342.7k, Time: 16.0851
Epoch: 99, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4931, Acc: 0.8058, Speed: 342.9k, Time: 20.0960
Epoch: 99, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4932, Acc: 0.8056, Speed: 343.6k, Time: 24.1224
Epoch: 99, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4937, Acc: 0.8053, Speed: 342.6k, Time: 28.1336
Epoch: 99, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4942, Acc: 0.8051, Speed: 342.9k, Time: 32.1292
Epoch: 99, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4940, Acc: 0.8052, Speed: 343.2k, Time: 36.1126
Train 0.8051
Val 0.8241
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103

skip saving model for perf <= 0.8249
Epoch: 100, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4938, Acc: 0.8040, Speed: 340.9k, Time: 4.0691
Epoch: 100, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4906, Acc: 0.8062, Speed: 341.8k, Time: 8.0633
Epoch: 100, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4905, Acc: 0.8066, Speed: 343.7k, Time: 12.0534
Epoch: 100, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4908, Acc: 0.8065, Speed: 345.2k, Time: 16.0359
Epoch: 100, Batch: 5000/9683, Batch size: 3, LR: 0.0500, Loss: 0.4906, Acc: 0.8064, Speed: 344.5k, Time: 20.0482
Epoch: 100, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4904, Acc: 0.8065, Speed: 343.0k, Time: 24.0657
Epoch: 100, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4902, Acc: 0.8068, Speed: 343.0k, Time: 28.0796
Epoch: 100, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4909, Acc: 0.8066, Speed: 344.1k, Time: 32.0584
Epoch: 100, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4908, Acc: 0.8066, Speed: 343.7k, Time: 36.0630
Train 0.8064
Val 0.8221
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071

skip saving model for perf <= 0.8249
Epoch: 101, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4926, Acc: 0.8072, Speed: 334.4k, Time: 4.0666
Epoch: 101, Batch: 2000/9683, Batch size: 33, LR: 0.0500, Loss: 0.4911, Acc: 0.8070, Speed: 340.5k, Time: 8.0705
Epoch: 101, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4925, Acc: 0.8061, Speed: 344.1k, Time: 12.0594
Epoch: 101, Batch: 4000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4907, Acc: 0.8065, Speed: 344.4k, Time: 16.0661
Epoch: 101, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4913, Acc: 0.8066, Speed: 344.0k, Time: 20.0659
Epoch: 101, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4908, Acc: 0.8071, Speed: 342.1k, Time: 24.0782
Epoch: 101, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4921, Acc: 0.8064, Speed: 342.4k, Time: 28.0829
Epoch: 101, Batch: 8000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4930, Acc: 0.8059, Speed: 343.2k, Time: 32.0970
Epoch: 101, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4934, Acc: 0.8057, Speed: 343.0k, Time: 36.0801
Train 0.8057
Val 0.8236
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595

skip saving model for perf <= 0.8249
Epoch: 102, Batch: 1000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4925, Acc: 0.8047, Speed: 347.7k, Time: 4.0666
Epoch: 102, Batch: 2000/9683, Batch size: 54, LR: 0.0500, Loss: 0.4895, Acc: 0.8055, Speed: 344.8k, Time: 8.0625
Epoch: 102, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4889, Acc: 0.8058, Speed: 343.8k, Time: 12.0599
Epoch: 102, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4893, Acc: 0.8063, Speed: 343.8k, Time: 16.0805
Epoch: 102, Batch: 5000/9683, Batch size: 5, LR: 0.0500, Loss: 0.4905, Acc: 0.8061, Speed: 342.5k, Time: 20.0922
Epoch: 102, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4914, Acc: 0.8056, Speed: 342.8k, Time: 24.0756
Epoch: 102, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4907, Acc: 0.8062, Speed: 343.0k, Time: 28.0942
Epoch: 102, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4908, Acc: 0.8063, Speed: 343.2k, Time: 32.1068
Epoch: 102, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4907, Acc: 0.8064, Speed: 342.9k, Time: 36.1143
Train 0.8063
Val 0.8243
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306

skip saving model for perf <= 0.8249
Epoch: 103, Batch: 1000/9683, Batch size: 32, LR: 0.0500, Loss: 0.4899, Acc: 0.8070, Speed: 336.2k, Time: 4.0982
Epoch: 103, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4906, Acc: 0.8070, Speed: 324.0k, Time: 8.5258
Epoch: 103, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4902, Acc: 0.8078, Speed: 307.7k, Time: 13.4635
Epoch: 103, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4899, Acc: 0.8079, Speed: 300.2k, Time: 18.4212
Epoch: 103, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4912, Acc: 0.8072, Speed: 296.5k, Time: 23.3735
Epoch: 103, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4909, Acc: 0.8074, Speed: 293.2k, Time: 28.2093
Epoch: 103, Batch: 7000/9683, Batch size: 5, LR: 0.0500, Loss: 0.4907, Acc: 0.8073, Speed: 291.2k, Time: 33.1459
Epoch: 103, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4901, Acc: 0.8075, Speed: 289.9k, Time: 38.0325
Epoch: 103, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4910, Acc: 0.8070, Speed: 289.1k, Time: 42.9176
Train 0.8071
Val 0.8250
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018

saving model to local_200_parikh.pt
Epoch: 104, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4950, Acc: 0.8036, Speed: 320.8k, Time: 4.3514
Epoch: 104, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4929, Acc: 0.8053, Speed: 325.3k, Time: 8.5362
Epoch: 104, Batch: 3000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4914, Acc: 0.8056, Speed: 322.4k, Time: 12.8701
Epoch: 104, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4911, Acc: 0.8060, Speed: 320.0k, Time: 17.2490
Epoch: 104, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4910, Acc: 0.8061, Speed: 321.4k, Time: 21.4830
Epoch: 104, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4908, Acc: 0.8063, Speed: 321.5k, Time: 25.8067
Epoch: 104, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4904, Acc: 0.8065, Speed: 322.4k, Time: 30.0668
Epoch: 104, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4901, Acc: 0.8066, Speed: 322.7k, Time: 34.3025
Epoch: 104, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4896, Acc: 0.8068, Speed: 321.7k, Time: 38.6240
Train 0.8068
Val 0.8265
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542

saving model to local_200_parikh.pt
Epoch: 105, Batch: 1000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4878, Acc: 0.8087, Speed: 309.4k, Time: 4.3493
Epoch: 105, Batch: 2000/9683, Batch size: 30, LR: 0.0500, Loss: 0.4875, Acc: 0.8087, Speed: 316.9k, Time: 8.6579
Epoch: 105, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4857, Acc: 0.8095, Speed: 317.3k, Time: 13.0247
Epoch: 105, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4868, Acc: 0.8090, Speed: 318.8k, Time: 17.3744
Epoch: 105, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4886, Acc: 0.8078, Speed: 319.6k, Time: 21.6680
Epoch: 105, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4886, Acc: 0.8079, Speed: 319.7k, Time: 25.9708
Epoch: 105, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4892, Acc: 0.8074, Speed: 320.3k, Time: 30.2679
Epoch: 105, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4887, Acc: 0.8073, Speed: 319.8k, Time: 34.5728
Epoch: 105, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4890, Acc: 0.8072, Speed: 319.2k, Time: 38.8685
Train 0.8071
Val 0.8263
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339

skip saving model for perf <= 0.8265
Epoch: 106, Batch: 1000/9683, Batch size: 3, LR: 0.0500, Loss: 0.4913, Acc: 0.8074, Speed: 319.4k, Time: 4.3240
Epoch: 106, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4913, Acc: 0.8067, Speed: 317.4k, Time: 8.6687
Epoch: 106, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4894, Acc: 0.8077, Speed: 314.3k, Time: 13.0270
Epoch: 106, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4887, Acc: 0.8079, Speed: 314.5k, Time: 17.3828
Epoch: 106, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4885, Acc: 0.8080, Speed: 315.7k, Time: 21.7198
Epoch: 106, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4882, Acc: 0.8083, Speed: 316.8k, Time: 26.0435
Epoch: 106, Batch: 7000/9683, Batch size: 35, LR: 0.0500, Loss: 0.4885, Acc: 0.8081, Speed: 317.3k, Time: 30.3192
Epoch: 106, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4886, Acc: 0.8081, Speed: 317.9k, Time: 34.6406
Epoch: 106, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4884, Acc: 0.8079, Speed: 318.6k, Time: 38.9037
Train 0.8079
Val 0.8242
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205

skip saving model for perf <= 0.8265
Epoch: 107, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4851, Acc: 0.8101, Speed: 316.8k, Time: 4.3303
Epoch: 107, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4868, Acc: 0.8095, Speed: 319.1k, Time: 8.5896
Epoch: 107, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4855, Acc: 0.8098, Speed: 321.1k, Time: 12.8075
Epoch: 107, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4859, Acc: 0.8093, Speed: 323.5k, Time: 17.0937
Epoch: 107, Batch: 5000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4871, Acc: 0.8086, Speed: 324.2k, Time: 21.3776
Epoch: 107, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4867, Acc: 0.8087, Speed: 324.7k, Time: 25.6238
Epoch: 107, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4869, Acc: 0.8088, Speed: 323.8k, Time: 29.9294
Epoch: 107, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4871, Acc: 0.8086, Speed: 323.3k, Time: 34.2367
Epoch: 107, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4874, Acc: 0.8086, Speed: 323.3k, Time: 38.4402
Train 0.8086
Val 0.8258
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831

skip saving model for perf <= 0.8265
Epoch: 108, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4893, Acc: 0.8054, Speed: 321.3k, Time: 4.2621
Epoch: 108, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4892, Acc: 0.8067, Speed: 322.3k, Time: 8.5818
Epoch: 108, Batch: 3000/9683, Batch size: 5, LR: 0.0500, Loss: 0.4867, Acc: 0.8081, Speed: 320.2k, Time: 12.8902
Epoch: 108, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4869, Acc: 0.8086, Speed: 321.1k, Time: 17.1958
Epoch: 108, Batch: 5000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4862, Acc: 0.8085, Speed: 321.4k, Time: 21.4463
Epoch: 108, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4870, Acc: 0.8079, Speed: 322.1k, Time: 25.6803
Epoch: 108, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4858, Acc: 0.8084, Speed: 322.0k, Time: 29.9386
Epoch: 108, Batch: 8000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4863, Acc: 0.8081, Speed: 322.1k, Time: 34.2161
Epoch: 108, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4865, Acc: 0.8079, Speed: 322.3k, Time: 38.4658
Train 0.8079
Val 0.8265
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542

skip saving model for perf <= 0.8265
Epoch: 109, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4902, Acc: 0.8063, Speed: 317.7k, Time: 4.3825
Epoch: 109, Batch: 2000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4899, Acc: 0.8071, Speed: 318.9k, Time: 8.6473
Epoch: 109, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4887, Acc: 0.8071, Speed: 318.4k, Time: 12.9557
Epoch: 109, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4873, Acc: 0.8076, Speed: 319.3k, Time: 17.3244
Epoch: 109, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4878, Acc: 0.8076, Speed: 319.8k, Time: 21.5739
Epoch: 109, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4874, Acc: 0.8077, Speed: 319.1k, Time: 25.9248
Epoch: 109, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4876, Acc: 0.8076, Speed: 318.4k, Time: 30.3190
Epoch: 109, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4873, Acc: 0.8077, Speed: 319.5k, Time: 34.5070
Epoch: 109, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4874, Acc: 0.8076, Speed: 320.1k, Time: 38.7003
Train 0.8075
Val 0.8277
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660

saving model to local_200_parikh.pt
Epoch: 110, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4838, Acc: 0.8081, Speed: 306.8k, Time: 4.3898
Epoch: 110, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4868, Acc: 0.8073, Speed: 312.1k, Time: 8.7128
Epoch: 110, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4853, Acc: 0.8078, Speed: 315.7k, Time: 13.0482
Epoch: 110, Batch: 4000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4869, Acc: 0.8074, Speed: 318.3k, Time: 17.3158
Epoch: 110, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4866, Acc: 0.8079, Speed: 317.7k, Time: 21.5952
Epoch: 110, Batch: 6000/9683, Batch size: 5, LR: 0.0500, Loss: 0.4868, Acc: 0.8076, Speed: 317.7k, Time: 25.9167
Epoch: 110, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4875, Acc: 0.8074, Speed: 318.7k, Time: 30.1787
Epoch: 110, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4877, Acc: 0.8074, Speed: 319.2k, Time: 34.4962
Epoch: 110, Batch: 9000/9683, Batch size: 6, LR: 0.0500, Loss: 0.4880, Acc: 0.8072, Speed: 319.0k, Time: 38.8339
Train 0.8074
Val 0.8255
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526

skip saving model for perf <= 0.8277
Epoch: 111, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4833, Acc: 0.8089, Speed: 281.1k, Time: 4.9238
Epoch: 111, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4859, Acc: 0.8074, Speed: 282.5k, Time: 9.8108
Epoch: 111, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4865, Acc: 0.8076, Speed: 279.4k, Time: 14.7578
Epoch: 111, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4861, Acc: 0.8081, Speed: 279.5k, Time: 19.6526
Epoch: 111, Batch: 5000/9683, Batch size: 21, LR: 0.0500, Loss: 0.4855, Acc: 0.8084, Speed: 280.4k, Time: 24.5664
Epoch: 111, Batch: 6000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4847, Acc: 0.8091, Speed: 278.9k, Time: 29.5425
Epoch: 111, Batch: 7000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4849, Acc: 0.8092, Speed: 279.4k, Time: 34.4705
Epoch: 111, Batch: 8000/9683, Batch size: 14, LR: 0.0500, Loss: 0.4857, Acc: 0.8089, Speed: 279.2k, Time: 39.4056
Epoch: 111, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4859, Acc: 0.8089, Speed: 279.3k, Time: 44.3274
Train 0.8087
Val 0.8285
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473

saving model to local_200_parikh.pt
Epoch: 112, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4845, Acc: 0.8102, Speed: 311.3k, Time: 4.3949
Epoch: 112, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4833, Acc: 0.8102, Speed: 312.0k, Time: 8.7911
Epoch: 112, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4819, Acc: 0.8109, Speed: 316.9k, Time: 13.0700
Epoch: 112, Batch: 4000/9683, Batch size: 20, LR: 0.0500, Loss: 0.4820, Acc: 0.8109, Speed: 318.5k, Time: 17.4103
Epoch: 112, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4836, Acc: 0.8103, Speed: 319.6k, Time: 21.6937
Epoch: 112, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4846, Acc: 0.8097, Speed: 322.7k, Time: 25.7481
Epoch: 112, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4853, Acc: 0.8091, Speed: 321.5k, Time: 30.0647
Epoch: 112, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4850, Acc: 0.8091, Speed: 320.3k, Time: 34.4378
Epoch: 112, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4855, Acc: 0.8088, Speed: 319.0k, Time: 38.8127
Train 0.8089
Val 0.8274
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355

skip saving model for perf <= 0.8285
Epoch: 113, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4806, Acc: 0.8107, Speed: 311.1k, Time: 4.3662
Epoch: 113, Batch: 2000/9683, Batch size: 35, LR: 0.0500, Loss: 0.4840, Acc: 0.8096, Speed: 315.0k, Time: 8.6777
Epoch: 113, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4864, Acc: 0.8093, Speed: 315.8k, Time: 13.0453
Epoch: 113, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4875, Acc: 0.8083, Speed: 317.6k, Time: 17.3461
Epoch: 113, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4864, Acc: 0.8085, Speed: 317.8k, Time: 21.7134
Epoch: 113, Batch: 6000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4863, Acc: 0.8089, Speed: 317.0k, Time: 26.0696
Epoch: 113, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4867, Acc: 0.8086, Speed: 316.0k, Time: 30.4197
Epoch: 113, Batch: 8000/9683, Batch size: 60, LR: 0.0500, Loss: 0.4863, Acc: 0.8087, Speed: 317.5k, Time: 34.6697
Epoch: 113, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4869, Acc: 0.8084, Speed: 318.3k, Time: 38.9528
Train 0.8087
Val 0.8260
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034

skip saving model for perf <= 0.8285
Epoch: 114, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4816, Acc: 0.8104, Speed: 307.9k, Time: 4.4329
Epoch: 114, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4831, Acc: 0.8106, Speed: 310.2k, Time: 8.8384
Epoch: 114, Batch: 3000/9683, Batch size: 5, LR: 0.0500, Loss: 0.4832, Acc: 0.8105, Speed: 316.6k, Time: 13.0828
Epoch: 114, Batch: 4000/9683, Batch size: 14, LR: 0.0500, Loss: 0.4850, Acc: 0.8097, Speed: 315.2k, Time: 17.4670
Epoch: 114, Batch: 5000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4839, Acc: 0.8101, Speed: 315.0k, Time: 21.8212
Epoch: 114, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4855, Acc: 0.8090, Speed: 316.6k, Time: 26.1248
Epoch: 114, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4861, Acc: 0.8087, Speed: 315.7k, Time: 30.5532
Epoch: 114, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4863, Acc: 0.8085, Speed: 315.9k, Time: 34.9008
Epoch: 114, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4862, Acc: 0.8085, Speed: 316.7k, Time: 39.2078
Train 0.8083
Val 0.8257
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729

skip saving model for perf <= 0.8285
Epoch: 115, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4818, Acc: 0.8092, Speed: 303.3k, Time: 4.4475
Epoch: 115, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4815, Acc: 0.8099, Speed: 310.3k, Time: 8.7423
Epoch: 115, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4842, Acc: 0.8091, Speed: 311.9k, Time: 13.0618
Epoch: 115, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4833, Acc: 0.8092, Speed: 314.4k, Time: 17.4200
Epoch: 115, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4834, Acc: 0.8095, Speed: 314.4k, Time: 21.8101
Epoch: 115, Batch: 6000/9683, Batch size: 8, LR: 0.0500, Loss: 0.4840, Acc: 0.8092, Speed: 316.8k, Time: 26.0911
Epoch: 115, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4846, Acc: 0.8089, Speed: 317.3k, Time: 30.4671
Epoch: 115, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4845, Acc: 0.8089, Speed: 317.7k, Time: 34.7465
Epoch: 115, Batch: 9000/9683, Batch size: 4, LR: 0.0500, Loss: 0.4845, Acc: 0.8088, Speed: 317.8k, Time: 39.0518
Train 0.8089
Val 0.8289
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879

saving model to local_200_parikh.pt
Epoch: 116, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4817, Acc: 0.8113, Speed: 282.8k, Time: 4.8804
Epoch: 116, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4813, Acc: 0.8108, Speed: 289.9k, Time: 9.4541
Epoch: 116, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4843, Acc: 0.8093, Speed: 301.1k, Time: 13.7568
Epoch: 116, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4834, Acc: 0.8097, Speed: 305.9k, Time: 18.0642
Epoch: 116, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4828, Acc: 0.8099, Speed: 308.6k, Time: 22.3317
Epoch: 116, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4833, Acc: 0.8101, Speed: 312.1k, Time: 26.5073
Epoch: 116, Batch: 7000/9683, Batch size: 33, LR: 0.0500, Loss: 0.4829, Acc: 0.8101, Speed: 313.8k, Time: 30.7521
Epoch: 116, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4830, Acc: 0.8099, Speed: 312.2k, Time: 35.1921
Epoch: 116, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4831, Acc: 0.8097, Speed: 313.0k, Time: 39.5686
Train 0.8096
Val 0.8280
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965

skip saving model for perf <= 0.8289
Epoch: 117, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4845, Acc: 0.8082, Speed: 314.4k, Time: 4.4138
Epoch: 117, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4831, Acc: 0.8099, Speed: 313.4k, Time: 8.7961
Epoch: 117, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4819, Acc: 0.8106, Speed: 313.9k, Time: 13.0877
Epoch: 117, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4829, Acc: 0.8106, Speed: 314.8k, Time: 17.4199
Epoch: 117, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4816, Acc: 0.8113, Speed: 305.3k, Time: 22.4911
Epoch: 117, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4824, Acc: 0.8109, Speed: 300.9k, Time: 27.4059
Epoch: 117, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4822, Acc: 0.8107, Speed: 298.0k, Time: 32.3921
Epoch: 117, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4826, Acc: 0.8106, Speed: 294.9k, Time: 37.3525
Epoch: 117, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4828, Acc: 0.8104, Speed: 293.2k, Time: 42.3018
Train 0.8104
Val 0.8294
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387

saving model to local_200_parikh.pt
Epoch: 118, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4812, Acc: 0.8097, Speed: 321.4k, Time: 4.3178
Epoch: 118, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4839, Acc: 0.8087, Speed: 320.1k, Time: 8.6362
Epoch: 118, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4828, Acc: 0.8092, Speed: 318.3k, Time: 13.0192
Epoch: 118, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4823, Acc: 0.8097, Speed: 318.6k, Time: 17.3475
Epoch: 118, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4826, Acc: 0.8097, Speed: 318.5k, Time: 21.6848
Epoch: 118, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4829, Acc: 0.8098, Speed: 316.7k, Time: 26.0745
Epoch: 118, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4828, Acc: 0.8100, Speed: 316.6k, Time: 30.4462
Epoch: 118, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4830, Acc: 0.8099, Speed: 316.9k, Time: 34.8322
Epoch: 118, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4829, Acc: 0.8098, Speed: 316.9k, Time: 39.1782
Train 0.8098
Val 0.8272
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152

skip saving model for perf <= 0.8294
Epoch: 119, Batch: 1000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4835, Acc: 0.8099, Speed: 303.5k, Time: 4.4657
Epoch: 119, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4843, Acc: 0.8095, Speed: 309.2k, Time: 8.8385
Epoch: 119, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4845, Acc: 0.8088, Speed: 310.5k, Time: 13.2022
Epoch: 119, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4846, Acc: 0.8091, Speed: 313.1k, Time: 17.5535
Epoch: 119, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4846, Acc: 0.8094, Speed: 313.7k, Time: 21.8452
Epoch: 119, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4864, Acc: 0.8089, Speed: 314.6k, Time: 26.1519
Epoch: 119, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4857, Acc: 0.8092, Speed: 314.8k, Time: 30.4817
Epoch: 119, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4857, Acc: 0.8091, Speed: 315.3k, Time: 34.8492
Epoch: 119, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4848, Acc: 0.8093, Speed: 316.0k, Time: 39.2240
Train 0.8093
Val 0.8282
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168

skip saving model for perf <= 0.8294
Epoch: 120, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4812, Acc: 0.8085, Speed: 311.2k, Time: 4.3922
Epoch: 120, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4843, Acc: 0.8080, Speed: 314.9k, Time: 8.6926
Epoch: 120, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4845, Acc: 0.8084, Speed: 315.6k, Time: 13.1047
Epoch: 120, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4854, Acc: 0.8085, Speed: 317.0k, Time: 17.4919
Epoch: 120, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4844, Acc: 0.8087, Speed: 318.0k, Time: 21.8141
Epoch: 120, Batch: 6000/9683, Batch size: 62, LR: 0.0500, Loss: 0.4842, Acc: 0.8091, Speed: 317.9k, Time: 26.1515
Epoch: 120, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4847, Acc: 0.8090, Speed: 317.6k, Time: 30.5105
Epoch: 120, Batch: 8000/9683, Batch size: 16, LR: 0.0500, Loss: 0.4850, Acc: 0.8087, Speed: 316.0k, Time: 34.9747
Epoch: 120, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4849, Acc: 0.8087, Speed: 311.1k, Time: 39.8644
Train 0.8087
Val 0.8257
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729

skip saving model for perf <= 0.8294
Epoch: 121, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4889, Acc: 0.8076, Speed: 317.1k, Time: 4.3886
Epoch: 121, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4885, Acc: 0.8071, Speed: 318.7k, Time: 8.7800
Epoch: 121, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4857, Acc: 0.8084, Speed: 301.9k, Time: 13.8140
Epoch: 121, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4859, Acc: 0.8086, Speed: 295.1k, Time: 18.7403
Epoch: 121, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4851, Acc: 0.8089, Speed: 300.4k, Time: 23.0408
Epoch: 121, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4843, Acc: 0.8095, Speed: 302.0k, Time: 27.3934
Epoch: 121, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4841, Acc: 0.8096, Speed: 304.9k, Time: 31.7042
Epoch: 121, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4844, Acc: 0.8093, Speed: 306.3k, Time: 36.0621
Epoch: 121, Batch: 9000/9683, Batch size: 15, LR: 0.0500, Loss: 0.4838, Acc: 0.8096, Speed: 307.1k, Time: 40.3527
Train 0.8096
Val 0.8302
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200

saving model to local_200_parikh.pt
Epoch: 122, Batch: 1000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4815, Acc: 0.8123, Speed: 312.4k, Time: 4.3874
Epoch: 122, Batch: 2000/9683, Batch size: 52, LR: 0.0500, Loss: 0.4840, Acc: 0.8110, Speed: 316.2k, Time: 8.6988
Epoch: 122, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4837, Acc: 0.8102, Speed: 316.7k, Time: 13.1061
Epoch: 122, Batch: 4000/9683, Batch size: 38, LR: 0.0500, Loss: 0.4822, Acc: 0.8107, Speed: 315.2k, Time: 17.5161
Epoch: 122, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4825, Acc: 0.8105, Speed: 315.9k, Time: 21.9000
Epoch: 122, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4824, Acc: 0.8105, Speed: 316.5k, Time: 26.2028
Epoch: 122, Batch: 7000/9683, Batch size: 4, LR: 0.0500, Loss: 0.4827, Acc: 0.8103, Speed: 315.8k, Time: 30.5373
Epoch: 122, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4825, Acc: 0.8102, Speed: 316.2k, Time: 34.8447
Epoch: 122, Batch: 9000/9683, Batch size: 3, LR: 0.0500, Loss: 0.4825, Acc: 0.8103, Speed: 316.3k, Time: 39.2317
Train 0.8103
Val 0.8280
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965

skip saving model for perf <= 0.8302
Epoch: 123, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4833, Acc: 0.8092, Speed: 322.5k, Time: 4.3243
Epoch: 123, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4817, Acc: 0.8096, Speed: 320.4k, Time: 8.6400
Epoch: 123, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4829, Acc: 0.8088, Speed: 319.7k, Time: 13.0045
Epoch: 123, Batch: 4000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4820, Acc: 0.8095, Speed: 319.5k, Time: 17.3094
Epoch: 123, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4822, Acc: 0.8095, Speed: 318.8k, Time: 21.6166
Epoch: 123, Batch: 6000/9683, Batch size: 55, LR: 0.0500, Loss: 0.4824, Acc: 0.8096, Speed: 318.8k, Time: 25.9275
Epoch: 123, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4829, Acc: 0.8095, Speed: 318.9k, Time: 30.2971
Epoch: 123, Batch: 8000/9683, Batch size: 59, LR: 0.0500, Loss: 0.4826, Acc: 0.8097, Speed: 319.0k, Time: 34.5830
Epoch: 123, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4825, Acc: 0.8099, Speed: 318.4k, Time: 38.9366
Train 0.8099
Val 0.8293
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286

skip saving model for perf <= 0.8302
Epoch: 124, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4809, Acc: 0.8116, Speed: 309.9k, Time: 4.3632
Epoch: 124, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4806, Acc: 0.8118, Speed: 315.6k, Time: 8.6764
Epoch: 124, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4828, Acc: 0.8103, Speed: 318.3k, Time: 12.9333
Epoch: 124, Batch: 4000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4817, Acc: 0.8108, Speed: 318.0k, Time: 17.2638
Epoch: 124, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4819, Acc: 0.8105, Speed: 319.0k, Time: 21.5913
Epoch: 124, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4829, Acc: 0.8100, Speed: 319.3k, Time: 25.9545
Epoch: 124, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4818, Acc: 0.8104, Speed: 320.5k, Time: 30.2341
Epoch: 124, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4819, Acc: 0.8104, Speed: 319.5k, Time: 34.5691
Epoch: 124, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4821, Acc: 0.8103, Speed: 318.1k, Time: 38.9529
Train 0.8105
Val 0.8273
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253

skip saving model for perf <= 0.8302
Epoch: 125, Batch: 1000/9683, Batch size: 26, LR: 0.0500, Loss: 0.4761, Acc: 0.8123, Speed: 311.2k, Time: 4.4329
Epoch: 125, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4772, Acc: 0.8125, Speed: 316.0k, Time: 8.7970
Epoch: 125, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4771, Acc: 0.8124, Speed: 315.1k, Time: 13.1278
Epoch: 125, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4771, Acc: 0.8126, Speed: 315.3k, Time: 17.4172
Epoch: 125, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4792, Acc: 0.8118, Speed: 313.6k, Time: 21.9010
Epoch: 125, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4813, Acc: 0.8107, Speed: 307.6k, Time: 26.8590
Epoch: 125, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4803, Acc: 0.8111, Speed: 303.6k, Time: 31.7967
Epoch: 125, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4801, Acc: 0.8112, Speed: 300.7k, Time: 36.6787
Epoch: 125, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4809, Acc: 0.8109, Speed: 298.3k, Time: 41.5489
Train 0.8106
Val 0.8290
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981

skip saving model for perf <= 0.8302
Epoch: 126, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4776, Acc: 0.8129, Speed: 313.1k, Time: 4.3647
Epoch: 126, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4772, Acc: 0.8126, Speed: 317.3k, Time: 8.7009
Epoch: 126, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4788, Acc: 0.8115, Speed: 334.0k, Time: 12.3764
Epoch: 126, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4793, Acc: 0.8116, Speed: 340.6k, Time: 16.2365
Epoch: 126, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4806, Acc: 0.8110, Speed: 341.7k, Time: 20.1418
Epoch: 126, Batch: 6000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4806, Acc: 0.8110, Speed: 340.3k, Time: 24.3889
Epoch: 126, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4806, Acc: 0.8110, Speed: 336.1k, Time: 28.7602
Epoch: 126, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4816, Acc: 0.8106, Speed: 333.2k, Time: 33.1369
Epoch: 126, Batch: 9000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4816, Acc: 0.8106, Speed: 331.3k, Time: 37.4703
Train 0.8105
Val 0.8296
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590

skip saving model for perf <= 0.8302
Epoch: 127, Batch: 1000/9683, Batch size: 45, LR: 0.0500, Loss: 0.4829, Acc: 0.8098, Speed: 318.7k, Time: 4.3484
Epoch: 127, Batch: 2000/9683, Batch size: 4, LR: 0.0500, Loss: 0.4838, Acc: 0.8101, Speed: 324.8k, Time: 8.6154
Epoch: 127, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4833, Acc: 0.8100, Speed: 325.3k, Time: 12.8896
Epoch: 127, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4808, Acc: 0.8116, Speed: 323.3k, Time: 17.1931
Epoch: 127, Batch: 5000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4816, Acc: 0.8113, Speed: 324.0k, Time: 21.3720
Epoch: 127, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4818, Acc: 0.8110, Speed: 322.9k, Time: 25.6699
Epoch: 127, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4814, Acc: 0.8112, Speed: 320.9k, Time: 30.0347
Epoch: 127, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4809, Acc: 0.8114, Speed: 321.9k, Time: 34.3472
Epoch: 127, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4805, Acc: 0.8115, Speed: 321.0k, Time: 38.6860
Train 0.8116
Val 0.8271
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050

skip saving model for perf <= 0.8302
Epoch: 128, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4779, Acc: 0.8124, Speed: 304.0k, Time: 4.4168
Epoch: 128, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4767, Acc: 0.8127, Speed: 313.2k, Time: 8.5749
Epoch: 128, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4772, Acc: 0.8129, Speed: 314.4k, Time: 12.9320
Epoch: 128, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4790, Acc: 0.8118, Speed: 317.2k, Time: 17.2407
Epoch: 128, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4796, Acc: 0.8115, Speed: 319.7k, Time: 21.4222
Epoch: 128, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4783, Acc: 0.8122, Speed: 321.3k, Time: 25.6294
Epoch: 128, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4794, Acc: 0.8119, Speed: 321.5k, Time: 29.9260
Epoch: 128, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4795, Acc: 0.8116, Speed: 320.9k, Time: 34.2651
Epoch: 128, Batch: 9000/9683, Batch size: 3, LR: 0.0500, Loss: 0.4795, Acc: 0.8117, Speed: 321.5k, Time: 38.5213
Train 0.8113
Val 0.8285
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473

skip saving model for perf <= 0.8302
Epoch: 129, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4808, Acc: 0.8111, Speed: 312.2k, Time: 4.3789
Epoch: 129, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4818, Acc: 0.8108, Speed: 318.7k, Time: 8.6509
Epoch: 129, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4814, Acc: 0.8103, Speed: 317.5k, Time: 12.9989
Epoch: 129, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4811, Acc: 0.8110, Speed: 318.7k, Time: 17.2722
Epoch: 129, Batch: 5000/9683, Batch size: 21, LR: 0.0500, Loss: 0.4812, Acc: 0.8108, Speed: 320.3k, Time: 21.4918
Epoch: 129, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4812, Acc: 0.8107, Speed: 321.4k, Time: 25.7195
Epoch: 129, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4807, Acc: 0.8111, Speed: 320.8k, Time: 30.0610
Epoch: 129, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4801, Acc: 0.8113, Speed: 320.3k, Time: 34.4336
Epoch: 129, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4803, Acc: 0.8115, Speed: 320.1k, Time: 38.7769
Train 0.8117
Val 0.8283
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269

skip saving model for perf <= 0.8302
Epoch: 130, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4742, Acc: 0.8141, Speed: 321.1k, Time: 4.3477
Epoch: 130, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4753, Acc: 0.8133, Speed: 323.7k, Time: 8.5812
Epoch: 130, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4771, Acc: 0.8129, Speed: 318.8k, Time: 12.8688
Epoch: 130, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4792, Acc: 0.8120, Speed: 321.1k, Time: 17.1434
Epoch: 130, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4783, Acc: 0.8123, Speed: 322.3k, Time: 21.4444
Epoch: 130, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4789, Acc: 0.8122, Speed: 322.2k, Time: 25.7259
Epoch: 130, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4802, Acc: 0.8116, Speed: 322.0k, Time: 30.0477
Epoch: 130, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4807, Acc: 0.8111, Speed: 321.6k, Time: 34.3042
Epoch: 130, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4810, Acc: 0.8109, Speed: 321.8k, Time: 38.5866
Train 0.8108
Val 0.8291
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082

skip saving model for perf <= 0.8302
Epoch: 131, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4775, Acc: 0.8130, Speed: 314.4k, Time: 4.3775
Epoch: 131, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4778, Acc: 0.8118, Speed: 321.2k, Time: 8.6777
Epoch: 131, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4781, Acc: 0.8116, Speed: 319.7k, Time: 13.0292
Epoch: 131, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4794, Acc: 0.8112, Speed: 320.7k, Time: 17.3251
Epoch: 131, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4792, Acc: 0.8112, Speed: 317.9k, Time: 21.7983
Epoch: 131, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4794, Acc: 0.8111, Speed: 315.0k, Time: 26.3120
Epoch: 131, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4789, Acc: 0.8114, Speed: 315.4k, Time: 30.6322
Epoch: 131, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4792, Acc: 0.8112, Speed: 314.5k, Time: 35.0365
Epoch: 131, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4805, Acc: 0.8107, Speed: 310.7k, Time: 39.8932
Train 0.8107
Val 0.8306
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607

saving model to local_200_parikh.pt
Epoch: 132, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4748, Acc: 0.8128, Speed: 319.4k, Time: 4.3681
Epoch: 132, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4800, Acc: 0.8107, Speed: 321.0k, Time: 8.6940
Epoch: 132, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4815, Acc: 0.8101, Speed: 320.8k, Time: 13.0841
Epoch: 132, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4806, Acc: 0.8107, Speed: 318.5k, Time: 17.4466
Epoch: 132, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4795, Acc: 0.8112, Speed: 318.4k, Time: 21.7888
Epoch: 132, Batch: 6000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4795, Acc: 0.8112, Speed: 318.8k, Time: 26.0540
Epoch: 132, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4802, Acc: 0.8110, Speed: 319.4k, Time: 30.3478
Epoch: 132, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4801, Acc: 0.8111, Speed: 315.2k, Time: 35.0527
Epoch: 132, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4798, Acc: 0.8111, Speed: 315.7k, Time: 39.3872
Train 0.8110
Val 0.8320
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029

saving model to local_200_parikh.pt
Epoch: 133, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4756, Acc: 0.8130, Speed: 312.7k, Time: 4.3817
Epoch: 133, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4747, Acc: 0.8132, Speed: 319.4k, Time: 8.7398
Epoch: 133, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4759, Acc: 0.8127, Speed: 317.3k, Time: 13.0595
Epoch: 133, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4761, Acc: 0.8127, Speed: 315.9k, Time: 17.3861
Epoch: 133, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4770, Acc: 0.8123, Speed: 316.1k, Time: 21.7566
Epoch: 133, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4784, Acc: 0.8115, Speed: 316.1k, Time: 26.0680
Epoch: 133, Batch: 7000/9683, Batch size: 27, LR: 0.0500, Loss: 0.4790, Acc: 0.8112, Speed: 316.9k, Time: 30.3183
Epoch: 133, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4795, Acc: 0.8110, Speed: 318.3k, Time: 34.6686
Epoch: 133, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4791, Acc: 0.8112, Speed: 318.9k, Time: 38.9508
Train 0.8111
Val 0.8250
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018

skip saving model for perf <= 0.8320
Epoch: 134, Batch: 1000/9683, Batch size: 57, LR: 0.0500, Loss: 0.4845, Acc: 0.8107, Speed: 311.6k, Time: 4.3742
Epoch: 134, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4814, Acc: 0.8109, Speed: 318.1k, Time: 8.7253
Epoch: 134, Batch: 3000/9683, Batch size: 3, LR: 0.0500, Loss: 0.4825, Acc: 0.8106, Speed: 317.7k, Time: 13.0664
Epoch: 134, Batch: 4000/9683, Batch size: 58, LR: 0.0500, Loss: 0.4815, Acc: 0.8104, Speed: 318.7k, Time: 17.3904
Epoch: 134, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4815, Acc: 0.8104, Speed: 318.5k, Time: 21.7373
Epoch: 134, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4807, Acc: 0.8108, Speed: 319.2k, Time: 26.0437
Epoch: 134, Batch: 7000/9683, Batch size: 45, LR: 0.0500, Loss: 0.4809, Acc: 0.8109, Speed: 318.6k, Time: 30.3844
Epoch: 134, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4808, Acc: 0.8108, Speed: 317.7k, Time: 34.7479
Epoch: 134, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4809, Acc: 0.8107, Speed: 316.7k, Time: 39.1614
Train 0.8109
Val 0.8312
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216

skip saving model for perf <= 0.8320
Epoch: 135, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4777, Acc: 0.8124, Speed: 319.1k, Time: 4.3319
Epoch: 135, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4770, Acc: 0.8130, Speed: 308.3k, Time: 9.0072
Epoch: 135, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4780, Acc: 0.8122, Speed: 299.0k, Time: 13.8751
Epoch: 135, Batch: 4000/9683, Batch size: 58, LR: 0.0500, Loss: 0.4782, Acc: 0.8121, Speed: 304.9k, Time: 18.1700
Epoch: 135, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4779, Acc: 0.8123, Speed: 309.3k, Time: 22.3765
Epoch: 135, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4777, Acc: 0.8123, Speed: 313.7k, Time: 26.5060
Epoch: 135, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4783, Acc: 0.8122, Speed: 315.5k, Time: 30.6740
Epoch: 135, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4787, Acc: 0.8120, Speed: 318.1k, Time: 34.7811
Epoch: 135, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4787, Acc: 0.8119, Speed: 318.2k, Time: 38.9935
Train 0.8120
Val 0.8301
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099

skip saving model for perf <= 0.8320
Epoch: 136, Batch: 1000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4818, Acc: 0.8116, Speed: 321.0k, Time: 4.3311
Epoch: 136, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4781, Acc: 0.8121, Speed: 319.2k, Time: 8.5715
Epoch: 136, Batch: 3000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4781, Acc: 0.8123, Speed: 320.2k, Time: 12.7902
Epoch: 136, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4775, Acc: 0.8126, Speed: 321.7k, Time: 17.0157
Epoch: 136, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4784, Acc: 0.8124, Speed: 323.7k, Time: 21.2101
Epoch: 136, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4792, Acc: 0.8121, Speed: 323.9k, Time: 25.4544
Epoch: 136, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4801, Acc: 0.8114, Speed: 324.1k, Time: 29.6840
Epoch: 136, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4814, Acc: 0.8109, Speed: 324.9k, Time: 33.9690
Epoch: 136, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4811, Acc: 0.8110, Speed: 325.2k, Time: 38.1278
Train 0.8113
Val 0.8300
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997

skip saving model for perf <= 0.8320
Epoch: 137, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4768, Acc: 0.8137, Speed: 324.4k, Time: 4.3077
Epoch: 137, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4753, Acc: 0.8135, Speed: 328.8k, Time: 8.4845
Epoch: 137, Batch: 3000/9683, Batch size: 20, LR: 0.0500, Loss: 0.4771, Acc: 0.8126, Speed: 329.4k, Time: 12.6711
Epoch: 137, Batch: 4000/9683, Batch size: 11, LR: 0.0500, Loss: 0.4774, Acc: 0.8127, Speed: 326.8k, Time: 16.9259
Epoch: 137, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4778, Acc: 0.8125, Speed: 325.5k, Time: 21.1950
Epoch: 137, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4771, Acc: 0.8128, Speed: 324.6k, Time: 25.3861
Epoch: 137, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4779, Acc: 0.8122, Speed: 323.9k, Time: 29.7321
Epoch: 137, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4787, Acc: 0.8120, Speed: 323.8k, Time: 34.0211
Epoch: 137, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4785, Acc: 0.8121, Speed: 324.4k, Time: 38.1934
Train 0.8122
Val 0.8299
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895

skip saving model for perf <= 0.8320
Epoch: 138, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4773, Acc: 0.8129, Speed: 315.5k, Time: 4.3228
Epoch: 138, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4769, Acc: 0.8127, Speed: 320.2k, Time: 8.6081
Epoch: 138, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4753, Acc: 0.8133, Speed: 321.8k, Time: 12.8854
Epoch: 138, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4765, Acc: 0.8126, Speed: 320.5k, Time: 17.2168
Epoch: 138, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4774, Acc: 0.8122, Speed: 322.8k, Time: 21.4253
Epoch: 138, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4771, Acc: 0.8127, Speed: 323.3k, Time: 25.6043
Epoch: 138, Batch: 7000/9683, Batch size: 18, LR: 0.0500, Loss: 0.4782, Acc: 0.8120, Speed: 323.4k, Time: 29.8077
Epoch: 138, Batch: 8000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4782, Acc: 0.8120, Speed: 324.7k, Time: 34.0082
Epoch: 138, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4783, Acc: 0.8120, Speed: 325.3k, Time: 38.1311
Train 0.8121
Val 0.8293
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286

skip saving model for perf <= 0.8320
Epoch: 139, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4827, Acc: 0.8081, Speed: 332.9k, Time: 4.1980
Epoch: 139, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4848, Acc: 0.8087, Speed: 332.1k, Time: 8.3048
Epoch: 139, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4825, Acc: 0.8094, Speed: 329.1k, Time: 12.5987
Epoch: 139, Batch: 4000/9683, Batch size: 40, LR: 0.0500, Loss: 0.4821, Acc: 0.8100, Speed: 329.1k, Time: 16.7959
Epoch: 139, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4817, Acc: 0.8104, Speed: 328.8k, Time: 21.0509
Epoch: 139, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4816, Acc: 0.8103, Speed: 328.7k, Time: 25.2890
Epoch: 139, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4813, Acc: 0.8104, Speed: 327.6k, Time: 29.5157
Epoch: 139, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4813, Acc: 0.8102, Speed: 325.9k, Time: 33.8215
Epoch: 139, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4819, Acc: 0.8101, Speed: 326.3k, Time: 37.9579
Train 0.8102
Val 0.8275
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457

skip saving model for perf <= 0.8320
Epoch: 140, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4747, Acc: 0.8153, Speed: 323.5k, Time: 4.3374
Epoch: 140, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4730, Acc: 0.8142, Speed: 325.9k, Time: 8.5158
Epoch: 140, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4767, Acc: 0.8125, Speed: 321.7k, Time: 12.8052
Epoch: 140, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4771, Acc: 0.8128, Speed: 322.2k, Time: 17.1472
Epoch: 140, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4774, Acc: 0.8124, Speed: 323.4k, Time: 21.4306
Epoch: 140, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4771, Acc: 0.8127, Speed: 322.3k, Time: 25.6761
Epoch: 140, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4774, Acc: 0.8125, Speed: 323.4k, Time: 29.8793
Epoch: 140, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4773, Acc: 0.8125, Speed: 323.5k, Time: 34.1605
Epoch: 140, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4777, Acc: 0.8124, Speed: 322.5k, Time: 38.4433
Train 0.8125
Val 0.8281
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066

skip saving model for perf <= 0.8320
Epoch: 141, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4807, Acc: 0.8127, Speed: 318.5k, Time: 4.3588
Epoch: 141, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4783, Acc: 0.8132, Speed: 325.4k, Time: 8.5258
Epoch: 141, Batch: 3000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4790, Acc: 0.8126, Speed: 324.7k, Time: 12.7490
Epoch: 141, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4789, Acc: 0.8121, Speed: 322.8k, Time: 17.0581
Epoch: 141, Batch: 5000/9683, Batch size: 7, LR: 0.0500, Loss: 0.4775, Acc: 0.8125, Speed: 324.3k, Time: 21.2184
Epoch: 141, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4762, Acc: 0.8132, Speed: 325.5k, Time: 25.3944
Epoch: 141, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4766, Acc: 0.8130, Speed: 325.5k, Time: 29.6338
Epoch: 141, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4770, Acc: 0.8128, Speed: 325.2k, Time: 33.8304
Epoch: 141, Batch: 9000/9683, Batch size: 3, LR: 0.0500, Loss: 0.4776, Acc: 0.8125, Speed: 326.8k, Time: 37.9360
Train 0.8126
Val 0.8294
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387

skip saving model for perf <= 0.8320
Epoch: 142, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4743, Acc: 0.8133, Speed: 314.3k, Time: 4.3591
Epoch: 142, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4787, Acc: 0.8125, Speed: 323.3k, Time: 8.5079
Epoch: 142, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4813, Acc: 0.8110, Speed: 322.9k, Time: 12.7267
Epoch: 142, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4792, Acc: 0.8116, Speed: 322.9k, Time: 16.9881
Epoch: 142, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4787, Acc: 0.8119, Speed: 324.6k, Time: 21.1440
Epoch: 142, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4774, Acc: 0.8127, Speed: 325.2k, Time: 25.3502
Epoch: 142, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4777, Acc: 0.8126, Speed: 326.1k, Time: 29.5092
Epoch: 142, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4780, Acc: 0.8125, Speed: 326.9k, Time: 33.7498
Epoch: 142, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4779, Acc: 0.8124, Speed: 326.6k, Time: 37.9525
Train 0.8123
Val 0.8282
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168

skip saving model for perf <= 0.8320
Epoch: 143, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4752, Acc: 0.8150, Speed: 319.5k, Time: 4.3334
Epoch: 143, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4776, Acc: 0.8121, Speed: 318.7k, Time: 8.7086
Epoch: 143, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4764, Acc: 0.8124, Speed: 325.2k, Time: 12.8357
Epoch: 143, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4765, Acc: 0.8124, Speed: 326.0k, Time: 16.9665
Epoch: 143, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4765, Acc: 0.8127, Speed: 326.6k, Time: 21.2489
Epoch: 143, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4754, Acc: 0.8132, Speed: 325.2k, Time: 25.5533
Epoch: 143, Batch: 7000/9683, Batch size: 46, LR: 0.0500, Loss: 0.4758, Acc: 0.8131, Speed: 325.0k, Time: 29.7442
Epoch: 143, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4753, Acc: 0.8135, Speed: 325.4k, Time: 33.9095
Epoch: 143, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4759, Acc: 0.8134, Speed: 325.1k, Time: 38.1482
Train 0.8132
Val 0.8289
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879

skip saving model for perf <= 0.8320
Epoch: 144, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4695, Acc: 0.8171, Speed: 319.9k, Time: 4.2952
Epoch: 144, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4719, Acc: 0.8149, Speed: 319.7k, Time: 8.5657
Epoch: 144, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4731, Acc: 0.8144, Speed: 320.1k, Time: 12.8276
Epoch: 144, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4751, Acc: 0.8133, Speed: 323.1k, Time: 17.0132
Epoch: 144, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4759, Acc: 0.8128, Speed: 323.4k, Time: 21.2247
Epoch: 144, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4755, Acc: 0.8131, Speed: 323.5k, Time: 25.4241
Epoch: 144, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4754, Acc: 0.8132, Speed: 324.4k, Time: 29.6409
Epoch: 144, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4762, Acc: 0.8128, Speed: 326.0k, Time: 33.8079
Epoch: 144, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4763, Acc: 0.8129, Speed: 325.5k, Time: 38.0928
Train 0.8130
Val 0.8285
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473

skip saving model for perf <= 0.8320
Epoch: 145, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4731, Acc: 0.8152, Speed: 327.3k, Time: 4.2791
Epoch: 145, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4738, Acc: 0.8149, Speed: 325.4k, Time: 8.4943
Epoch: 145, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4734, Acc: 0.8155, Speed: 324.9k, Time: 12.7975
Epoch: 145, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4752, Acc: 0.8141, Speed: 327.3k, Time: 16.9845
Epoch: 145, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4758, Acc: 0.8137, Speed: 327.9k, Time: 21.1379
Epoch: 145, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4752, Acc: 0.8137, Speed: 329.2k, Time: 25.2157
Epoch: 145, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4757, Acc: 0.8135, Speed: 328.9k, Time: 29.4159
Epoch: 145, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4761, Acc: 0.8134, Speed: 328.7k, Time: 33.5653
Epoch: 145, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4771, Acc: 0.8129, Speed: 328.1k, Time: 37.8221
Train 0.8129
Val 0.8319
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928

skip saving model for perf <= 0.8320
Epoch: 146, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4715, Acc: 0.8154, Speed: 312.0k, Time: 4.3592
Epoch: 146, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4717, Acc: 0.8150, Speed: 318.1k, Time: 8.6018
Epoch: 146, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4714, Acc: 0.8151, Speed: 322.3k, Time: 12.7889
Epoch: 146, Batch: 4000/9683, Batch size: 40, LR: 0.0500, Loss: 0.4732, Acc: 0.8140, Speed: 324.5k, Time: 16.9723
Epoch: 146, Batch: 5000/9683, Batch size: 46, LR: 0.0500, Loss: 0.4741, Acc: 0.8136, Speed: 324.3k, Time: 21.2111
Epoch: 146, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4746, Acc: 0.8133, Speed: 325.0k, Time: 25.2944
Epoch: 146, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4750, Acc: 0.8133, Speed: 325.9k, Time: 29.4649
Epoch: 146, Batch: 8000/9683, Batch size: 10, LR: 0.0500, Loss: 0.4752, Acc: 0.8132, Speed: 326.8k, Time: 33.6426
Epoch: 146, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4751, Acc: 0.8132, Speed: 328.4k, Time: 37.8054
Train 0.8135
Val 0.8280
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965

skip saving model for perf <= 0.8320
Epoch: 147, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4776, Acc: 0.8125, Speed: 320.4k, Time: 4.2540
Epoch: 147, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4811, Acc: 0.8107, Speed: 326.8k, Time: 8.4165
Epoch: 147, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4816, Acc: 0.8100, Speed: 331.0k, Time: 12.5848
Epoch: 147, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4815, Acc: 0.8105, Speed: 331.5k, Time: 16.7343
Epoch: 147, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4799, Acc: 0.8112, Speed: 329.2k, Time: 20.9827
Epoch: 147, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4787, Acc: 0.8120, Speed: 328.6k, Time: 25.2357
Epoch: 147, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4785, Acc: 0.8122, Speed: 328.4k, Time: 29.4162
Epoch: 147, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4776, Acc: 0.8127, Speed: 329.0k, Time: 33.6172
Epoch: 147, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4784, Acc: 0.8122, Speed: 328.8k, Time: 37.7514
Train 0.8123
Val 0.8296
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590

skip saving model for perf <= 0.8320
Epoch: 148, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4732, Acc: 0.8139, Speed: 322.8k, Time: 4.2565
Epoch: 148, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4717, Acc: 0.8145, Speed: 323.2k, Time: 8.4675
Epoch: 148, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4733, Acc: 0.8140, Speed: 327.7k, Time: 12.6404
Epoch: 148, Batch: 4000/9683, Batch size: 62, LR: 0.0500, Loss: 0.4735, Acc: 0.8138, Speed: 328.9k, Time: 16.8328
Epoch: 148, Batch: 5000/9683, Batch size: 7, LR: 0.0500, Loss: 0.4732, Acc: 0.8140, Speed: 329.4k, Time: 20.9767
Epoch: 148, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4731, Acc: 0.8142, Speed: 329.3k, Time: 25.1620
Epoch: 148, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4733, Acc: 0.8140, Speed: 328.7k, Time: 29.3440
Epoch: 148, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4737, Acc: 0.8141, Speed: 327.7k, Time: 33.6245
Epoch: 148, Batch: 9000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4751, Acc: 0.8135, Speed: 326.7k, Time: 37.9808
Train 0.8132
Val 0.8299
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895

skip saving model for perf <= 0.8320
Epoch: 149, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4719, Acc: 0.8147, Speed: 323.5k, Time: 4.2473
Epoch: 149, Batch: 2000/9683, Batch size: 26, LR: 0.0500, Loss: 0.4744, Acc: 0.8135, Speed: 326.3k, Time: 8.4426
Epoch: 149, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4762, Acc: 0.8133, Speed: 326.5k, Time: 12.6058
Epoch: 149, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4756, Acc: 0.8136, Speed: 327.7k, Time: 16.7977
Epoch: 149, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4758, Acc: 0.8136, Speed: 328.6k, Time: 20.9272
Epoch: 149, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4753, Acc: 0.8136, Speed: 327.1k, Time: 25.2019
Epoch: 149, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4749, Acc: 0.8139, Speed: 325.8k, Time: 29.4847
Epoch: 149, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4751, Acc: 0.8137, Speed: 327.0k, Time: 33.6532
Epoch: 149, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4748, Acc: 0.8136, Speed: 326.7k, Time: 37.8951
Train 0.8134
Val 0.8296
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590

skip saving model for perf <= 0.8320
Epoch: 150, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4744, Acc: 0.8121, Speed: 330.1k, Time: 4.1914
Epoch: 150, Batch: 2000/9683, Batch size: 3, LR: 0.0500, Loss: 0.4755, Acc: 0.8125, Speed: 327.0k, Time: 8.4166
Epoch: 150, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4744, Acc: 0.8134, Speed: 328.2k, Time: 12.6550
Epoch: 150, Batch: 4000/9683, Batch size: 8, LR: 0.0500, Loss: 0.4743, Acc: 0.8135, Speed: 327.6k, Time: 16.8174
Epoch: 150, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4750, Acc: 0.8130, Speed: 328.2k, Time: 21.0330
Epoch: 150, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4758, Acc: 0.8129, Speed: 328.0k, Time: 25.2719
Epoch: 150, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4763, Acc: 0.8125, Speed: 327.3k, Time: 29.5062
Epoch: 150, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4761, Acc: 0.8126, Speed: 327.1k, Time: 33.7178
Epoch: 150, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4749, Acc: 0.8131, Speed: 326.4k, Time: 38.0054
Train 0.8131
Val 0.8301
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099

skip saving model for perf <= 0.8320
Epoch: 151, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4750, Acc: 0.8131, Speed: 321.8k, Time: 4.2425
Epoch: 151, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4777, Acc: 0.8125, Speed: 319.5k, Time: 8.4906
Epoch: 151, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4787, Acc: 0.8118, Speed: 325.3k, Time: 12.7121
Epoch: 151, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4787, Acc: 0.8115, Speed: 327.8k, Time: 16.8933
Epoch: 151, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4770, Acc: 0.8121, Speed: 325.9k, Time: 21.0859
Epoch: 151, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4762, Acc: 0.8126, Speed: 327.8k, Time: 25.2200
Epoch: 151, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4765, Acc: 0.8124, Speed: 327.2k, Time: 29.4005
Epoch: 151, Batch: 8000/9683, Batch size: 35, LR: 0.0500, Loss: 0.4764, Acc: 0.8125, Speed: 327.2k, Time: 33.6278
Epoch: 151, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4772, Acc: 0.8121, Speed: 327.7k, Time: 37.7860
Train 0.8120
Val 0.8283
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269

skip saving model for perf <= 0.8320
Epoch: 152, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4776, Acc: 0.8138, Speed: 326.2k, Time: 4.2234
Epoch: 152, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4739, Acc: 0.8152, Speed: 325.9k, Time: 8.4909
Epoch: 152, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4747, Acc: 0.8145, Speed: 323.4k, Time: 12.7295
Epoch: 152, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4749, Acc: 0.8141, Speed: 323.8k, Time: 17.0354
Epoch: 152, Batch: 5000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4755, Acc: 0.8138, Speed: 323.6k, Time: 21.2099
Epoch: 152, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4764, Acc: 0.8131, Speed: 324.6k, Time: 25.4659
Epoch: 152, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4759, Acc: 0.8133, Speed: 325.0k, Time: 29.6438
Epoch: 152, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4762, Acc: 0.8132, Speed: 325.0k, Time: 33.8798
Epoch: 152, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4757, Acc: 0.8134, Speed: 324.5k, Time: 38.1125
Train 0.8132
Val 0.8303
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302

skip saving model for perf <= 0.8320
Epoch: 153, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4684, Acc: 0.8159, Speed: 314.7k, Time: 4.2620
Epoch: 153, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4720, Acc: 0.8147, Speed: 324.2k, Time: 8.4318
Epoch: 153, Batch: 3000/9683, Batch size: 7, LR: 0.0500, Loss: 0.4740, Acc: 0.8139, Speed: 324.7k, Time: 12.6226
Epoch: 153, Batch: 4000/9683, Batch size: 55, LR: 0.0500, Loss: 0.4753, Acc: 0.8133, Speed: 327.9k, Time: 16.7719
Epoch: 153, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4739, Acc: 0.8137, Speed: 326.2k, Time: 21.0427
Epoch: 153, Batch: 6000/9683, Batch size: 4, LR: 0.0500, Loss: 0.4741, Acc: 0.8135, Speed: 328.0k, Time: 25.1852
Epoch: 153, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4742, Acc: 0.8133, Speed: 328.0k, Time: 29.3997
Epoch: 153, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4745, Acc: 0.8133, Speed: 328.1k, Time: 33.5509
Epoch: 153, Batch: 9000/9683, Batch size: 7, LR: 0.0500, Loss: 0.4748, Acc: 0.8132, Speed: 327.9k, Time: 37.7590
Train 0.8131
Val 0.8306
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607

skip saving model for perf <= 0.8320
Epoch: 154, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4748, Acc: 0.8144, Speed: 321.0k, Time: 4.2803
Epoch: 154, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4746, Acc: 0.8144, Speed: 320.8k, Time: 8.5156
Epoch: 154, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4733, Acc: 0.8145, Speed: 324.7k, Time: 12.6697
Epoch: 154, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4741, Acc: 0.8144, Speed: 326.3k, Time: 16.8976
Epoch: 154, Batch: 5000/9683, Batch size: 37, LR: 0.0500, Loss: 0.4731, Acc: 0.8147, Speed: 327.0k, Time: 21.1260
Epoch: 154, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4738, Acc: 0.8143, Speed: 326.8k, Time: 25.3885
Epoch: 154, Batch: 7000/9683, Batch size: 54, LR: 0.0500, Loss: 0.4740, Acc: 0.8140, Speed: 326.2k, Time: 29.5708
Epoch: 154, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4743, Acc: 0.8139, Speed: 326.5k, Time: 33.7219
Epoch: 154, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4741, Acc: 0.8139, Speed: 326.6k, Time: 37.9202
Train 0.8139
Val 0.8293
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286

skip saving model for perf <= 0.8320
Epoch: 155, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4668, Acc: 0.8168, Speed: 321.3k, Time: 4.2635
Epoch: 155, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4684, Acc: 0.8152, Speed: 323.5k, Time: 8.4463
Epoch: 155, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4704, Acc: 0.8148, Speed: 328.6k, Time: 12.5490
Epoch: 155, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4714, Acc: 0.8145, Speed: 329.3k, Time: 16.6859
Epoch: 155, Batch: 5000/9683, Batch size: 20, LR: 0.0500, Loss: 0.4718, Acc: 0.8147, Speed: 330.7k, Time: 20.8096
Epoch: 155, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4724, Acc: 0.8143, Speed: 331.4k, Time: 24.9762
Epoch: 155, Batch: 7000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4729, Acc: 0.8141, Speed: 330.3k, Time: 29.1827
Epoch: 155, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4725, Acc: 0.8142, Speed: 329.7k, Time: 33.3391
Epoch: 155, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4730, Acc: 0.8140, Speed: 330.1k, Time: 37.5182
Train 0.8139
Val 0.8293
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286

skip saving model for perf <= 0.8320
Epoch: 156, Batch: 1000/9683, Batch size: 5, LR: 0.0500, Loss: 0.4747, Acc: 0.8139, Speed: 323.8k, Time: 4.2559
Epoch: 156, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4739, Acc: 0.8145, Speed: 328.9k, Time: 8.4310
Epoch: 156, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4736, Acc: 0.8141, Speed: 329.0k, Time: 12.6135
Epoch: 156, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4720, Acc: 0.8149, Speed: 326.8k, Time: 16.8763
Epoch: 156, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4737, Acc: 0.8140, Speed: 326.0k, Time: 21.1193
Epoch: 156, Batch: 6000/9683, Batch size: 4, LR: 0.0500, Loss: 0.4727, Acc: 0.8144, Speed: 324.0k, Time: 25.4640
Epoch: 156, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4728, Acc: 0.8144, Speed: 324.8k, Time: 29.7183
Epoch: 156, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4731, Acc: 0.8142, Speed: 325.2k, Time: 33.9411
Epoch: 156, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4726, Acc: 0.8146, Speed: 325.7k, Time: 38.0900
Train 0.8146
Val 0.8283
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269

skip saving model for perf <= 0.8320
Epoch: 157, Batch: 1000/9683, Batch size: 26, LR: 0.0500, Loss: 0.4697, Acc: 0.8155, Speed: 317.2k, Time: 4.3182
Epoch: 157, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4680, Acc: 0.8162, Speed: 323.7k, Time: 8.5062
Epoch: 157, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4686, Acc: 0.8160, Speed: 324.8k, Time: 12.6896
Epoch: 157, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4693, Acc: 0.8159, Speed: 323.9k, Time: 16.9678
Epoch: 157, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4713, Acc: 0.8150, Speed: 324.4k, Time: 21.2063
Epoch: 157, Batch: 6000/9683, Batch size: 35, LR: 0.0500, Loss: 0.4716, Acc: 0.8152, Speed: 323.3k, Time: 25.4922
Epoch: 157, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4727, Acc: 0.8148, Speed: 325.3k, Time: 29.6007
Epoch: 157, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4730, Acc: 0.8146, Speed: 326.0k, Time: 33.7857
Epoch: 157, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4729, Acc: 0.8148, Speed: 325.6k, Time: 38.0465
Train 0.8145
Val 0.8303
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302

skip saving model for perf <= 0.8320
Epoch: 158, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4718, Acc: 0.8152, Speed: 319.7k, Time: 4.2578
Epoch: 158, Batch: 2000/9683, Batch size: 3, LR: 0.0500, Loss: 0.4725, Acc: 0.8151, Speed: 316.5k, Time: 8.5877
Epoch: 158, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4717, Acc: 0.8150, Speed: 315.8k, Time: 12.9082
Epoch: 158, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4709, Acc: 0.8155, Speed: 319.6k, Time: 17.0490
Epoch: 158, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4720, Acc: 0.8151, Speed: 320.2k, Time: 21.3160
Epoch: 158, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4710, Acc: 0.8152, Speed: 320.2k, Time: 25.6019
Epoch: 158, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4708, Acc: 0.8151, Speed: 322.2k, Time: 29.7888
Epoch: 158, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4710, Acc: 0.8150, Speed: 323.5k, Time: 33.9799
Epoch: 158, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4718, Acc: 0.8147, Speed: 324.1k, Time: 38.2484
Train 0.8146
Val 0.8303
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302

skip saving model for perf <= 0.8320
Epoch: 159, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4734, Acc: 0.8142, Speed: 321.0k, Time: 4.3046
Epoch: 159, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4720, Acc: 0.8145, Speed: 319.6k, Time: 8.6327
Epoch: 159, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4733, Acc: 0.8143, Speed: 320.1k, Time: 12.9443
Epoch: 159, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4735, Acc: 0.8144, Speed: 321.7k, Time: 17.1602
Epoch: 159, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4728, Acc: 0.8146, Speed: 325.3k, Time: 21.1241
Epoch: 159, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4717, Acc: 0.8149, Speed: 328.0k, Time: 25.1066
Epoch: 159, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4715, Acc: 0.8149, Speed: 328.9k, Time: 29.3127
Epoch: 159, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4726, Acc: 0.8143, Speed: 323.9k, Time: 33.9689
Epoch: 159, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4725, Acc: 0.8145, Speed: 321.9k, Time: 38.5346
Train 0.8142
Val 0.8276
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558

skip saving model for perf <= 0.8320
Epoch: 160, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4726, Acc: 0.8159, Speed: 346.7k, Time: 4.0211
Epoch: 160, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4716, Acc: 0.8156, Speed: 338.7k, Time: 8.1644
Epoch: 160, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4704, Acc: 0.8155, Speed: 344.1k, Time: 12.1145
Epoch: 160, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4723, Acc: 0.8149, Speed: 353.7k, Time: 15.7197
Epoch: 160, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4732, Acc: 0.8142, Speed: 359.1k, Time: 19.3196
Epoch: 160, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4732, Acc: 0.8141, Speed: 360.9k, Time: 22.9145
Epoch: 160, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4730, Acc: 0.8144, Speed: 363.0k, Time: 26.5122
Epoch: 160, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4732, Acc: 0.8143, Speed: 365.2k, Time: 30.1860
Epoch: 160, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4735, Acc: 0.8141, Speed: 367.0k, Time: 33.7844
Train 0.8143
Val 0.8285
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473

skip saving model for perf <= 0.8320
Epoch: 161, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4740, Acc: 0.8141, Speed: 389.7k, Time: 3.6048
Epoch: 161, Batch: 2000/9683, Batch size: 26, LR: 0.0500, Loss: 0.4709, Acc: 0.8151, Speed: 391.4k, Time: 7.2092
Epoch: 161, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4717, Acc: 0.8149, Speed: 387.7k, Time: 10.8043
Epoch: 161, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4724, Acc: 0.8148, Speed: 374.2k, Time: 14.9025
Epoch: 161, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4721, Acc: 0.8152, Speed: 335.5k, Time: 20.6326
Epoch: 161, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4717, Acc: 0.8154, Speed: 314.3k, Time: 26.2941
Epoch: 161, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4726, Acc: 0.8150, Speed: 302.1k, Time: 32.0280
Epoch: 161, Batch: 8000/9683, Batch size: 51, LR: 0.0500, Loss: 0.4734, Acc: 0.8148, Speed: 290.3k, Time: 38.0283
Epoch: 161, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4737, Acc: 0.8145, Speed: 284.9k, Time: 43.5232
Train 0.8139
Val 0.8306
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607

skip saving model for perf <= 0.8320
Epoch: 162, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4710, Acc: 0.8160, Speed: 241.3k, Time: 5.6500
Epoch: 162, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4714, Acc: 0.8152, Speed: 241.9k, Time: 11.3081
Epoch: 162, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4725, Acc: 0.8147, Speed: 237.9k, Time: 17.2717
Epoch: 162, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4733, Acc: 0.8139, Speed: 237.8k, Time: 22.9650
Epoch: 162, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4732, Acc: 0.8139, Speed: 240.2k, Time: 28.4891
Epoch: 162, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4732, Acc: 0.8141, Speed: 243.1k, Time: 33.9376
Epoch: 162, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4734, Acc: 0.8140, Speed: 243.5k, Time: 39.6066
Epoch: 162, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4735, Acc: 0.8141, Speed: 244.6k, Time: 45.1440
Epoch: 162, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4726, Acc: 0.8146, Speed: 244.5k, Time: 50.7296
Train 0.8148
Val 0.8317
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724

skip saving model for perf <= 0.8320
Epoch: 163, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4665, Acc: 0.8174, Speed: 234.8k, Time: 5.9316
Epoch: 163, Batch: 2000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4697, Acc: 0.8157, Speed: 234.1k, Time: 11.8633
Epoch: 163, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4716, Acc: 0.8155, Speed: 236.0k, Time: 17.6149
Epoch: 163, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4710, Acc: 0.8156, Speed: 239.0k, Time: 22.9723
Epoch: 163, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4707, Acc: 0.8158, Speed: 239.8k, Time: 28.6223
Epoch: 163, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4701, Acc: 0.8160, Speed: 237.8k, Time: 34.6125
Epoch: 163, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4707, Acc: 0.8155, Speed: 238.3k, Time: 40.3196
Epoch: 163, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4705, Acc: 0.8156, Speed: 239.7k, Time: 45.8815
Epoch: 163, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4716, Acc: 0.8151, Speed: 241.0k, Time: 51.4007
Train 0.8150
Val 0.8311
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115

skip saving model for perf <= 0.8320
Epoch: 164, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4676, Acc: 0.8151, Speed: 252.1k, Time: 5.4600
Epoch: 164, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4688, Acc: 0.8151, Speed: 251.9k, Time: 11.0446
Epoch: 164, Batch: 3000/9683, Batch size: 3, LR: 0.0500, Loss: 0.4692, Acc: 0.8151, Speed: 251.6k, Time: 16.4692
Epoch: 164, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4685, Acc: 0.8159, Speed: 247.6k, Time: 22.2806
Epoch: 164, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4696, Acc: 0.8157, Speed: 243.8k, Time: 28.2459
Epoch: 164, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4712, Acc: 0.8150, Speed: 245.1k, Time: 33.7444
Epoch: 164, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4715, Acc: 0.8147, Speed: 244.8k, Time: 39.4312
Epoch: 164, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4714, Acc: 0.8148, Speed: 242.1k, Time: 45.5164
Epoch: 164, Batch: 9000/9683, Batch size: 63, LR: 0.0500, Loss: 0.4716, Acc: 0.8148, Speed: 241.1k, Time: 51.4792
Train 0.8147
Val 0.8321
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131

saving model to local_200_parikh.pt
Epoch: 165, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4715, Acc: 0.8150, Speed: 216.0k, Time: 6.2701
Epoch: 165, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4711, Acc: 0.8152, Speed: 232.4k, Time: 11.8225
Epoch: 165, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4716, Acc: 0.8152, Speed: 238.5k, Time: 17.3503
Epoch: 165, Batch: 4000/9683, Batch size: 44, LR: 0.0500, Loss: 0.4721, Acc: 0.8149, Speed: 240.3k, Time: 23.0687
Epoch: 165, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4716, Acc: 0.8152, Speed: 239.7k, Time: 28.8236
Epoch: 165, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4713, Acc: 0.8151, Speed: 238.6k, Time: 34.7206
Epoch: 165, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4711, Acc: 0.8151, Speed: 239.5k, Time: 40.3209
Epoch: 165, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4707, Acc: 0.8153, Speed: 238.7k, Time: 46.2004
Epoch: 165, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4711, Acc: 0.8152, Speed: 240.5k, Time: 51.5980
Train 0.8153
Val 0.8310
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013

skip saving model for perf <= 0.8321
Epoch: 166, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4686, Acc: 0.8164, Speed: 240.0k, Time: 5.6970
Epoch: 166, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4703, Acc: 0.8155, Speed: 244.0k, Time: 11.2056
Epoch: 166, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4717, Acc: 0.8149, Speed: 236.6k, Time: 17.4242
Epoch: 166, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4725, Acc: 0.8143, Speed: 239.9k, Time: 22.9071
Epoch: 166, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4728, Acc: 0.8141, Speed: 242.7k, Time: 28.4304
Epoch: 166, Batch: 6000/9683, Batch size: 45, LR: 0.0500, Loss: 0.4734, Acc: 0.8140, Speed: 243.3k, Time: 34.0068
Epoch: 166, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4732, Acc: 0.8139, Speed: 243.9k, Time: 39.5574
Epoch: 166, Batch: 8000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4731, Acc: 0.8141, Speed: 243.7k, Time: 45.0841
Epoch: 166, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4731, Acc: 0.8141, Speed: 244.1k, Time: 50.8490
Train 0.8140
Val 0.8281
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066

skip saving model for perf <= 0.8321
Epoch: 167, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4711, Acc: 0.8155, Speed: 232.6k, Time: 5.8659
Epoch: 167, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4699, Acc: 0.8153, Speed: 239.3k, Time: 11.4736
Epoch: 167, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4707, Acc: 0.8150, Speed: 241.4k, Time: 17.0990
Epoch: 167, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4715, Acc: 0.8152, Speed: 242.8k, Time: 22.7084
Epoch: 167, Batch: 5000/9683, Batch size: 23, LR: 0.0500, Loss: 0.4720, Acc: 0.8150, Speed: 238.8k, Time: 28.7037
Epoch: 167, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4718, Acc: 0.8148, Speed: 238.7k, Time: 34.3710
Epoch: 167, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4718, Acc: 0.8149, Speed: 240.2k, Time: 39.9477
Epoch: 167, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4719, Acc: 0.8146, Speed: 239.4k, Time: 45.9642
Epoch: 167, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4724, Acc: 0.8142, Speed: 238.2k, Time: 51.9950
Train 0.8142
Val 0.8300
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997

skip saving model for perf <= 0.8321
Epoch: 168, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4681, Acc: 0.8152, Speed: 235.4k, Time: 5.9145
Epoch: 168, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4699, Acc: 0.8152, Speed: 229.6k, Time: 12.1652
Epoch: 168, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4690, Acc: 0.8156, Speed: 232.0k, Time: 17.8663
Epoch: 168, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4693, Acc: 0.8156, Speed: 232.0k, Time: 23.7527
Epoch: 168, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4704, Acc: 0.8152, Speed: 235.1k, Time: 29.3376
Epoch: 168, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4708, Acc: 0.8148, Speed: 236.1k, Time: 34.9319
Epoch: 168, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4705, Acc: 0.8151, Speed: 237.3k, Time: 40.5513
Epoch: 168, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4710, Acc: 0.8148, Speed: 237.9k, Time: 46.2538
Epoch: 168, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4711, Acc: 0.8149, Speed: 237.0k, Time: 52.2544
Train 0.8149
Val 0.8305
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505

skip saving model for perf <= 0.8321
Epoch: 169, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4664, Acc: 0.8155, Speed: 245.7k, Time: 5.6856
Epoch: 169, Batch: 2000/9683, Batch size: 27, LR: 0.0500, Loss: 0.4655, Acc: 0.8165, Speed: 241.9k, Time: 11.4340
Epoch: 169, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4674, Acc: 0.8162, Speed: 244.2k, Time: 16.9167
Epoch: 169, Batch: 4000/9683, Batch size: 5, LR: 0.0500, Loss: 0.4690, Acc: 0.8155, Speed: 243.7k, Time: 22.6721
Epoch: 169, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4694, Acc: 0.8154, Speed: 244.7k, Time: 28.2355
Epoch: 169, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4701, Acc: 0.8152, Speed: 245.2k, Time: 33.7830
Epoch: 169, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4707, Acc: 0.8150, Speed: 242.7k, Time: 39.7261
Epoch: 169, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4709, Acc: 0.8148, Speed: 242.4k, Time: 45.5015
Epoch: 169, Batch: 9000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4706, Acc: 0.8149, Speed: 242.8k, Time: 51.0304
Train 0.8149
Val 0.8335
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452

saving model to local_200_parikh.pt
Epoch: 170, Batch: 1000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4744, Acc: 0.8150, Speed: 239.8k, Time: 5.7767
Epoch: 170, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4709, Acc: 0.8158, Speed: 240.8k, Time: 11.4263
Epoch: 170, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4711, Acc: 0.8152, Speed: 240.7k, Time: 17.1707
Epoch: 170, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4702, Acc: 0.8157, Speed: 242.4k, Time: 22.6563
Epoch: 170, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4707, Acc: 0.8155, Speed: 241.9k, Time: 28.4494
Epoch: 170, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4696, Acc: 0.8160, Speed: 243.7k, Time: 33.9612
Epoch: 170, Batch: 7000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4695, Acc: 0.8158, Speed: 243.5k, Time: 39.5891
Epoch: 170, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4697, Acc: 0.8158, Speed: 242.2k, Time: 45.4105
Epoch: 170, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4698, Acc: 0.8156, Speed: 242.3k, Time: 51.1705
Train 0.8157
Val 0.8310
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013

skip saving model for perf <= 0.8335
Epoch: 171, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4693, Acc: 0.8167, Speed: 232.3k, Time: 5.8718
Epoch: 171, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4676, Acc: 0.8174, Speed: 230.0k, Time: 11.9245
Epoch: 171, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4689, Acc: 0.8171, Speed: 234.5k, Time: 17.5240
Epoch: 171, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4678, Acc: 0.8174, Speed: 238.1k, Time: 22.9955
Epoch: 171, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4683, Acc: 0.8169, Speed: 241.1k, Time: 28.4563
Epoch: 171, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4693, Acc: 0.8164, Speed: 243.1k, Time: 33.9183
Epoch: 171, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4694, Acc: 0.8162, Speed: 243.4k, Time: 39.5616
Epoch: 171, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4694, Acc: 0.8160, Speed: 242.4k, Time: 45.4581
Epoch: 171, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4693, Acc: 0.8161, Speed: 241.1k, Time: 51.3846
Train 0.8161
Val 0.8298
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794

skip saving model for perf <= 0.8335
Epoch: 172, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4710, Acc: 0.8153, Speed: 270.3k, Time: 5.0259
Epoch: 172, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4699, Acc: 0.8157, Speed: 251.8k, Time: 10.8489
Epoch: 172, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4698, Acc: 0.8156, Speed: 249.5k, Time: 16.5164
Epoch: 172, Batch: 4000/9683, Batch size: 16, LR: 0.0500, Loss: 0.4713, Acc: 0.8143, Speed: 248.8k, Time: 22.0284
Epoch: 172, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4708, Acc: 0.8144, Speed: 247.7k, Time: 27.6683
Epoch: 172, Batch: 6000/9683, Batch size: 21, LR: 0.0500, Loss: 0.4711, Acc: 0.8144, Speed: 247.7k, Time: 33.1824
Epoch: 172, Batch: 7000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4713, Acc: 0.8143, Speed: 247.8k, Time: 38.6726
Epoch: 172, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4711, Acc: 0.8144, Speed: 246.3k, Time: 44.6809
Epoch: 172, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4711, Acc: 0.8146, Speed: 246.3k, Time: 50.3768
Train 0.8147
Val 0.8294
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387

skip saving model for perf <= 0.8335
Epoch: 173, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4703, Acc: 0.8151, Speed: 236.0k, Time: 5.9260
Epoch: 173, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4693, Acc: 0.8150, Speed: 235.7k, Time: 11.7353
Epoch: 173, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4689, Acc: 0.8150, Speed: 236.5k, Time: 17.5294
Epoch: 173, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4694, Acc: 0.8151, Speed: 236.7k, Time: 23.3133
Epoch: 173, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4691, Acc: 0.8152, Speed: 238.5k, Time: 28.9577
Epoch: 173, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4688, Acc: 0.8156, Speed: 240.2k, Time: 34.4908
Epoch: 173, Batch: 7000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4681, Acc: 0.8160, Speed: 238.3k, Time: 40.5384
Epoch: 173, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4687, Acc: 0.8158, Speed: 239.0k, Time: 46.1757
Epoch: 173, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4690, Acc: 0.8159, Speed: 239.0k, Time: 51.8752
Train 0.8158
Val 0.8299
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895

skip saving model for perf <= 0.8335
Epoch: 174, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4692, Acc: 0.8147, Speed: 253.4k, Time: 5.4992
Epoch: 174, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4686, Acc: 0.8160, Speed: 248.4k, Time: 11.1299
Epoch: 174, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4702, Acc: 0.8158, Speed: 248.0k, Time: 16.7762
Epoch: 174, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4694, Acc: 0.8165, Speed: 247.0k, Time: 22.3595
Epoch: 174, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4696, Acc: 0.8163, Speed: 247.4k, Time: 27.9439
Epoch: 174, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4703, Acc: 0.8159, Speed: 247.9k, Time: 33.3818
Epoch: 174, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4704, Acc: 0.8159, Speed: 246.5k, Time: 39.1933
Epoch: 174, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4703, Acc: 0.8157, Speed: 247.4k, Time: 44.5976
Epoch: 174, Batch: 9000/9683, Batch size: 10, LR: 0.0500, Loss: 0.4700, Acc: 0.8158, Speed: 245.5k, Time: 50.5063
Train 0.8158
Val 0.8308
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810

skip saving model for perf <= 0.8335
Epoch: 175, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4702, Acc: 0.8166, Speed: 235.3k, Time: 5.8659
Epoch: 175, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4704, Acc: 0.8159, Speed: 239.2k, Time: 11.4946
Epoch: 175, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4689, Acc: 0.8166, Speed: 242.3k, Time: 17.0003
Epoch: 175, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4686, Acc: 0.8165, Speed: 244.7k, Time: 22.4570
Epoch: 175, Batch: 5000/9683, Batch size: 15, LR: 0.0500, Loss: 0.4699, Acc: 0.8159, Speed: 242.6k, Time: 28.3767
Epoch: 175, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4704, Acc: 0.8156, Speed: 241.8k, Time: 34.2397
Epoch: 175, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4703, Acc: 0.8157, Speed: 242.7k, Time: 39.8025
Epoch: 175, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4696, Acc: 0.8160, Speed: 242.3k, Time: 45.5032
Epoch: 175, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4694, Acc: 0.8160, Speed: 243.3k, Time: 50.9860
Train 0.8163
Val 0.8308
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810

skip saving model for perf <= 0.8335
Epoch: 176, Batch: 1000/9683, Batch size: 9, LR: 0.0500, Loss: 0.4678, Acc: 0.8157, Speed: 249.6k, Time: 5.5855
Epoch: 176, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4671, Acc: 0.8166, Speed: 241.5k, Time: 11.4102
Epoch: 176, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4684, Acc: 0.8160, Speed: 242.2k, Time: 17.0647
Epoch: 176, Batch: 4000/9683, Batch size: 47, LR: 0.0500, Loss: 0.4696, Acc: 0.8160, Speed: 240.3k, Time: 22.9439
Epoch: 176, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4699, Acc: 0.8157, Speed: 243.0k, Time: 28.5360
Epoch: 176, Batch: 6000/9683, Batch size: 3, LR: 0.0500, Loss: 0.4698, Acc: 0.8158, Speed: 242.0k, Time: 34.3387
Epoch: 176, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4701, Acc: 0.8157, Speed: 241.1k, Time: 40.1459
Epoch: 176, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4703, Acc: 0.8156, Speed: 241.8k, Time: 45.6836
Epoch: 176, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4697, Acc: 0.8159, Speed: 242.0k, Time: 51.3059
Train 0.8159
Val 0.8315
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521

skip saving model for perf <= 0.8335
Epoch: 177, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4682, Acc: 0.8169, Speed: 234.5k, Time: 5.8682
Epoch: 177, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4705, Acc: 0.8153, Speed: 239.4k, Time: 11.5565
Epoch: 177, Batch: 3000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4703, Acc: 0.8154, Speed: 238.7k, Time: 17.4292
Epoch: 177, Batch: 4000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4711, Acc: 0.8151, Speed: 240.0k, Time: 23.0281
Epoch: 177, Batch: 5000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4718, Acc: 0.8146, Speed: 238.9k, Time: 28.8805
Epoch: 177, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4720, Acc: 0.8146, Speed: 238.4k, Time: 34.7011
Epoch: 177, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4718, Acc: 0.8145, Speed: 238.5k, Time: 40.5185
Epoch: 177, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4712, Acc: 0.8148, Speed: 239.2k, Time: 46.1435
Epoch: 177, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4712, Acc: 0.8147, Speed: 238.5k, Time: 52.0823
Train 0.8149
Val 0.8325
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537

skip saving model for perf <= 0.8335
Epoch: 178, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4701, Acc: 0.8163, Speed: 235.4k, Time: 5.8739
Epoch: 178, Batch: 2000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4711, Acc: 0.8158, Speed: 240.5k, Time: 11.4775
Epoch: 178, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4710, Acc: 0.8161, Speed: 236.5k, Time: 17.4203
Epoch: 178, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4706, Acc: 0.8158, Speed: 238.1k, Time: 23.0772
Epoch: 178, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4692, Acc: 0.8163, Speed: 240.0k, Time: 28.6374
Epoch: 178, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4689, Acc: 0.8163, Speed: 237.8k, Time: 34.7533
Epoch: 178, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4695, Acc: 0.8160, Speed: 240.0k, Time: 40.1360
Epoch: 178, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4699, Acc: 0.8157, Speed: 240.0k, Time: 45.8821
Epoch: 178, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4704, Acc: 0.8155, Speed: 238.9k, Time: 51.8695
Train 0.8155
Val 0.8317
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724

skip saving model for perf <= 0.8335
Epoch: 179, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4698, Acc: 0.8157, Speed: 245.8k, Time: 5.6294
Epoch: 179, Batch: 2000/9683, Batch size: 62, LR: 0.0500, Loss: 0.4704, Acc: 0.8158, Speed: 247.2k, Time: 11.3267
Epoch: 179, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4702, Acc: 0.8160, Speed: 247.8k, Time: 16.8148
Epoch: 179, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4690, Acc: 0.8163, Speed: 246.4k, Time: 22.5671
Epoch: 179, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4688, Acc: 0.8163, Speed: 247.0k, Time: 27.9939
Epoch: 179, Batch: 6000/9683, Batch size: 50, LR: 0.0500, Loss: 0.4691, Acc: 0.8163, Speed: 245.3k, Time: 33.7751
Epoch: 179, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4697, Acc: 0.8160, Speed: 243.3k, Time: 39.6572
Epoch: 179, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4701, Acc: 0.8158, Speed: 243.4k, Time: 45.1757
Epoch: 179, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4705, Acc: 0.8156, Speed: 244.5k, Time: 50.6250
Train 0.8156
Val 0.8328
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842

skip saving model for perf <= 0.8335
Epoch: 180, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4712, Acc: 0.8167, Speed: 245.3k, Time: 5.7444
Epoch: 180, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4708, Acc: 0.8157, Speed: 243.5k, Time: 11.4444
Epoch: 180, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4689, Acc: 0.8165, Speed: 244.2k, Time: 17.1609
Epoch: 180, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4693, Acc: 0.8160, Speed: 246.5k, Time: 22.7229
Epoch: 180, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4688, Acc: 0.8156, Speed: 246.8k, Time: 28.2906
Epoch: 180, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4695, Acc: 0.8153, Speed: 245.6k, Time: 33.9717
Epoch: 180, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4695, Acc: 0.8154, Speed: 242.3k, Time: 40.0262
Epoch: 180, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4693, Acc: 0.8156, Speed: 241.4k, Time: 45.7487
Epoch: 180, Batch: 9000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4695, Acc: 0.8155, Speed: 241.3k, Time: 51.4563
Train 0.8155
Val 0.8325
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537

skip saving model for perf <= 0.8335
Epoch: 181, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4672, Acc: 0.8164, Speed: 238.5k, Time: 5.7272
Epoch: 181, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4683, Acc: 0.8160, Speed: 227.5k, Time: 12.1005
Epoch: 181, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4687, Acc: 0.8159, Speed: 232.9k, Time: 17.8264
Epoch: 181, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4675, Acc: 0.8162, Speed: 237.8k, Time: 23.3182
Epoch: 181, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4678, Acc: 0.8161, Speed: 239.2k, Time: 28.9789
Epoch: 181, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4688, Acc: 0.8157, Speed: 238.2k, Time: 34.9858
Epoch: 181, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4683, Acc: 0.8159, Speed: 240.3k, Time: 40.4241
Epoch: 181, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4681, Acc: 0.8160, Speed: 240.3k, Time: 45.9831
Epoch: 181, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4682, Acc: 0.8160, Speed: 240.5k, Time: 51.5723
Train 0.8161
Val 0.8323
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537
181	0.816053	0.832334

skip saving model for perf <= 0.8335
Epoch: 182, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4657, Acc: 0.8157, Speed: 254.0k, Time: 5.5835
Epoch: 182, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4672, Acc: 0.8157, Speed: 250.6k, Time: 11.1323
Epoch: 182, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4673, Acc: 0.8154, Speed: 249.5k, Time: 16.6933
Epoch: 182, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4676, Acc: 0.8158, Speed: 247.9k, Time: 22.4127
Epoch: 182, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4680, Acc: 0.8159, Speed: 250.4k, Time: 27.8046
Epoch: 182, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4686, Acc: 0.8158, Speed: 251.9k, Time: 33.1616
Epoch: 182, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4689, Acc: 0.8157, Speed: 247.7k, Time: 39.1989
Epoch: 182, Batch: 8000/9683, Batch size: 52, LR: 0.0500, Loss: 0.4684, Acc: 0.8161, Speed: 247.8k, Time: 44.5953
Epoch: 182, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4683, Acc: 0.8163, Speed: 246.0k, Time: 50.4222
Train 0.8162
Val 0.8313
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537
181	0.816053	0.832334
182	0.816219	0.831318

skip saving model for perf <= 0.8335
Epoch: 183, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4680, Acc: 0.8161, Speed: 244.4k, Time: 5.6475
Epoch: 183, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4657, Acc: 0.8167, Speed: 243.7k, Time: 11.3348
Epoch: 183, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4682, Acc: 0.8153, Speed: 243.4k, Time: 17.0171
Epoch: 183, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4674, Acc: 0.8159, Speed: 242.8k, Time: 22.6236
Epoch: 183, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4670, Acc: 0.8160, Speed: 238.8k, Time: 28.8771
Epoch: 183, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4664, Acc: 0.8165, Speed: 236.7k, Time: 34.8998
Epoch: 183, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4660, Acc: 0.8168, Speed: 238.1k, Time: 40.4850
Epoch: 183, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4669, Acc: 0.8164, Speed: 239.7k, Time: 45.9693
Epoch: 183, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4671, Acc: 0.8166, Speed: 240.8k, Time: 51.4609
Train 0.8165
Val 0.8304
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537
181	0.816053	0.832334
182	0.816219	0.831318
183	0.816478	0.830403

skip saving model for perf <= 0.8335
Epoch: 184, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4688, Acc: 0.8166, Speed: 227.7k, Time: 5.8685
Epoch: 184, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4666, Acc: 0.8179, Speed: 238.6k, Time: 11.4827
Epoch: 184, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4659, Acc: 0.8177, Speed: 242.7k, Time: 16.9604
Epoch: 184, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4660, Acc: 0.8179, Speed: 244.4k, Time: 22.4127
Epoch: 184, Batch: 5000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4659, Acc: 0.8178, Speed: 243.9k, Time: 28.1506
Epoch: 184, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4665, Acc: 0.8174, Speed: 243.9k, Time: 33.7790
Epoch: 184, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4668, Acc: 0.8171, Speed: 241.4k, Time: 39.8326
Epoch: 184, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4665, Acc: 0.8172, Speed: 240.9k, Time: 45.6031
Epoch: 184, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4668, Acc: 0.8170, Speed: 240.8k, Time: 51.4215
Train 0.8167
Val 0.8311
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537
181	0.816053	0.832334
182	0.816219	0.831318
183	0.816478	0.830403
184	0.816749	0.831115

skip saving model for perf <= 0.8335
Epoch: 185, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4651, Acc: 0.8186, Speed: 240.2k, Time: 5.8460
Epoch: 185, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4643, Acc: 0.8187, Speed: 242.0k, Time: 11.5119
Epoch: 185, Batch: 3000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4629, Acc: 0.8188, Speed: 246.0k, Time: 16.9907
Epoch: 185, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4637, Acc: 0.8180, Speed: 244.8k, Time: 22.6874
Epoch: 185, Batch: 5000/9683, Batch size: 10, LR: 0.0500, Loss: 0.4643, Acc: 0.8180, Speed: 244.8k, Time: 28.3405
Epoch: 185, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4648, Acc: 0.8181, Speed: 243.1k, Time: 34.2672
Epoch: 185, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4656, Acc: 0.8177, Speed: 242.9k, Time: 39.9227
Epoch: 185, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4656, Acc: 0.8176, Speed: 242.9k, Time: 45.5602
Epoch: 185, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4666, Acc: 0.8170, Speed: 241.6k, Time: 51.3856
Train 0.8168
Val 0.8308
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537
181	0.816053	0.832334
182	0.816219	0.831318
183	0.816478	0.830403
184	0.816749	0.831115
185	0.816838	0.830810

skip saving model for perf <= 0.8335
Epoch: 186, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4606, Acc: 0.8187, Speed: 238.8k, Time: 5.7513
Epoch: 186, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4605, Acc: 0.8197, Speed: 246.0k, Time: 11.1929
Epoch: 186, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4637, Acc: 0.8180, Speed: 243.4k, Time: 16.9528
Epoch: 186, Batch: 4000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4641, Acc: 0.8176, Speed: 245.7k, Time: 22.4724
Epoch: 186, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4646, Acc: 0.8175, Speed: 243.7k, Time: 28.3048
Epoch: 186, Batch: 6000/9683, Batch size: 33, LR: 0.0500, Loss: 0.4650, Acc: 0.8174, Speed: 243.3k, Time: 34.0085
Epoch: 186, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4658, Acc: 0.8171, Speed: 242.1k, Time: 39.7966
Epoch: 186, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4666, Acc: 0.8167, Speed: 241.5k, Time: 45.5761
Epoch: 186, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4664, Acc: 0.8168, Speed: 238.9k, Time: 51.8402
Train 0.8165
Val 0.8307
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537
181	0.816053	0.832334
182	0.816219	0.831318
183	0.816478	0.830403
184	0.816749	0.831115
185	0.816838	0.830810
186	0.816485	0.830708

skip saving model for perf <= 0.8335
Epoch: 187, Batch: 1000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4667, Acc: 0.8162, Speed: 258.2k, Time: 5.4589
Epoch: 187, Batch: 2000/9683, Batch size: 6, LR: 0.0500, Loss: 0.4641, Acc: 0.8179, Speed: 243.4k, Time: 11.4955
Epoch: 187, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4655, Acc: 0.8171, Speed: 242.7k, Time: 17.1622
Epoch: 187, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4660, Acc: 0.8174, Speed: 242.7k, Time: 22.7500
Epoch: 187, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4665, Acc: 0.8172, Speed: 244.5k, Time: 28.2640
Epoch: 187, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4676, Acc: 0.8169, Speed: 243.9k, Time: 34.0620
Epoch: 187, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4676, Acc: 0.8168, Speed: 244.3k, Time: 39.6600
Epoch: 187, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4679, Acc: 0.8168, Speed: 241.9k, Time: 45.7123
Epoch: 187, Batch: 9000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4677, Acc: 0.8168, Speed: 242.5k, Time: 51.2452
Train 0.8168
Val 0.8319
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537
181	0.816053	0.832334
182	0.816219	0.831318
183	0.816478	0.830403
184	0.816749	0.831115
185	0.816838	0.830810
186	0.816485	0.830708
187	0.816802	0.831928

skip saving model for perf <= 0.8335
Epoch: 188, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4618, Acc: 0.8186, Speed: 251.1k, Time: 5.5414
Epoch: 188, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4649, Acc: 0.8176, Speed: 244.0k, Time: 11.3347
Epoch: 188, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4627, Acc: 0.8184, Speed: 243.7k, Time: 16.9168
Epoch: 188, Batch: 4000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4645, Acc: 0.8173, Speed: 238.9k, Time: 22.9996
Epoch: 188, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4653, Acc: 0.8169, Speed: 241.1k, Time: 28.5094
Epoch: 188, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4662, Acc: 0.8166, Speed: 241.2k, Time: 34.1569
Epoch: 188, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4668, Acc: 0.8166, Speed: 244.2k, Time: 39.6163
Epoch: 188, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4670, Acc: 0.8164, Speed: 243.6k, Time: 45.2835
Epoch: 188, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4669, Acc: 0.8165, Speed: 244.0k, Time: 50.8081
Train 0.8165
Val 0.8311
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537
181	0.816053	0.832334
182	0.816219	0.831318
183	0.816478	0.830403
184	0.816749	0.831115
185	0.816838	0.830810
186	0.816485	0.830708
187	0.816802	0.831928
188	0.816519	0.831115

skip saving model for perf <= 0.8335
Epoch: 189, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4690, Acc: 0.8161, Speed: 241.1k, Time: 5.6719
Epoch: 189, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4676, Acc: 0.8163, Speed: 243.6k, Time: 11.3177
Epoch: 189, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4683, Acc: 0.8158, Speed: 239.0k, Time: 17.1850
Epoch: 189, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4669, Acc: 0.8169, Speed: 240.5k, Time: 22.8750
Epoch: 189, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4671, Acc: 0.8168, Speed: 241.5k, Time: 28.5091
Epoch: 189, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4673, Acc: 0.8165, Speed: 239.3k, Time: 34.4992
Epoch: 189, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4676, Acc: 0.8165, Speed: 239.3k, Time: 40.1735
Epoch: 189, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4678, Acc: 0.8164, Speed: 240.7k, Time: 45.7758
Epoch: 189, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4676, Acc: 0.8166, Speed: 241.6k, Time: 51.3902
Train 0.8165
Val 0.8300
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537
181	0.816053	0.832334
182	0.816219	0.831318
183	0.816478	0.830403
184	0.816749	0.831115
185	0.816838	0.830810
186	0.816485	0.830708
187	0.816802	0.831928
188	0.816519	0.831115
189	0.816541	0.829997

skip saving model for perf <= 0.8335
Epoch: 190, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4598, Acc: 0.8217, Speed: 254.1k, Time: 5.4367
Epoch: 190, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4618, Acc: 0.8194, Speed: 247.3k, Time: 11.0822
Epoch: 190, Batch: 3000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4634, Acc: 0.8189, Speed: 244.6k, Time: 16.8194
Epoch: 190, Batch: 4000/9683, Batch size: 3, LR: 0.0500, Loss: 0.4647, Acc: 0.8181, Speed: 243.7k, Time: 22.5548
Epoch: 190, Batch: 5000/9683, Batch size: 34, LR: 0.0500, Loss: 0.4647, Acc: 0.8180, Speed: 240.4k, Time: 28.4960
Epoch: 190, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4653, Acc: 0.8177, Speed: 243.6k, Time: 33.9599
Epoch: 190, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4653, Acc: 0.8179, Speed: 242.9k, Time: 39.6309
Epoch: 190, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4657, Acc: 0.8177, Speed: 242.4k, Time: 45.3851
Epoch: 190, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4662, Acc: 0.8177, Speed: 243.8k, Time: 50.8720
Train 0.8177
Val 0.8311
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537
181	0.816053	0.832334
182	0.816219	0.831318
183	0.816478	0.830403
184	0.816749	0.831115
185	0.816838	0.830810
186	0.816485	0.830708
187	0.816802	0.831928
188	0.816519	0.831115
189	0.816541	0.829997
190	0.817717	0.831115

skip saving model for perf <= 0.8335
Epoch: 191, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4690, Acc: 0.8161, Speed: 246.5k, Time: 5.6096
Epoch: 191, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4701, Acc: 0.8157, Speed: 247.5k, Time: 11.1479
Epoch: 191, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4684, Acc: 0.8162, Speed: 241.7k, Time: 17.0854
Epoch: 191, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4674, Acc: 0.8166, Speed: 238.2k, Time: 23.0523
Epoch: 191, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4678, Acc: 0.8163, Speed: 238.7k, Time: 28.7630
Epoch: 191, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4672, Acc: 0.8165, Speed: 239.8k, Time: 34.4848
Epoch: 191, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4671, Acc: 0.8166, Speed: 239.8k, Time: 40.2392
Epoch: 191, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4671, Acc: 0.8166, Speed: 240.4k, Time: 45.9083
Epoch: 191, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4664, Acc: 0.8167, Speed: 241.0k, Time: 51.4840
Train 0.8168
Val 0.8304
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537
181	0.816053	0.832334
182	0.816219	0.831318
183	0.816478	0.830403
184	0.816749	0.831115
185	0.816838	0.830810
186	0.816485	0.830708
187	0.816802	0.831928
188	0.816519	0.831115
189	0.816541	0.829997
190	0.817717	0.831115
191	0.816760	0.830403

skip saving model for perf <= 0.8335
Epoch: 192, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4606, Acc: 0.8212, Speed: 233.1k, Time: 5.8870
Epoch: 192, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4602, Acc: 0.8199, Speed: 239.8k, Time: 11.5802
Epoch: 192, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4620, Acc: 0.8188, Speed: 237.7k, Time: 17.4846
Epoch: 192, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4637, Acc: 0.8183, Speed: 239.5k, Time: 23.0830
Epoch: 192, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4644, Acc: 0.8179, Speed: 239.4k, Time: 28.8304
Epoch: 192, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4655, Acc: 0.8173, Speed: 240.2k, Time: 34.3617
Epoch: 192, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4659, Acc: 0.8172, Speed: 241.2k, Time: 39.9071
Epoch: 192, Batch: 8000/9683, Batch size: 7, LR: 0.0500, Loss: 0.4668, Acc: 0.8170, Speed: 242.2k, Time: 45.3868
Epoch: 192, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4675, Acc: 0.8166, Speed: 242.8k, Time: 51.0031
Train 0.8165
Val 0.8301
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537
181	0.816053	0.832334
182	0.816219	0.831318
183	0.816478	0.830403
184	0.816749	0.831115
185	0.816838	0.830810
186	0.816485	0.830708
187	0.816802	0.831928
188	0.816519	0.831115
189	0.816541	0.829997
190	0.817717	0.831115
191	0.816760	0.830403
192	0.816505	0.830099

skip saving model for perf <= 0.8335
Epoch: 193, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4663, Acc: 0.8161, Speed: 248.0k, Time: 5.4921
Epoch: 193, Batch: 2000/9683, Batch size: 52, LR: 0.0500, Loss: 0.4667, Acc: 0.8166, Speed: 255.1k, Time: 10.7620
Epoch: 193, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4671, Acc: 0.8167, Speed: 252.3k, Time: 16.2670
Epoch: 193, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4674, Acc: 0.8165, Speed: 251.3k, Time: 22.0353
Epoch: 193, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4671, Acc: 0.8169, Speed: 249.6k, Time: 27.8083
Epoch: 193, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4677, Acc: 0.8166, Speed: 246.4k, Time: 33.7237
Epoch: 193, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4670, Acc: 0.8167, Speed: 244.2k, Time: 39.6334
Epoch: 193, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4675, Acc: 0.8165, Speed: 244.2k, Time: 45.2399
Epoch: 193, Batch: 9000/9683, Batch size: 7, LR: 0.0500, Loss: 0.4686, Acc: 0.8161, Speed: 244.2k, Time: 50.7793
Train 0.8161
Val 0.8312
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537
181	0.816053	0.832334
182	0.816219	0.831318
183	0.816478	0.830403
184	0.816749	0.831115
185	0.816838	0.830810
186	0.816485	0.830708
187	0.816802	0.831928
188	0.816519	0.831115
189	0.816541	0.829997
190	0.817717	0.831115
191	0.816760	0.830403
192	0.816505	0.830099
193	0.816081	0.831216

skip saving model for perf <= 0.8335
Epoch: 194, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4704, Acc: 0.8151, Speed: 255.7k, Time: 5.4016
Epoch: 194, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4725, Acc: 0.8143, Speed: 246.8k, Time: 11.2219
Epoch: 194, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4713, Acc: 0.8149, Speed: 245.6k, Time: 16.8990
Epoch: 194, Batch: 4000/9683, Batch size: 9, LR: 0.0500, Loss: 0.4688, Acc: 0.8160, Speed: 240.4k, Time: 22.9918
Epoch: 194, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4699, Acc: 0.8156, Speed: 241.0k, Time: 28.7816
Epoch: 194, Batch: 6000/9683, Batch size: 19, LR: 0.0500, Loss: 0.4695, Acc: 0.8159, Speed: 240.4k, Time: 34.6041
Epoch: 194, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4696, Acc: 0.8158, Speed: 241.7k, Time: 40.0787
Epoch: 194, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4686, Acc: 0.8162, Speed: 242.4k, Time: 45.4943
Epoch: 194, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4684, Acc: 0.8162, Speed: 241.6k, Time: 51.2380
Train 0.8164
Val 0.8312
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537
181	0.816053	0.832334
182	0.816219	0.831318
183	0.816478	0.830403
184	0.816749	0.831115
185	0.816838	0.830810
186	0.816485	0.830708
187	0.816802	0.831928
188	0.816519	0.831115
189	0.816541	0.829997
190	0.817717	0.831115
191	0.816760	0.830403
192	0.816505	0.830099
193	0.816081	0.831216
194	0.816403	0.831216

skip saving model for perf <= 0.8335
Epoch: 195, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4709, Acc: 0.8152, Speed: 256.7k, Time: 5.3684
Epoch: 195, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4654, Acc: 0.8171, Speed: 253.6k, Time: 10.8953
Epoch: 195, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4661, Acc: 0.8173, Speed: 250.0k, Time: 16.6177
Epoch: 195, Batch: 4000/9683, Batch size: 6, LR: 0.0500, Loss: 0.4668, Acc: 0.8172, Speed: 249.5k, Time: 22.1195
Epoch: 195, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4664, Acc: 0.8173, Speed: 246.5k, Time: 28.0327
Epoch: 195, Batch: 6000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4670, Acc: 0.8168, Speed: 244.3k, Time: 33.8304
Epoch: 195, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4668, Acc: 0.8168, Speed: 243.0k, Time: 39.6710
Epoch: 195, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4658, Acc: 0.8173, Speed: 243.2k, Time: 45.2353
Epoch: 195, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4656, Acc: 0.8173, Speed: 244.6k, Time: 50.7493
Train 0.8172
Val 0.8308
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537
181	0.816053	0.832334
182	0.816219	0.831318
183	0.816478	0.830403
184	0.816749	0.831115
185	0.816838	0.830810
186	0.816485	0.830708
187	0.816802	0.831928
188	0.816519	0.831115
189	0.816541	0.829997
190	0.817717	0.831115
191	0.816760	0.830403
192	0.816505	0.830099
193	0.816081	0.831216
194	0.816403	0.831216
195	0.817229	0.830810

skip saving model for perf <= 0.8335
Epoch: 196, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4720, Acc: 0.8149, Speed: 239.6k, Time: 5.9283
Epoch: 196, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4694, Acc: 0.8159, Speed: 245.9k, Time: 11.3943
Epoch: 196, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4699, Acc: 0.8157, Speed: 239.7k, Time: 17.3810
Epoch: 196, Batch: 4000/9683, Batch size: 2, LR: 0.0500, Loss: 0.4678, Acc: 0.8167, Speed: 239.7k, Time: 23.0324
Epoch: 196, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4670, Acc: 0.8169, Speed: 242.1k, Time: 28.5021
Epoch: 196, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4674, Acc: 0.8164, Speed: 241.5k, Time: 34.1799
Epoch: 196, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4674, Acc: 0.8163, Speed: 240.9k, Time: 40.0722
Epoch: 196, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4678, Acc: 0.8162, Speed: 241.9k, Time: 45.6286
Epoch: 196, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4679, Acc: 0.8163, Speed: 241.9k, Time: 51.3232
Train 0.8163
Val 0.8314
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537
181	0.816053	0.832334
182	0.816219	0.831318
183	0.816478	0.830403
184	0.816749	0.831115
185	0.816838	0.830810
186	0.816485	0.830708
187	0.816802	0.831928
188	0.816519	0.831115
189	0.816541	0.829997
190	0.817717	0.831115
191	0.816760	0.830403
192	0.816505	0.830099
193	0.816081	0.831216
194	0.816403	0.831216
195	0.817229	0.830810
196	0.816274	0.831420

skip saving model for perf <= 0.8335
Epoch: 197, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4736, Acc: 0.8141, Speed: 245.4k, Time: 5.6412
Epoch: 197, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4683, Acc: 0.8157, Speed: 238.2k, Time: 11.5258
Epoch: 197, Batch: 3000/9683, Batch size: 4, LR: 0.0500, Loss: 0.4684, Acc: 0.8158, Speed: 238.8k, Time: 17.3531
Epoch: 197, Batch: 4000/9683, Batch size: 56, LR: 0.0500, Loss: 0.4687, Acc: 0.8159, Speed: 238.1k, Time: 23.0128
Epoch: 197, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4688, Acc: 0.8156, Speed: 239.7k, Time: 28.5914
Epoch: 197, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4682, Acc: 0.8159, Speed: 239.5k, Time: 34.3781
Epoch: 197, Batch: 7000/9683, Batch size: 19, LR: 0.0500, Loss: 0.4678, Acc: 0.8162, Speed: 240.9k, Time: 39.9152
Epoch: 197, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4674, Acc: 0.8164, Speed: 242.8k, Time: 45.2820
Epoch: 197, Batch: 9000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4673, Acc: 0.8165, Speed: 243.0k, Time: 51.0233
Train 0.8165
Val 0.8296
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537
181	0.816053	0.832334
182	0.816219	0.831318
183	0.816478	0.830403
184	0.816749	0.831115
185	0.816838	0.830810
186	0.816485	0.830708
187	0.816802	0.831928
188	0.816519	0.831115
189	0.816541	0.829997
190	0.817717	0.831115
191	0.816760	0.830403
192	0.816505	0.830099
193	0.816081	0.831216
194	0.816403	0.831216
195	0.817229	0.830810
196	0.816274	0.831420
197	0.816514	0.829590

skip saving model for perf <= 0.8335
Epoch: 198, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4698, Acc: 0.8147, Speed: 255.2k, Time: 5.3470
Epoch: 198, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4689, Acc: 0.8155, Speed: 253.0k, Time: 10.8397
Epoch: 198, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4670, Acc: 0.8163, Speed: 251.4k, Time: 16.3392
Epoch: 198, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4673, Acc: 0.8161, Speed: 251.8k, Time: 21.8816
Epoch: 198, Batch: 5000/9683, Batch size: 50, LR: 0.0500, Loss: 0.4667, Acc: 0.8164, Speed: 249.8k, Time: 27.5902
Epoch: 198, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4671, Acc: 0.8165, Speed: 250.8k, Time: 33.0281
Epoch: 198, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4667, Acc: 0.8169, Speed: 250.5k, Time: 38.5634
Epoch: 198, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4663, Acc: 0.8169, Speed: 250.9k, Time: 44.0655
Epoch: 198, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4665, Acc: 0.8168, Speed: 248.9k, Time: 49.8124
Train 0.8171
Val 0.8292
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537
181	0.816053	0.832334
182	0.816219	0.831318
183	0.816478	0.830403
184	0.816749	0.831115
185	0.816838	0.830810
186	0.816485	0.830708
187	0.816802	0.831928
188	0.816519	0.831115
189	0.816541	0.829997
190	0.817717	0.831115
191	0.816760	0.830403
192	0.816505	0.830099
193	0.816081	0.831216
194	0.816403	0.831216
195	0.817229	0.830810
196	0.816274	0.831420
197	0.816514	0.829590
198	0.817082	0.829184

skip saving model for perf <= 0.8335
Epoch: 199, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4688, Acc: 0.8170, Speed: 250.0k, Time: 5.5302
Epoch: 199, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4691, Acc: 0.8168, Speed: 243.4k, Time: 11.2354
Epoch: 199, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4682, Acc: 0.8168, Speed: 244.4k, Time: 16.7778
Epoch: 199, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4671, Acc: 0.8172, Speed: 243.2k, Time: 22.5614
Epoch: 199, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4671, Acc: 0.8169, Speed: 243.7k, Time: 28.2565
Epoch: 199, Batch: 6000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4660, Acc: 0.8177, Speed: 245.3k, Time: 33.6508
Epoch: 199, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4658, Acc: 0.8177, Speed: 246.2k, Time: 39.1647
Epoch: 199, Batch: 8000/9683, Batch size: 6, LR: 0.0500, Loss: 0.4651, Acc: 0.8178, Speed: 242.1k, Time: 45.3923
Epoch: 199, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4659, Acc: 0.8175, Speed: 242.1k, Time: 51.1905
Train 0.8174
Val 0.8303
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537
181	0.816053	0.832334
182	0.816219	0.831318
183	0.816478	0.830403
184	0.816749	0.831115
185	0.816838	0.830810
186	0.816485	0.830708
187	0.816802	0.831928
188	0.816519	0.831115
189	0.816541	0.829997
190	0.817717	0.831115
191	0.816760	0.830403
192	0.816505	0.830099
193	0.816081	0.831216
194	0.816403	0.831216
195	0.817229	0.830810
196	0.816274	0.831420
197	0.816514	0.829590
198	0.817082	0.829184
199	0.817371	0.830302

skip saving model for perf <= 0.8335
Epoch: 200, Batch: 1000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4670, Acc: 0.8168, Speed: 222.9k, Time: 6.1632
Epoch: 200, Batch: 2000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4679, Acc: 0.8161, Speed: 229.6k, Time: 12.1171
Epoch: 200, Batch: 3000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4674, Acc: 0.8163, Speed: 234.8k, Time: 17.5848
Epoch: 200, Batch: 4000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4685, Acc: 0.8159, Speed: 237.3k, Time: 23.2289
Epoch: 200, Batch: 5000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4694, Acc: 0.8156, Speed: 237.1k, Time: 28.9941
Epoch: 200, Batch: 6000/9683, Batch size: 1, LR: 0.0500, Loss: 0.4685, Acc: 0.8157, Speed: 238.3k, Time: 34.7225
Epoch: 200, Batch: 7000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4676, Acc: 0.8165, Speed: 240.7k, Time: 40.1160
Epoch: 200, Batch: 8000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4680, Acc: 0.8161, Speed: 240.4k, Time: 45.9648
Epoch: 200, Batch: 9000/9683, Batch size: 64, LR: 0.0500, Loss: 0.4681, Acc: 0.8161, Speed: 238.8k, Time: 52.0142
Train 0.8162
Val 0.8307
1	0.382903	0.410832
2	0.456146	0.489381
3	0.498593	0.515090
4	0.549561	0.584900
5	0.585631	0.608373
6	0.598832	0.622498
7	0.612965	0.632659
8	0.627787	0.656742
9	0.640507	0.668936
10	0.658645	0.700234
11	0.677672	0.709074
12	0.691410	0.727670
13	0.707967	0.736714
14	0.718410	0.744437
15	0.726973	0.755919
16	0.734290	0.759171
17	0.739838	0.764252
18	0.744567	0.770755
19	0.748800	0.774108
20	0.752653	0.779697
21	0.753751	0.777970
22	0.757561	0.784676
23	0.760311	0.785083
24	0.762621	0.789656
25	0.765573	0.790773
26	0.766740	0.786302
27	0.768883	0.793822
28	0.770954	0.797988
29	0.771136	0.795752
30	0.773222	0.799106
31	0.774453	0.801138
32	0.776524	0.803475
33	0.777556	0.800122
34	0.779264	0.800732
35	0.779045	0.803170
36	0.781193	0.804999
37	0.782395	0.807641
38	0.781685	0.805609
39	0.782666	0.810995
40	0.784069	0.810080
41	0.784710	0.805406
42	0.785309	0.810995
43	0.786798	0.804491
44	0.786064	0.807743
45	0.787632	0.811706
46	0.788034	0.811096
47	0.789546	0.812417
48	0.789743	0.812519
49	0.789820	0.813942
50	0.790073	0.811401
51	0.791350	0.810792
52	0.791705	0.813332
53	0.792335	0.815466
54	0.793234	0.815161
55	0.793125	0.814653
56	0.794348	0.813942
57	0.794401	0.815568
58	0.793983	0.815669
59	0.794831	0.815263
60	0.795164	0.814551
61	0.796529	0.818006
62	0.796837	0.817905
63	0.795464	0.817092
64	0.797456	0.818210
65	0.797468	0.817295
66	0.798120	0.816990
67	0.797401	0.818311
68	0.798409	0.817498
69	0.799354	0.817600
70	0.799531	0.819632
71	0.798828	0.818006
72	0.799909	0.819429
73	0.799642	0.818006
74	0.801062	0.818413
75	0.801353	0.819226
76	0.801622	0.820648
77	0.801686	0.820039
78	0.800914	0.818514
79	0.800672	0.820953
80	0.800978	0.821156
81	0.802518	0.821461
82	0.802450	0.821563
83	0.802747	0.820750
84	0.802623	0.821969
85	0.802512	0.821969
86	0.803359	0.822173
87	0.803339	0.821055
88	0.804132	0.822985
89	0.803612	0.822477
90	0.804258	0.822579
91	0.803748	0.822985
92	0.804258	0.824916
93	0.804558	0.824916
94	0.804678	0.823392
95	0.805920	0.824306
96	0.804362	0.820242
97	0.805547	0.823798
98	0.804946	0.822376
99	0.805095	0.824103
100	0.806420	0.822071
101	0.805652	0.823595
102	0.806289	0.824306
103	0.807099	0.825018
104	0.806799	0.826542
105	0.807116	0.826339
106	0.807880	0.824205
107	0.808630	0.825831
108	0.807868	0.826542
109	0.807522	0.827660
110	0.807438	0.825526
111	0.808696	0.828473
112	0.808918	0.827355
113	0.808656	0.826034
114	0.808312	0.825729
115	0.808880	0.828879
116	0.809641	0.827965
117	0.810385	0.829387
118	0.809795	0.827152
119	0.809349	0.828168
120	0.808652	0.825729
121	0.809646	0.830200
122	0.810321	0.827965
123	0.809914	0.829286
124	0.810549	0.827253
125	0.810603	0.828981
126	0.810531	0.829590
127	0.811559	0.827050
128	0.811321	0.828473
129	0.811674	0.828269
130	0.810778	0.829082
131	0.810653	0.830607
132	0.810993	0.832029
133	0.811073	0.825018
134	0.810877	0.831216
135	0.811954	0.830099
136	0.811286	0.829997
137	0.812151	0.829895
138	0.812094	0.829286
139	0.810208	0.827457
140	0.812518	0.828066
141	0.812648	0.829387
142	0.812296	0.828168
143	0.813159	0.828879
144	0.812964	0.828473
145	0.812913	0.831928
146	0.813483	0.827965
147	0.812302	0.829590
148	0.813156	0.829895
149	0.813430	0.829590
150	0.813112	0.830099
151	0.811967	0.828269
152	0.813243	0.830302
153	0.813055	0.830607
154	0.813909	0.829286
155	0.813933	0.829286
156	0.814630	0.828269
157	0.814513	0.830302
158	0.814632	0.830302
159	0.814219	0.827558
160	0.814266	0.828473
161	0.813922	0.830607
162	0.814794	0.831724
163	0.814992	0.831115
164	0.814666	0.832131
165	0.815340	0.831013
166	0.814024	0.828066
167	0.814213	0.829997
168	0.814870	0.830505
169	0.814919	0.833452
170	0.815657	0.831013
171	0.816052	0.829794
172	0.814712	0.829387
173	0.815760	0.829895
174	0.815797	0.830810
175	0.816296	0.830810
176	0.815860	0.831521
177	0.814894	0.832537
178	0.815518	0.831724
179	0.815569	0.832842
180	0.815478	0.832537
181	0.816053	0.832334
182	0.816219	0.831318
183	0.816478	0.830403
184	0.816749	0.831115
185	0.816838	0.830810
186	0.816485	0.830708
187	0.816802	0.831928
188	0.816519	0.831115
189	0.816541	0.829997
190	0.817717	0.831115
191	0.816760	0.830403
192	0.816505	0.830099
193	0.816081	0.831216
194	0.816403	0.831216
195	0.817229	0.830810
196	0.816274	0.831420
197	0.816514	0.829590
198	0.817082	0.829184
199	0.817371	0.830302
200	0.816161	0.830708

skip saving model for perf <= 0.8335
converting gpu model to cpu model...
saving model to local_200_parikh.pt
